---
title: 'Logistic Regression'
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
bibliography: "../data.bib"
---

```{r setup, include=FALSE, message=FALSE}
set.seed(1)
library(latex2exp)
library(tidyverse)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Logistic Regression for Binary Response Variables
- Sections 14.1 to 14.12 of KNNL

```{r}
library(broom)
```

# Motivation

# Model

# Interpretation (odds ratios)

# Two ways to input data

# Bias reduction

- For small sample sizes, the MLE's of the logistic regression coefficients can be biased. A quick way to reduce this bias is through the [`{brglm2}`](https://cran.r-project.org/package=brglm2) package, which uses the method of @firth1993bias.

Their example:

```{r}
## The lizards example from ?brglm::brglm
data("lizards", package = "brglm2")
# Fit the model using maximum likelihood
lizardsML <- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards)
# Mean bias-reduced fit:
lizardsBR <- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards,
                 method = brglm2::brglmFit)
tidy(lizardsML)
tidy(lizardsBR)
```


# References

