---
title: 'Logistic Regression'
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
bibliography: "../data.bib"
---

```{r setup, include=FALSE, message=FALSE}
set.seed(1)
library(latex2exp)
library(tidyverse)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Logistic Regression for Binary Response Variables
- Chapters 20 and 21 of the *Statistical Sleuth*

```{r, message = FALSE}
library(tidyverse)
library(Sleuth3)
library(broom)
library(sandwich)
library(lmtest)
```

# Motivation

- Lots of regression problems involve response variables that are **binary** --- the only possible values are either 0 or 1.
    - Alive (1) vs dead (0)
    - Success (1) vs failure (0)
    - Presence (1) vs absence (0)
    
- It is arbitrary and unimportant which variable you encode as 1 vs 0. 
    - E.g. we could have coded "dead" as 1 and "alive" as 0.
    - Just keep track of the coding for interpretation purposes.
    
- E.g., either survival ($Y_i = 1$) or death ($Y_i = 0$) of individual $i$ from the Donner Party of 1846.

    ```{r}
    data("case2001")
    donner <- case2001
    glimpse(donner)
    ```
    
- We often want to quantify the association between a quantitative predictor $x$ and the binary response $y$. E.g. older folks tended to die at a higher frequency:
    ```{r}
    qplot(x = Age, y = Status, data = donner, geom = "point")
    ```

# Linear Probability Model (LPM)

- If your goal is *not* prediction, then it is not entirely incorrect to just use linear regression [@hellevik2009linear; @gomila2021logistic].
    
- Let $Y_i \sim Bern(p)$. That is $Y_i$ is 1 with probability $p$ and is 0 with probability $1-p$. Then

    - $E[Y_i] = p$
    - $var(Y_i) = p(1-p)$
    
- Suppose $Y_i$ is 1 for "success" and "0" for failure. Suppose we have the model

    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1} + \epsilon_i.
    $$
    
- Since $E[Y_i]$ is the probability that $Y_i$ is 1, that mean's we can interpret the regression line as the probability that $Y_i$ is 1.

- Interpreting the regression coefficients: "Individuals that are 1 unit larger in $X_{ik}$ are $\beta_k \times 100$ percentage points more probable to have a $Y$ value of 1, adjusting for other variables."

- Let's apply this to the Donner party data

    ```{r}
    donner <- mutate(donner, Survived = if_else(Status == "Survived", 1, 0))
    lmout <- lm(Survived ~ Age, data = donner)
    lmout
    ```

- Thus, individuals that were one year older were 1.3 percentage points less likely to survive.
    
- Bernoulli random variables are always heteroscedastic since their variance is a function of the mean $p(1-p)$. So if you use the LPM, you *need* to use heteroscedastic robust standard errors.

    ```{r}
    cout <- coeftest(x = lmout, vcov. = vcovHC)
    tidy(cout)
    ```
    
- Residual plots are kind of useless here:

    ```{r}
    aout <- augment(lmout)
    qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
    ```


## Issues

- Econometricians and psychologists love the LPM because of its ease of interpretability.

- Statisticians typically do not like it (though I'm agnostic).

- The possible values of $\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1}$ are any real numbers between $-\infty$ and $\infty$, but the probability of $Y_i$ being 1 can only be between 0 and 1.

- So it is possible (and typical in many datasets) to have probability estimates that are negative or greater than 1. This makes folks squeamish.

- E.g., the point-wise confidence bands from the Donner party make no sense:

    ```{r}
    ggplot(donner, aes(x = Age, y = Survived)) +
      geom_point() +
      geom_smooth(method = "lm", formula = y ~ x)
    ```

- When the probability of a "1" is between about 0.2 and 0.8, then the linear and logistic (see below) models produce about the same results.

- Let's look at the differences in the Donner party example:

    ```{r}
    ggplot(donner, aes(x = Age, y = Survived)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
      geom_smooth(method = "glm", 
                  se = FALSE,
                  method.args = list(family = "binomial"),
                  color = "red", 
                  lty = 2,
                  formula = y ~ x)
    ```

# Model

- Let $p_i$ be the probability that individual $i$ is 1. I.e. $E[Y_i] = p_i$. Then our model is
    $$
    \text{logit}(p_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots \beta_{p-1}X_{i,p-1}
    $$

- Here, $\text{logit}(\cdot)$ is the "logit" function,
    $$
    \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right).
    $$

- The inverse of the logit function is the "expit" function (or "logistic")
    $$
    \text{expit}(\eta) = \frac{e^{\eta}}{1 + e^{\eta}}.
    $$
    
- So we can equivalently write this model as
    $$
    p_i = \text{expit}\left(\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots \beta_{p-1}X_{i,p-1}\right)
    $$

- The $\text{expit}(\cdot)$ function takes a number from $-\infty$ to $\infty$ and places it between 0 and 1. Thus, it forces the probabilities to be between 0 and 1.

    ```{r, echo = FALSE}
    tibble(x = seq(-5, 5, length.out = 200)) %>%
      mutate(p = exp(x) / (1 + exp(x))) %>%
      ggplot(aes(x = x, y = p)) +
      geom_line() +
      ylab("expit(x)")
    ```


## Generalized Linear Model

- This is an example of a "generalized linear model". Let $Y_i$ follow any distribution we specify Let $\mu_i$ be the mean of $Y_i$. Then a generalized linear model is

    $$
    g(\mu_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots \beta_{p-1}X_{i,p-1}
    $$
    
- Here, $g(\cdot)$ is called the "link function". The model is linear on the link scale (hence "generalized linear model").

- In logistic regression, $Y_i \sim Bern(\mu_i)$ and $g(\mu_i) = \text{logit}(\mu_i)$.

- Another example is probit regression (more popular in econ) where $g(\mu_i) = \Phi^{-1}(\mu_i)$, the standard normal quantile function (`qnorm()`). This gives you similar results, but statisicians don't like it because it's less interpretable than the logit.

    ```{r, echo = FALSE, fig.width = 5}
    tibble(x = seq(-5, 5, length.out = 200)) %>%
      mutate(probit = pnorm(x),
             logit = exp(x) / (1 + exp(x))) %>%
      gather(-x, key = "Link Function", value = "g") %>%
      ggplot(aes(x = x, y = g, color = `Link Function`)) +
      geom_line() +
      ylab("g(x)")
    ```
    
- In [log-linear models](https://en.wikipedia.org/wiki/Poisson_regression) (which we might later), we model counts with $Y_i \sim Poi(\mu_i)$ ([Poisson Distribution](https://en.wikipedia.org/wiki/Poisson_distribution)) and $g(\mu_i) = \log(\mu_i)$.

# Estimation

## Maximum Likelihood Estimation

- We don't use least squares for estimation. Rather, we use Maximum Likelihood.

- The probability that $Y_i = 1$ is $p_i$ and the probability that $Y_i = 0$ is $1 - p_i$. 

- Let $y_i \in \{0,1\}$ be the observed value of the random variable $Y_i$. Then we can succinctly write the probabilities of $Y_i$ as
    $$
    Pr(Y_i = y_i) = p_i^{y_i}(1-p_i)^{1 - y_i}
    $$
    Go ahead and plug in $y_i = 0$ and $y_i = 1$ to see this.

- So the probability of our data given $p_i$ is
    $$
    Pr(Y_1 = y_1, Y_2 = y_2, \ldots, Y_n = y_n) = \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}
    $$
    
- Recall that
    $$
    \text{logit}(p_i) = \beta_0 + \beta_1X_{i1} + \cdots \beta_{p-1}X_{i,p-1}.
    $$
    
- So the probability of the data is a function of $\beta_0,\beta_1,\ldots,\beta_{p-1}$.

- IDEA: Choose the $\beta_k$'s that make our observed data as probable as possible.

- The objective function is called the "likelihood".

- The resulting estimates are called the "maximum likelihood estimates" (MLE's).

- Typically, we actually maximize the log of the likelihood

$$
L(\mathbf{\beta}) = \sum_{i=1}^n [y_i\log(p_i) + (1-y_i)\log(1-p_i)]
$$


```{r, eval = FALSE, echo = FALSE}
library(ggrepel)
library(latex2exp)
beta0_seq <- seq(10, 1.8185, length.out = 20)
beta1_seq <- seq(-0.3, -0.0665, length.out = 20)
expit <- function(x) {
  exp(x) / (1 + exp(x))
}
df <- tibble(Age = seq(min(donner$Age), max(donner$Age), length.out = 200))
plist <- list()
for (i in seq_along(beta1_seq)) {
  beta0 <- beta0_seq[[i]]
  beta1 <- beta1_seq[[i]]
  
  df %>%
    mutate(Survived = expit(beta0 + beta1 * Age)) ->
    df
  
  donner %>%
    mutate(pi = expit(beta0 + beta1 * Age),
           pi = if_else(Survived == 1, pi, 1 - pi)) ->
    dontemp
  
  ll <- sum(log(dontemp$pi))
  
  dontemp %>%
    mutate(pi = round(pi, digits = 2)) %>%
    distinct(Age, Survived, .keep_all = TRUE) ->
    dontemp
  
  ggplot(dontemp, aes(x = Age, y = Survived, label = pi)) +
    geom_point() +
    geom_text_repel(min.segment.length = 0) +
    geom_line(data = df) +
    ggtitle(TeX(paste0("LL = $\\sum_{i=1}^n [y_i\\log(p_i) + (1-y_i)\\log(1-p_i)] = $", round(ll, digits = 2)))) ->
    plist[[i]]
}

saveGIF(expr = {
  for (i in seq_along(plist)) {
    print(plist[[i]])
  }
}, movie.name = "./06_figs/logistic_mle.gif", interval = 1)
```

- Below, the expit curve is morphing according to different values of $\beta_0$ and $\beta_1$. The text by the points is the probability of the those observed points, which in this case is just the height (or one minus the height) of the expit function. The likelihood is the product of those probabilities, and the log-likelihood is the sum of the log of those probabilities.

  ![](./06_figs/logistic_mle.gif)\ 

## Practical Use in R

- Let $Y_i$ be an indicator for survival for individual $i$, let $X_{i1}$ be the age of individual $i$, and let $X_{i2}$ be an indicator for male Then we fit the model
    \begin{align}
    Y_i &\sim Bern(p_i)\\
    \text{logit}(p_i) &= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2}
    \end{align}
    
- We fit all generalized linear models in R with `glm()`. The arguments are
    - `formula`: The same idea as in `lm()`. Response variable on the left of the tilde, explanatory variables on the right of the tilde.
    - `data`: The data frame where the variables are.
    - `family`: The distribution of $Y_i$. 
        - `gaussian` will assume normal errors (like in `lm()`).
        - `binomial` in this case will fit the bernoulli model since $Bern(p_i) = Bin(1, p_i)$.
    - You can specify the link function here as well. So for probit models you would do `binomial(link = "probit")`.
    
- Let's fit the model
    ```{r}
    glm_out <- glm(Survived ~ Age + Sex, data = donner, family = binomial)
    ```

- You again use `tidy()` from the `{broom}` package to get estimates / standard errors

    ```{r}
    tidy(glm_out)
    ```
    
- So the estimated regression surface is
    $$
    \text{logit}(p_i) = 3.23 -0.08X_{1} - 1.60X_{2}
    $$
    
- **Exercise**: What if $X_2$ would have been an indicator for female? What would the coefficient estimate of $\beta_2$ be?

    ```{block, eval = FALSE, echo = FALSE}
    1.60 instead of -1.60
    ```

# Interpretation

## Odds

- The interpretation of the logistic regression model requires you to be familiar with odds.

- As I write this, the Baltimore Ravens have 3 to 2 odds of defeating the Miami Dolphins tomorrow. This means that the odds of a Ravens victory are $3/2 = 1.5$.

- The odds of an event are the probability of that event divided by the probability of not that event.

- Let $p$ be the probability of an event, and let $\omega$ be the odds of the same event. Then we have the relations:
    $$
    \omega = \frac{p}{1-p}\\
    p = \frac{\omega}{1+\omega}
    $$

- So odds are just another description of probabilities. 

- In the football example above, the Ravens' probability of wining is 0.6, so the odds are 0.6/0.4 = 3/2 = 1.5.

- **Exercise**: The Chiefs have 2 to 1 odds over the Chargers tomorrow. What's the probability of a Chief's win?

    ```{block, eval = FALSE, echo = FALSE}
    2/3
    ```

## Interpreting Logistic Regression Model

- Let $\omega_{old}$ be the odds of an individual. We can write the logistic regression model as
    $$
    \omega_{old} = \exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} + \cdots \beta_{p-1}X_{p-1}\right)
    $$
    
- Suppose that a different individual has the exact same predictor values except one unit higher $X_1$. Then
    \begin{align}
    \omega_{new} &= \exp\left(\beta_0 + \beta_1(X_{1} + 1) + \beta_2X_{2} + \cdots \beta_{p-1}X_{p-1}\right)\\
    &= \exp\left(\beta_0 + \beta_1X_{1} + \beta_1 + \beta_2X_{2} + \cdots \beta_{p-1}X_{p-1}\right)\\
    &= \exp(\beta_1)\exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} + \cdots \beta_{p-1}X_{p-1}\right)\\
    &= \exp(\beta_1)\omega_{old}
    \end{align}
    
- Thus, the odds of the second individual are $e^{\beta_1}$ times as large as the odds of the first individual.

- Different way to say this: The odds ratio for the two individuals is $e^{\beta_1}$.

- Recall the estimated regression relationship from the Donner party example:
    $$
    \text{logit}(p_i) = 3.23 -0.08X_{1} - 1.60X_{2}
    $$
    
- So individuals of the same sex that are 10 years younger have about twice the odds of surviving ($e^{0.08} \times 10 = 2.186$). Also, a woman's odds of survival were about 5 times that of a man of the same age ($e^{1.6} = 4.95$).

- **Exercise**: The nocturia data set, described [here](https://dcgerard.github.io/stat_415_615/data.html#Nocturia) and dowloadable from [here](https://dcgerard.github.io/stat_415_615/data/nocturia.csv), contains patient covariates that are believed to be associated with whether the individual has nocturia (wakes up to pee). Download the data, the fit of logistic regression model of nocturia on age. Interpret the resulting coefficients.

    ```{r, eval = FALSE, echo = FALSE}
    nocturia <- read_csv("https://dcgerard.github.io/stat_415_615/data/nocturia.csv")
    qplot(x = age, y = nocturia, data = nocturia, geom = "jitter", width = 0, height = 0.05) +
      geom_smooth(method = "glm", se = FALSE, method.args = list(family = binomial))
    
    glmout <- glm(nocturia ~ age, data = nocturia, family = binomial)
    tout <- tidy(glmout)
    tout
    exp(tout$estimate[[2]] * 10)
    ```
    
    ```{block, eval = FALSE, echo = FALSE}
    Individuals 10 years older have about 1.6 times the odds of having nocturia.
    ```

# Retrospective studies

- In a **prospective** study, the explanatory variables are fixed and we imagine $Y_i$ being sampled given $X_i$.
    - E.g. we might choose $200$ patients and observe their cancer status.

- In a **retrospective** study, we fix $Y_i$ and later observe their $X_i$. 
    - E.g. we might choose $n = 100$ cancer patients and $n = 100$ controls.
    - This is typically done with the probability of one group is really small and you want more samples of that group.
    
- The LPM model (and probit model) are **not** valid in retrospective studies, but the logistic regression model **is** valid. So in retrospective studies you **must** use logistic regression.

- The reason is that the odds ratios for prospective studies and retrospective studies are the same, even though the probabilities are not.

- You can interpret slopes in the usual way in retrospective studies, but you cannot interpret the $Y$-intercept, or **any** fixed estimate of the probability. In these studies, **only** the slope (the odds ratio) is interpretable.

# Inference

- (Essentially) all maximum likelihood estimates enjoy good properties

1. Mostly unbiased.
2. Equations for standard deviation calculations are available.
3. Sampling distribution of MLE's is approximately normal for moderate to large sample sizes.

- That means that we can take


# Bias reduction

- For small sample sizes, the MLE's of the logistic regression coefficients can be biased. A quick way to reduce this bias is through the [`{brglm2}`](https://cran.r-project.org/package=brglm2) package, which uses the method of @firth1993bias.

Their example:

```{r, warning=FALSE}
## The lizards example from ?brglm::brglm
data("lizards", package = "brglm2")
# Fit the model using maximum likelihood
lizardsML <- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards)
# Mean bias-reduced fit:
lizardsBR <- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards,
                 method = brglm2::brglmFit)
tidy(lizardsML)
tidy(lizardsBR)
```


# References

