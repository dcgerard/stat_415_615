---
title: 'Diagnostics and Remedial Measures'
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
urlcolor: "blue"
---

```{r setup, include=FALSE}
set.seed(1)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Chapter 3 of KNNL (skip Sections 3.4--3.6 and the Box-Cox transformation). 
- Interpreting residuals plots.
- Diagnosing violations in the assumptions of the simple linear model.
- Suggesting solutions to remedy the violations.

# Recall: Assumptions in Decreasing Order of Importance

1. **Linearity** - Does the relationship look like a straight line?

2. **Independence** - knowledge of the value of one observation does not give you any information on the value of another.

3. **Equal Variance** - The spread is the same for every value of $x$

4. **Normality** - The distribution isn't too skewed and there aren't any too extreme points. (only an issue if you have outliers and a small number of observations because of the CLT, or if you are doing prediction intervals).

# Problems when Violated

1. **Linearity** - Linear regression line does not pick up actual relationship

2. **Independence** - Linear regression line is unbiased, but standard errors are off.

3. **Equal Variance** - Linear regression line is unbiased, but standard errors are off.

4. **Normality** - Unstable results if outliers are present and sample size is small.

## Assessment Tools: Scatterplots and Residual Plots

- Make a scatterplot of the explanatory variable ($x$-axis) vs the response ($y$-axis) to check for non-linearity, equal variance, and normality violations.

- Residuals ($y$-axis) vs fitted values ($x$-axis) is often more clear because the signal is removed.

- You should get used to residual plots, because they are necessary when we start doing multiple linear regression 
    - $x$ versus $y$ does not work well as a diagnostic device when you have many $x$'s.

# Dataset 1: Gold Standard

## Dataset 1: Scatterplot

```{r, message = FALSE, echo = TRUE}
library(tidyverse)
library(broom)
x <- runif(100, -3, 3)
y <- x + rnorm(100)
df <- data.frame(x = x, y = y)
```


```{r, echo = TRUE, message=FALSE}
qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
```

## Dataset 1: Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

## Dataset 1: Summary

- Means are straight lines

- Residuals seem to be centered at 0 for all $x$

- Variance looks equal for all $x$

- Everything looks perfect

- **Exercise**: Describe any issues with this plot.
    ```{r, echo = FALSE, message = FALSE}
    x <- c(runif(30, -3, -2), rep(0, 10), runif(30, 2, 3))
    y <- x + rnorm(length(x))
    df <- data.frame(x = x, y = y)
    qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
    ```
    
    ```{block, eval = FALSE, echo = FALSE}
    This plot still looks good. Remember, we have no distributional assumptions on the predictors. We are just looking as spread along columns of the predictors.
    ```

# Dataset 2: Curved Monotone Relationship, Equal Variances

## Dataset 2: Scatterplot

```{r, echo = TRUE}
x <- runif(100, 0, 6)
x <- x - min(x) + 0.5
y <- log(x) * 20 + rnorm(100, sd = 4)
df <- data.frame(x = x, y = y)
```


```{r, echo = TRUE, message=FALSE}
qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
```

## Dataset 2: Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

## Dataset 2: Summary

- Curved (but always increasing *or* always decreasing) relationship between $x$ and $y$.

- Variance looks equal for all $x$

- Residual plot has a parabolic shape.

- These indicate a $\log$ transformation of $x$ could help.

- Why not $\log(y)$? Because taking transforming $y$ can change the variance, and we already have constant variance, so we do not want to mess with that.
    - Recall, random variation occurs in the $y$ direction, not the $x$ direction.
    
- **Exercise**: Consider the following data:
    ```{r}
    dftoy <- tribble(~x, ~y,
                     1, 1, 
                     1, 2, 
                     2, 1, 
                     2, 5,
                     3, 1,
                     3, 20)
    ```
    Plot $x$ versus $y$. Then log-transform $x$ and plot $\log(x)$ versus $y$. Then try log-transforming $y$ and plotting $x$ versus $\log(y)$. Discuss how variation changes (vertically or horizontally) when we log $x$ versus logging $y$.
    
    ```{r, echo = FALSE, eval = FALSE}
    qplot(x = x, y = y, data = dftoy)
    qplot(x = log(x), y = y, data = dftoy)
    qplot(x = x, y = log(y), data = dftoy)
    ```
    
    ```{block, eval = FALSE, echo = FALSE}
    Logging $x$ changes the variation in $x$. Logging $y$ changes the variation in $y$.
    ```

## Dataset 2: Transformed $x$ Scatterplot

```{r, echo = TRUE, message=FALSE}
df <- mutate(df, x_log = log(x))
qplot(x = x_log, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
```

## Dataset 2: Transformed $x$ Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x_log, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

# Dataset 3: Curved Non-monotone Relationship, Equal Variances

## Dataset 3: Scatterplot

```{r}
x <- runif(100, -3, 3)
y <- -x^2 + rnorm(100)
df <- data.frame(x = x, y = y)
```


```{r, echo = TRUE, message=FALSE}
qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
```


## Dataset 3: Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

## Dataset 3: Summary

- Curved relationship between $x$ and $y$

- Sometimes the relationship is increasing, sometimes it is decreasing.

- Variance looks equal for all $x$

- Residual plot has a parabolic form.

## Dataset 3: Solution

- Fit model:

    $$
    E[Y_i] = \beta_0 + \beta_1 X_i + \beta_2 X_i^2
    $$

- A more complicated solution (which we will not cover) would be to fit model
    $$
    Y_i = 
    \begin{cases}
    \beta_0 + \beta_1 X_i & \text{ if } X_i < C\\
    \beta_0^* + \beta_1^* X_i & \text{ if } X_i > C\\
    \end{cases}
    $$

## Dataset 3: Fitting $E[Y_i] = \beta_0 + \beta_1 X_i + \beta_2 X_i^2$

```{r, echo = TRUE}
df <- mutate(df, x2 = x^2) ## create x^2 first
quad_lm <- lm(y ~ x + x2, data = df) ## lm of x^2 + x
aout <- augment(quad_lm)
qplot(x = x, y = y, data = aout) +
  geom_line(mapping = aes(x = x, y = .fitted), col = "blue", lwd = 1)
```

## Dataset 3: Solution 1 Residuals

```{r, echo = TRUE}
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

## Dataset 3: Peak estimation
```{r}
ttemp <- tidy(quad_lm)
beta0 <- ttemp$estimate[[1]]
beta1 <- ttemp$estimate[[2]]
beta2 <- ttemp$estimate[[3]]
```


- Recall that $y = \beta_0 + \beta_1x + \beta_2x^2$ is the equation of a parabola.

- The estimated parabola is $y = `r beta0` + `r beta1` x + `r beta2` x^2$
    

- We can get these coefficient estimates by running
    ```{r}
    tidy(quad_lm)
    ```

- The negative coefficient for $x^2$ indicates that the parabola has a maximum, which we can recall occurs at
    $$
    x = -\frac{\beta_1}{2\beta_2} = -\frac{`r beta1`}{2 \times `r beta2`} = `r -beta1 / (2 * beta2)`
    $$
    So we could say that the trend increases to about 0, then decreases after 0.

# Dataset 4: Curved Monotone Relationship, Variance Increases with $Y$

## Dataset 4: Scatterplot

```{r, echo = TRUE}
x <- runif(100, 0, 2)
y <- exp(x + rnorm(100, sd = 1/2))
df <- data.frame(x = x, y = y)
```

```{r, echo = TRUE, message=FALSE}
qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
```


## Dataset 4: Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```


## Dataset 4: Summary

- Curved relationship between $x$ and $y$

- Variance looks like it increases as $y$ increases

- Residual plot has a parabolic form.

- Residual plot variance looks larger to the right and smaller to the left.

## Dataset 4: Solution

- Take a log-transformation of $y$.

    ```{r, echo = TRUE, message = FALSE}
    df <- mutate(df, y_log = log(y))
    qplot(x = x, y = y_log, data = df) + geom_smooth(method = "lm", se = FALSE)
    ```

## Dataset 4: Solution

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y_log ~ x, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

- **Exercise**: What if you see something like this? Do you think logging $y$ will help?
    ```{r, echo = FALSE}
    x <- runif(100, 0, 2)
    y <- exp(-x + rnorm(100, sd = 1/2))
    df <- data.frame(x = x, y = y)
    qplot(x = x, y = y, data = df)
    ```

    ```{block, eval = FALSE, echo = FALSE}
    Yes. What's important is that we have increasing variance at higher $y$ and that it is curved. You can try it out!
    ```
    
    ```{r, echo = FALSE, eval = FALSE}
    qplot(x = x, y = log(y), data = df)
    ```

# Dataset 5: Linear Relationship, Equal Variances, Skewed Distribution

## Dataset 5: Scatterplot

```{r}
x <- runif(200)
y <- 15 * x + rexp(200, 0.2)
df <- data.frame(x = x, y = y)
```

```{r, echo = TRUE, message = FALSE}
qplot(x, y, data = df) + geom_smooth(method = "lm", se = FALSE)
```

## Dataset 5: Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

## Dataset 5: Summary

- Straight line relationship between $x$ and $y$.

- Variances about equal for all $x$

- Skew for all $x$

- Residual plots show skew.

## Dataset 5: Solution

- Do nothing, but report skew (usually OK to do)

- Be fancy, fit quantile regression:

$$
Median(Y_i) = \beta_0 + \beta_1 X_i
$$

- Be fancy, run a bootstrap (maybe we'll talk about this later).

# Dataset 6: Linear Relationship, Unequal Variances

## Dataset 6: Scatterplot

```{r}
x <- runif(100)
y <- x + rnorm(100, sd = (x + 0.3)^2 / 2)
df <- data.frame(x = x, y = y)
```

```{r, echo = TRUE, message=FALSE}
qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
```

## Dataset 6: Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = lmout) + geom_hline(yintercept = 0)
```

## Dataset 6: Summary

- Linear relationship between $x$ and $y$.

- Variance is different for different values of $x$. This is called **heteroscedasticity**.

- Residual plots really good at showing this.

## Dataset 6: Solution

- You can try logging both $x$ and $y$, sometimes that works. But that won't work here. Be careful about negative values.
    ```{r}
    df <- mutate(df, y_log = log(y + 0.5),
                 x_log = log(x))
    qplot(x = x_log, y = y_log, data = df)
    ```


- The book will suggest weighted least squares (with weights inverse to the variance).

- But the modern solution is to use **sandwich** estimates of the standard errors.

    ```{r, echo = TRUE, message = FALSE}
    library(lmtest)
    library(sandwich)
    cout <- coeftest(x = lmout, vcov. = vcovHC(x = lmout))
    tidy(cout, conf.int = TRUE)
    ```
    
- `vcovHC()` stands for "Heteroscedastic-consistent variance/covariance".

- Compare to old standard errors
    ```{r}
    tidy(lmout, conf.int = TRUE)
    ```


## Intuition of Sandwich Estimator of Variance

- Simplified Model: $Y_i = \beta_1 x_i$ (so zero intercept)

- Using Calculus: $\hat{\beta}_1 = \frac{\sum_{i = 1}^n x_i y_i}{\sum_{i = 1}^n x_i^2}$\pause

So
\begin{align*}
Var(\hat{\beta}_1) &= Var\left(\frac{\sum_{i = 1}^n x_i y_i}{\sum_{i = 1}^n x_i^2}\right)\\
&=\frac{\sum_{i = 1}^n x_i^2 Var(y_i|x_i)}{\left(\sum_{i = 1}^n x_i^2\right)^2}
\end{align*}\pause

- Usual Method: Estimate $Var(y_i|x_i) = \sigma^2$ with the MSE. 

    - Assumes variance estimate is same for all $i$

- Sandwich Method: Estimate $Var(y_i|x_i)$ with $(y_i - \hat{\beta}_1x_i)^2$ 

    - Allows variance estimate to differ at each $i$

## Notes on Sandwich

- They result in accurate standard errors of the coefficient estimates as long as

    1. The linearity assumption is satisfied.
    
    2. Independence is satisfied
    
    3. You have a large enough sample size.
    
- You cannot use them for prediction intervals

- We will talk more about the sandwich estimator later.

# Dataset 7: Outlying observations

```{r}
x <- runif(100, -3, 3)
y <- x + rnorm(100)
x[[100]] <- 2.6
y[[100]] <- 10
df <- data.frame(x = x, y = y)
```

```{r, echo = TRUE, message=FALSE}
qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
```

## Dataset 7: Residual Plot

```{r, echo = TRUE, message=FALSE}
lmout <- lm(y ~ x, data = df)
aout <- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)
```

- So we see one observation very high in residual plot compared to the other points around that area.

## Dataset 7: Solution

- It's generally a bad idea to discard outlying observations unless you know that this is a processing error or a calculation mistake.

- Generally the procedure to dealing with outliers is:
  1. Detect influential points.
  2. Verify that the outliers are not caused by numerical errors.
  3. Fit the model both with and without the outliers. If the results do not change, report both results.
  4. If the results change, try fitting a robust approach.

- Some robust approaches are described in Section 11.3 of KNNL. One possible approach is quantile regression, which assumes the model
    $$
    Median(Y_i) = \beta_0 + \beta_1X_i
    $$

    ```{r, message = FALSE}
    library(quantreg)
    rqout <- rq(y ~ x, data = df)
    t_rq <- tidy(rqout, conf.int = TRUE)
    t_rq
    ```
    
- We have a large sample size, so the results do not change much.

    ```{r}
    tidy(lmout, conf.int = TRUE)
    ```

- Chapter 10 contains more details on evaluating the influence of outliers.

# Dataset 8: Sequence plots of residuals to check independence

- Sometimes, the residuals can exhibit **autocorrelation** where residuals close to each other in time (or a sequence) are more similar than those further away.

    ```{r}
    x <- sort(runif(100, -3, 3))
    epsilon <- rep(NA_real_, length.out = length(x))
    epsilon[[1]] <- 0
    rho <- 0.9
    for (i in 2:length(epsilon)) {
      epsilon[[i]] <- 0.85 * epsilon[[i-1]] + rnorm(1)
    }
    y <- x + epsilon
    df <- data.frame(x = x, y = y)
    ```
    
    ```{r, message = FALSE}
    qplot(x = x, y = y, data = df) + geom_smooth(method = "lm", se = FALSE)
    ```

    ```{r}
    lmout <- lm(y ~ x, data = df)
    aout <- augment(lmout)
    aout <- mutate(aout, index = row_number())
    qplot(x = index, y = .resid, data = aout) + geom_hline(yintercept = 0)
    ```
    
- This is a specific issue with the **independence** assumption.

- The OLS fit is still unbiased, but the standard errors will be too small (because we have less information than we think).

## Dataset 8: Solution

- This is discussed in detail in Chapter 12 of KNNL.

- You should first check if including omitted variables helps remove autocorrelation.
    - E.g. plotting annual sales versus average price over time, if you are missing population size, then adjacent years probably have more similar errors.

- You can try to estimate autocorrelation directly (Section 12.4 of KNNL).

- You can also adjust the OLS standard errors using a similar approach to sandwich estimation. This is called the "Newey-West HAC" (Heteroskedasticity- and autocorrelation-consistent) estimate of the standard error.

    ```{r, echo = TRUE, message = FALSE}
    library(lmtest)
    library(sandwich)
    cout <- coeftest(x = lmout, vcov. = vcovHAC(x = lmout))
    tidy(cout, conf.int = TRUE)
    ```
    
    - Notice we used `vcovHAC()` instead of `vcovHC()`.

- Compare to original standard errors
    ```{r}
    tidy(lmout, conf.int = TRUE)
    ```

- You should have large sample sizes for accurate inference ($n\geq 100$) for estimating the autocorrelation or adjusting the standard errors.

# Quantile-Quantile plots of residuals to check normality

- Plot sample quantile of residual against theoretical quantile of normal distribution.

- **Quantile**: The $q$th quantile of a variable is the value at with $q$ proportion of the observations fall.

- Sample 0.25-quantile

    ```{r, echo = FALSE, warning = FALSE}
    data.frame(x = rnorm(1000)) %>%
      mutate(islow = x < quantile(x = x, probs = 0.25)) ->
      df
    x25 <- quantile(df$x, probs = 0.25)
    ggplot(df, aes(x = x, fill = islow)) +
      geom_histogram(bins = 25, color = "black") +
      scale_fill_manual(values = c("white", "blue")) +
      geom_vline(xintercept = x25, lty = 2, col = 2, lwd = 2) +
      theme(legend.position = "none") +
      xlim(-3, 3)
    ```

- Theoretical 0.25-quantile

    ```{r, echo = FALSE}
    data.frame(x = seq(-3, 3, length.out = 500)) %>%
      mutate(y = dnorm(x)) ->
      df
    sdf <- filter(df, x < qnorm(0.25))
    ggplot() +
      geom_line(data = df, mapping = aes(x = x, y = y)) +
      geom_ribbon(data = sdf, mapping = aes(x = x, ymin = 0, ymax = y), fill = "blue") +
      geom_vline(xintercept = qnorm(0.25), lty = 2, col = 2, lwd = 2)
    ```
    
- If the residuals were approximately normal, then we would expect the 0.25 quantile of the data to be about the 0.25 quantile of the normal, the 0.7 quantile of the data to be about the 0.7 quantile of the normal, etc.

- We can graphically compare the quantiles by making a scatterplot of the sample quantiles to the theoretical quantiles. This is called a **QQ-plot** (for quantile-quantile).
  
- We usually place the theoretical quantiles on the $x$-axis and the sample quantiles on the $y$-axis.

- Constructing a QQ-plot

  ![](./03_figs/qqplot.gif)\ 
  
- QQ-plots Can show us deviations from normality
  
  ![](./03_figs/bad_qqplot.gif)\ 
  
- QQ-plot indicating right skew.
    ```{r, echo = FALSE}
    x <- rexp(100)
    qplot(sample = x, geom = "qq") +
      geom_qq_line() +
      xlab("Theoretical") +
      ylab("Sample")
    ```

- QQ-plot indicating left skew.
    ```{r, echo = FALSE}
    x <- -rexp(100)
    qplot(sample = x, geom = "qq") +
      geom_qq_line() +
      xlab("Theoretical") +
      ylab("Sample")
    ```
- QQ-plot indicating heavy tails.
    ```{r, echo = FALSE}
    x <- -rt(100, 1)
    qplot(sample = x, geom = "qq") +
      geom_qq_line() +
      xlab("Theoretical") +
      ylab("Sample")
    ```
    
- QQ-plot indicating light tails.
    ```{r, echo = FALSE}
    x <- -runif(100)
    qplot(sample = x, geom = "qq") +
      geom_qq_line() +
      xlab("Theoretical") +
      ylab("Sample")
    ```

  
## QQ-plots in R

- We can evaluate the normality assumption from the bread and peace example.

    ```{r, message = FALSE}
    hibbs <- read_csv("https://dcgerard.github.io/stat_415_615/data/hibbs.csv")
    lmhibbs <- lm(vote ~ growth, data = hibbs)
    ```

- Use `augment()` from the `{broom}` package to obtain the residuals. Then use the `geom = "qq"` argument in the `qplot()` function. Make sure you specify `sample = .resid`, *not* `x = .resid`.

    ```{r}
    aout <- augment(lmhibbs)
    qplot(sample = .resid, data = aout, geom = "qq") +
      geom_qq_line()
    ```
    
- There is a small deviation, but it's not too bad.

## Final notes about normality

- Only worry about normality if you see extreme outliers or other extreme deviations from normality (or if you want prediction intervals).

- Check for normality **last**. Violations of linearity could make it seem like normality is violated.

# Residual plots against omitted predictors

- If you have predictors that you did not use in the model, it is always a good idea to make plots of the residuals against those omitted predictors.

- Consider the Palmer Penguins Data.
    ```{r, message = FALSE, warning = FALSE}
    library(palmerpenguins)
    data("penguins")
    glimpse(penguins)
    lmpen <- lm(body_mass_g ~ bill_length_mm, data = penguins)
    aout <- augment(lmpen)
    penguins %>%
      mutate(.rownames = as.character(row_number())) %>%
      left_join(aout) %>%
      qplot(x = species, y  = .resid, data = ., geom = "boxplot")
    ```
    
- Whenever you see a systematic pattern between the residuals and an omitted predictor (as above), this indicates that you should include that predictor in your model.

# $F$-test for lack of fit

- In many cases, you have repeat observations at the same value of the predictors. If you design a study, it is a good idea to include repeats at the same predictor levels.

- If so, then it is possible to run a **lack-of-fit** test:
    - $H_0: E[Y_i] = \beta_0 + \beta_1 X_i$
    - $H_A: E[Y_i] \neq \beta_0 + \beta_1 X_i$
    
- Rejecting this test indicates that the linear model is not a good fit.

- Failing to reject this test tells us that at least we do not have evidence that the linear model is a bad fit.

- We will consider this example in the context of a experiment run on 11 branches of a bank. Each branch offered a set gift size ($X$) to open a new account. The branches then measured the number of new accounts opened ($Y$).
    ```{r}
    bank <- tibble::tribble(~gift, ~accounts,
                            125, 160,
                            100, 112,
                            200, 124,
                             75,  28,
                            150, 152,
                            175, 156,
                             75,  42,
                            175, 124,
                            125, 150,
                            200, 104,
                            100, 136
                            )
    ```
    
    There are 6 values of gift size, with all but one value having two units.

- The basic idea of the lack-of-fit test is to compare the residuals under the regression model (the reduced model) to the residuals under a "saturated" model where each level of $X$ is allowed to have its own mean.


- Reduced Model: SSE(R)
    ```{r, echo = FALSE}
    lm_r <- lm(accounts ~ gift, data = bank)
    aout <- augment(lm_r)
    ggplot(data = aout) +
      geom_segment(aes(x = gift, xend = gift, y = accounts, yend = .fitted), alpha = 1/4, lwd = 2) +
      geom_point(aes(x = gift, y = accounts)) +
      geom_line(aes(x = gift, y = .fitted), color = "blue", lwd = 2) +
      xlab("Gift Size") +
      ylab("Number of Accounts")
    ```

- Full Model: SSE(F)
    ```{r, echo = FALSE}
    bank %>%
      mutate(gift = as.factor(gift)) %>%
      lm(accounts ~ gift, data = .) ->
      lm_f
    aout <- augment(lm_f)
    
    aout %>%
      mutate(gift = as.numeric(as.character(gift))) ->
      aout
    
    aout %>%
      select(gift, .fitted) %>%
      distinct() %>%
      mutate(gift = as.numeric(as.character(gift)),
             gl = gift - 3,
             gu = gift + 3) ->
      gmean
    
    
    ggplot(data = aout) +
      geom_segment(aes(x = gift, xend = gift, y = accounts, yend = .fitted), alpha = 1/4, lwd = 2) +
      geom_segment(data = gmean, aes(x = gl, xend = gu, y = .fitted, yend = .fitted), color = "blue", lwd = 2) +
      geom_point(aes(x = gift, y = accounts)) +
      xlab("Gift Size") +
      ylab("Number of Accounts")
    ```

- There are 11 observations.

- In the reduced model, there are two parameter ($y$-intercept and slope), so $df_R = 11 - 2 = 9$.

- In the full model, there are six parameters (one for each mean to estimate), so $df_F = 11 - 6 = 5$.

- Use the $F$-statistic
    $$
    F^* = \frac{[SSE(R) - SSE(F)] / (df_R - df_F)}{SSE(F) / df_F}
    $$
    
- Under the reduced model, we have
    $$
    F^* \sim F(df_R - df_F, df_F)
    $$

    So we can compare it to this distribution to obtain a $p$-value.
    
## Lack-of-fit Test in R

- First, fit both the reduced and full models.

- The reduced model we've seen before.
    ```{r}
    lm_r <- lm(accounts ~ gift, data = bank)
    ```

- The full model is fit automatically if you convert `gift` to a factor variable. We'll talk about the specifics of how this works when we get to indicator variables in multiple linear regression.

    ```{r}
    df_full <- mutate(bank, gift_factor = as.factor(gift))
    lm_f <- lm(accounts ~ gift_factor, data = df_full)
    ```

- Then use the `anova()` function to compare these two models.

    ```{r}
    anova(lm_r, lm_f)
    ```

- We can verify this $p$-value by calculating the $F$-statistic manually.

    ```{r}
    resid_r <- augment(lm_r)$.resid
    resid_f <- augment(lm_f)$.resid
    sse_r <- sum(resid_r^2)
    sse_f <- sum(resid_f^2)
    df_r <- nrow(bank) - 2
    df_f <- nrow(bank) - 6
    f_star <- ((sse_r - sse_f) / (df_r - df_f)) / (sse_f / df_f)
    pf(q = f_star, df1 = df_r - df_f, df2 = df_f, lower.tail = FALSE)
    ```

## Math notation

- We will change notation. Let $Y_{ij}$ be the $j$th individual in group $i$. Let $X_i$ be the level of group $i$.

- You might recognize the above "saturated" model as a one-way ANOVA model
    $$
    Y_{ij} \sim N(\mu_i, \sigma^2)
    $$

- We compare the ANOVA model to the "reduced" linear regression model.
    $$
    Y_{ij} \sim N(\beta_0 + \beta_1X_i, \sigma^2)
    $$
    where $X_i$ is the level of group $i$.
    
- This makes it more clear that the regression model is a reduced version of the ANOVA model.

## Lack-of-fit test Summary

- Lack-of-fit $F$-test tests the assumption of linearity.
- Needs multiple observations at different predictor values (but some values can have only one observation).
- Small $p$-values indicate lack-of-fit (so linearity is not a valid assumption).

# Other transformations

- The book suggests other transformations (square root, Box-Cox).

- Square root on the response will compress values more mildly than the log.

- The Box-Cox transformation on the response is really a set of transformations that includes the log and the square root as special cases.

- $1/X$ and $exp(X)$ and $exp(-X)$ and $\sqrt{X}$ could all help improve linearity.

- If your goal is *prediction*, then have at it.

- But if your goal is inference, then you lose all interpretability by using transformations other than $log()$. So think carefully before trying other transformations.
