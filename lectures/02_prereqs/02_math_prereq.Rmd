---
title: "Math Prerequisites"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
---

```{r setup, include=FALSE}
library(latex2exp)
library(broom)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Minimum mathematics required to complete this course.
- Summations/averages
- Equations for lines
- Logarithms/exponentials
- Chapter 3 of [ROS](https://avehtari.github.io/ROS-Examples/)

# Motivation

- Mathematics are the building blocks of linear regression.

- You must be proficient with linear equations and summations to implement and understand linear models.

- Log/exponential transformations are important for the practice and interpretation of many types of linear models.

- Here, we will provide a brief mathematical review.

# Weighted Averages

- Consider the following data on USA, Mexico, and Canada:

    | Label  | Population $N_j$  | Average Age $\bar{y}_j$|
    |--------|-------------------|------------------------|
    | USA    | 310 Million       | 36.8                   |
    | Mexico | 112 Million       | 26.7                   |
    | Canada | 34 Million        | 40.7                   |
    
- What is the average age for these three countries?

- Proportion USA: $\frac{310}{310 + 112 + 34} = 0.68$.

- Proportion Mexico: $\frac{112}{310 + 112 + 34} = 0.25$.

- Proportion Canada: $\frac{34}{310 + 112 + 34} = 0.07$.

- So the USA contributes 68\% of the population, mexico contributes 25\% of the population, and Canada contributes 7\% of the population. To find the overall average age, we calculate:

$$
0.68 \times 36.8 + 0.25 \times 26.7 + 0.07 \times 40.7 = 34.6
$$

- We can equivalently write this as

$$
\frac{310 \times 36.8 + 112 \times 26.7 + 34 \times 40.7}{310 + 112 + 34} = \frac{310}{456}\times 36.8 + \frac{112}{456}\times 26.7 + \frac{34}{456}\times 40.7
$$

- The proportions 0.68, 0.25, and 0.07 are called the **weights**. When the weights sum to one, the overall sumation is called a **weighted average**.

- In **summation notation** (using the capital-sigma), we would write:

    \begin{align}
    \text{weighted average} &= \sum_{j=1}^n w_j y_j\\
    &= w_1y_1 + w_2y_2 + \cdots w_ny_n
    \end{align}
    
    where $w_j$ is the $j$th weight and $y_j$ is the $j$th value.

- What would be an "unweighted" average? This is where each $w_j = \frac{1}{n}$ since

    \begin{align}
    \sum_{j=1}^n w_j y_j &= \sum_{j=1}^n \frac{1}{n} y_j\\
    &= \frac{1}{n}y_1 + \frac{1}{n}y_2 + \cdots + \frac{1}{n}y_n\\
    &= \frac{1}{n}(y_1 +y_2 + \cdots + y_n)\\
    &= \frac{1}{n}\sum_{j=1}^n y_j\\
    &= \bar{y}
    \end{align}
    
- Properties of summations:
    - $\sum_{i=1}^n c a_i = c\sum_{i=1}^na_i$
    - $\sum_{i=1}^n a_i + \sum_{i=1}^nb_i = \sum_{i=1}^n(a_i + b_i)$
    
- **Exercise**: 51\% of Americans are female while 49\% of Americans are male. 79\% of teachers are female while 21\% of teachers are male. Female teachers make on average \$45,865, while male teachers make on average \$49,207. What is the average salary for all teachers?

    ```{block, eval = FALSE, echo = FALSE}
    We need to use the 79\%/21\% split, since we are interested in the population of teachers, not the total population. So we have
    
    0.79 * 45865 + 0.21 * 49207 = 46567
    ```

- **Exercise**: What is $\sum_{i=0}^4 i$?

    ```{block, eval = FALSE, echo = FALSE}
    0 + 1 + 2 + 3 + 4 = 10
    ```

- **Exercise**: Prove that it is not generally true that $\left(\sum_{i=1}^n y_i\right)^2 = \sum_{i=1}^n y_i^2$ (*hint*: provide a counterexample).

    ```{block, eval = FALSE, echo = FALSE}
    $(1 + 2)^2 = 3^2 = 9 \neq 5 = 1^2 + 2^2$
    ```
    
## Products

- We use capital-pi notation to represent product.

    $$
    \prod_{i=1}^n a_i = a_1 \times a_2 \times \cdots \times a_n
    $$

# Lines

- All lines are of the form 
    $$
    y = \beta_0 + \beta_1 x
    $$

- $\beta_1$ is the **slope**, the amount $y$ is larger by when $x$ is 1 unit larger.

- When $\beta_1$ is *negative*, the line slopes *down*.

    ```{r, echo = FALSE, fig.height=3, fig.width=5}
    a <- 0.95
    b <- -0.4
    par(mar=c(3,3,1,1), mgp=c(2,.5,0), tck=-.01)
    plot(c(0,2.2), c(0,a+.2), pch=20, cex=.5, main=TeX("$y = \\beta_0 + \\beta_1 x$ (with $\\beta_1 < 0$)"),
      bty="l", type="n", xlab="x", ylab="y", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
    axis(1, c(0,1,2))
    axis(2, c(a,a+b,a+2*b), c(TeX("$\\beta_0$"),TeX("$\\beta_0+\\beta_1$"),TeX("$\\beta_0+2\\beta_1$")))
    abline(a, b, lwd = 3, col = "blue")
    ```

- When $\beta_1$ is *positive*, the line slopes *up*.

    ```{r, echo = FALSE, fig.height=3, fig.width=5}
    a <- 0.15
    b <- 0.4
    par(mar=c(3,3,1,1), mgp=c(2,.5,0), tck=-.01)
    plot(c(0,2.2), c(0,a+2.2*b), pch=20, cex=.5, main=TeX("$y = \\beta_0 + \\beta_1 x$ (with $\\beta_1 > 0$)"),
      bty="l", type="n", xlab="x", ylab="y", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
    axis(1, c(0,1,2))
    axis(2, c(a,a+b,a+2*b), c(TeX("$\\beta_0$"),TeX("$\\beta_0+\\beta_1$"),TeX("$\\beta_0+2\\beta_1$")))
    abline(a, b, lwd = 3, col = "blue")
    ```

- When $\beta_1$ is *0*, the line is *horizontal*. In this case, $y$ is the same for every value of $x$ (in other words, $x$ does not affect $y$).

    ```{r, echo = FALSE, fig.height=3, fig.width=5}
    a <- 0.15
    b <- 0
    par(mar=c(3,3,1,1), mgp=c(2,.5,0), tck=-.01)
    plot(c(0, 2), c(a, a), pch=20, cex=.5, main=TeX("$y = \\beta_0 + \\beta_1 x$ (with $\\beta_1 = 0$)"),
      bty="l", type="n", xlab="x", ylab="y", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
    axis(1, c(0,1,2))
    axis(2, c(a), c(TeX("$\\beta_0$")))
    abline(a, b, lwd = 3, col = "blue")
    ```
    
- **Example**: The progression of mile world records during the 20th century is well approximated by the line $y = 1007 - 0.393x$

    ```{r, message = FALSE}
    library(readr)
    library(ggplot2)
    mile <- read_csv("https://dcgerard.github.io/stat_415_615/data/mile.csv")
    ggplot(data = mile, mapping = aes(x = year, y = seconds)) +
      geom_point() +
      geom_abline(slope = -0.393, intercept = 1007, lty = 2, col = 2)
    ```
    
- The world record in 1950 was about 
    $$
    y = 1007 - 0.393 * 1950 = 240.6 \text{ seconds},
    $$
    which is actually between the two world record values of 241.4 seconds in 1946 and 239.4 seconds in 1954.
    
- Interpretation of $\beta_0$: Can we interpret 1007 seconds (16.8 minutes) as the approximate world record in ancient times? No! Our data are only from 1913 to 1999, so this is an obviously improper extrapolation. It is just the $y$-intercept, with no other interpretation.

- Interpretation of $\beta_1$: Each year, on average, the world record is 0.393 seconds lower.

- Do *not* say "the world record decreases by about 0.393 seconds each year" as this creates an implicit causal connection.

- **Exercise**: Year versus maximum life expectancy (where maximum was taken over country) is well approximated by the line $y = -296 + 0.189 x$.
    
    ```{r, echo = FALSE, message = FALSE}
    library(gapminder)
    library(tidyverse)
    data("gapminder_unfiltered")
    gapminder_unfiltered %>%
      group_by(year) %>%
      filter(lifeExp == max(lifeExp)) %>%
      select(country, lifeExp, year) %>%
      arrange(year) ->
      sumdat
    ggplot(sumdat, aes(x = year, y = lifeExp)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE) +
      ylab("Maximum Life Expectancy")
    ```
    
    a. What is the expected life expectancy in 1990?
        ```{block, eval = FALSE, echo = FALSE}
        -296 + 0.189 * 1990 = 80.11
        ```
    b. Interpret -296
        ```{block, eval = FALSE, echo = FALSE}
        It is the $y$-intercept
        ```
    c. Interpret 0.189
        ```{block, eval = FALSE, echo = FALSE}
        Each year, on average, life expectancy is 0.189 years higher.
        ```

# Additive/Multiplicative Comparisons

- This semester, we will spend a lot of time talking about differences in one variable associated with differences in another variable.

- We need to know how to properly speak about "differences", or "numerical comparisons".

- Suppose car $A$ has a gas mileage of 30 mpg, and car $B$ has a gas mileage of 20 mpg. Then obviously car $A$ has 10 mpg better gas mileage. This is called an *additive* comparison since we can write it as $A + -B = 10$.

- We can also describe this comparison as *multiplicative*. Here are some common ways to describe multiplicative comparisons.

- $A$ has 50 percent more gas mileage than $B$. 
    - $A = B(1 + 0.5) = B + 0.5B = B + 50\% B$
    - Thus, $A$ is $B$ plus 50 percent of $B$'s value. This is why we say "50 percent more".
    - In general, $A$ is $\frac{A - B}{B} \times 100\%$ larger than $B$
    
- $B$ has 66.7 percent the value of $A$.
    - This is since $B / A = 2/3 \approx 0.667 = 66.7\%$.
    - In general, $B$ has $\frac{B}{A}\times 100\%$ the value of $A$.
    
- $A$ has 150 percent the value of $B$.
    - This is since $A / B = 3/2 = 1.5 = 150\%$
    - In general, $A$ has $\frac{A}{B}\times 100\%$ the value of $B$.
    
- $B$ has 33.3 percent less gas mileage than $A$.
    - $B = A(1 - 1/3) = A - (1/3)A \approx A - 0.333A = A - 33.3\%A$
    - Thus, $B$ is $A$ minus 33.3 percent of $A$'s value. This is why we say "33.3 percent less".
    - In general, $B$ is $\frac{A-B}{A}\times 100\%$ smaller than $A$.
    
- **Exercise**: Suppose John makes \$40,000 a year and Alina makes \$50,000 a year. Provide four different ways to describe the multiplicative comparison between John's and Alina's salaries.

    ```{block, eval = FALSE, echo = FALSE}
    - Alina makes 25\% more money than John.
    - John makes 20\% less money than Alina.
    - John's salary is 80\% that of Alina's.
    - Anlina's salary is 125\% that of John's.
    ```

- Sometimes we have additive comparsons for a variable whose scale is percent.

- For example, candidate $A$ won 40\% of the vote, and candidate $B$ won 30\% of the vote.

- If we want to describe the *additive* comparison between these two candidates, we cannot say "percent" because that would imply a mulitiplicative comparison.

- To describe additive comparisons of a variable whose units are percent, we say **percentage point**. 

- Thus, candidate $A$'s vote share is 10 percentage points higher than candidate $B$'s.

- We can still describe multiplicative comparisons for these variables. E.g. Candidate $A$'s vote share is 33.3 percent higher than candidate $B$'s.

# Logarithms and Exponentials

- Often, we consider linear relationships on the log-scale. So we need to know something about logarithms and exponentials.

- Let's start with exponentials:
    $$
    \exp(x) = e^x = \underbrace{e \times \dots \times e}_{x\, \textrm{times}}
    $$.
    
    ```{r}
    ## Define e
    e <- exp(1)
    e
    
    ## show exp(3) == e * e * e
    exp(3) 
    e * e * e
    ```
    
- Recall that $e$ is [Euler's number](https://en.wikipedia.org/wiki/E_(mathematical_constant)), which is about `r exp(1)`.

- The last equality in the above equation for `exp()` only follows if $x$ is positive integer, but exponentiation can be extended to any real number.

    ```{r}
    exp(1.414)
    ```

- $\log(x)$ is the **natural** logarithm of $x$ (**not** base 10). This is the inverse of exponentiation
    $$
    \log(\exp(x)) = \exp(\log(x)) = x
    $$
    
- You can verify this in R
    ```{r}
    exp(log(31))
    log(exp(31))
    ```
    
- You can also interpret $\log(x)$ as the number of times you have to divide $x$ by $e$ to obtain 1. For example, since you would have to divide $e^4$ by $e$ 4 times to get 1 ($1 = \frac{e^4}{e \times e \times e \times e}$), we have that $\log(e^4) = 4$.

- A useful property of logs/exponents is how it can convert multiplication to summation and vice versa.
    - $\exp(a + b) = \exp(a)\exp(b)$.
    - $\log(ab) = \log(a) + \log(b)$.
    
- Why do we care about exponentials and logarithms? Because many relationships in the real world follow **exponential** or **power** laws.

## Exponential growth/decline

- $\log(y) = \beta_0 + \beta_1 x$ represents **exponential growth** if $\beta_1 > 0$ and **exponential decline** if $\beta_1 < 0$.

- Exponentiating both sides, we get
    $$
    y = e^{\beta_0}e^{\beta_1 x}
    $$
    
- Exponential growth:

    ```{r, echo = FALSE}
    xseq <- seq(0, 4, length.out = 100)
    df <- tibble(x = xseq, y = exp(xseq))
    ggplot(df, aes(x = x, y = y)) +
      geom_line() +
      theme(axis.text = element_blank(),
            axis.ticks = element_blank())
    ```
    
- Exponential decline:

    ```{r, echo = FALSE}
    xseq <- seq(0, 4, length.out = 100)
    df <- tibble(x = xseq, y = exp(-xseq))
    ggplot(df, aes(x = x, y = y)) +
      geom_line() +
      theme(axis.text = element_blank(),
            axis.ticks = element_blank())
    ```

- $e^{\beta_0}$ is the value of $y$ when $x = 0$

- $\beta_1$ determines the rate of growth or decline.

- A 1-unit difference in $x$ corresponds to a multiplicative factor of $e^{\beta_1}$ in $y$. (you multiply the old $y$ value by $e^{\beta_1}$ to figure out the new $y$ value when you have an $x$ value that is 1 larger).
    
- This follows from:
    \begin{align}
    y_{old} &= e^{\beta_0}e^{\beta_1 x}\\
    y_{new} &= e^{\beta_0}e^{\beta_1 (x + 1)}\\
            &= e^{\beta_0}e^{\beta_1 x + \beta_1}\\
            &= e^{\beta_0}e^{\beta_1 x} e^{\beta_1}\\
            &= y_{old}e^{\beta_1}
    \end{align}
    
- **Example**: The early growth of [COVID-19 in DC](https://dcgerard.github.io/stat_415_615/data.html#dc-covid-tests) looked exponential:
    ```{r, message = FALSE}
    dc <- read_csv("https://dcgerard.github.io/stat_415_615/data/dccovid.csv")
    dc <- select(dc, day, positives)
    dc <- filter(dc, day <= "2020-04-01", day >= "2020-03-11") 
    ggplot(data = dc, mapping = aes(x = day, y = positives)) +
      geom_point()
    ```
    
    We determine if an exponential growth model is appropriate by seeing if day versus log-positives is approximately linear:
    
    ```{r}
    dc <- mutate(dc, 
                 lpos = log(positives),
                 daysin = as.numeric(day) - as.numeric(day[[1]]))
    ggplot(data = dc, mapping = aes(x = daysin, y = lpos)) +
      geom_point()
    ```
    
    ```{r, echo = FALSE}
    coefvec <- coef(lm(lpos ~ daysin, data = dc))
    beta0 <- coefvec[[1]]
    beta1 <- coefvec[[2]]
    ```

    A curve that fits this growth well is $y = `r exp(beta0)`e^{`r beta1`x}$.
    
    ```{r, echo = FALSE}
    df <- data.frame(x = seq(min(dc$daysin), max(dc$daysin), length.out = 100))
    df$y <- exp(beta0) * exp(df$x * beta1)
    ggplot() +
      geom_point(data = dc, mapping = aes(x = daysin, y = positives)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```
    
    So that means that early on in the pandemic, each day positive tests were multiplicatively larger by about $e^{\beta_1} = e^{`r beta1`} = `r exp(beta1)`$. Or, about a 24\% larger each day.
    
- **Exercise**: Consider the population growth of DC from 1800 to 1950, taken from [Wikipedia](https://en.wikipedia.org/wiki/Washington,_D.C.#Demographics).
    ```{r}
    dcpop <- tribble(~year,   ~pop,
                     1800L,   8144,
                     1810L,  15471,
                     1820L,  23336,
                     1830L,  30261,
                     1840L,  33745,
                     1850L,  51687,
                     1860L,  75080,
                     1870L, 131700,
                     1880L, 177624,
                     1890L, 230392,
                     1900L, 278718,
                     1910L, 331069,
                     1920L, 437571,
                     1930L, 486869,
                     1940L, 663091,
                     1950L, 802178)
    ggplot(data = dcpop, mapping = aes(x = year, y = pop)) +
      geom_point()
    ```
    ```{r, echo = FALSE}
    coefvec <- coef(lm(log(pop) ~ year, data = dcpop))
    beta0 <- coefvec[[1]]
    beta1 <- coefvec[[2]]
    ```
    
    A researcher has determined that the following relationship approximates this growth well:
    
    $$
    \log(y) = `r beta0` + `r beta1` x
    $$
    
    ```{r, echo = FALSE}
    df <- data.frame(x = seq(min(dcpop$year), max(dcpop$year), length.out = 100))
    df$y <- exp(beta0) * exp(df$x * beta1)
    ggplot() +
      geom_point(data = dcpop, mapping = aes(x = year, y = pop)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```
    

    1. Interpret the `r beta1` value.
    
    ```{block, eval = FALSE, echo = FALSE}
    DC experienced about 3\% growth each year. This is since `exp(0.0289) = 1.03`.
    ```

    2. Interpret the `r beta0` value.

    ```{block, eval = FALSE, echo = FALSE}
    It is the $y$-intercept of the line $\log(y) = \beta_0 + \beta_1x$. It has no other interpretation because the range of values are from 1800 to 1950, and 0 is outside of this range.
    
    So it is **incorrect** to say that `exp(beta0)` is the population at year 0.
    ```
    
    3. What is the average growth every 10 years?
    
    ```{block, echo = FALSE, eval = FALSE}
    `exp(10 * 0.0299)` = 1.349, or about 35\% growth every 10 years.
    ```
    
## Why use $e$ and not some other base?

- Tradition.

- For small values of $\beta_1$ (say, $-0.1 \leq \beta_1 \leq 0.1$), we can interpret $\beta_1$ as the proportion change for a 1 unit difference in $x$.

- This is because for small $\beta_1$, we have $e^{\beta_1} \approx 1 + \beta_1$.

```{r, echo = FALSE}
tibble(beta1 = seq(0, 0.2, length = 11)) %>%
  mutate(`exp(beta1)` = exp(beta1),
         `Percent Change` = (`exp(beta1)` - 1) * 100) %>%
  knitr::kable()
```

- In many real world applications, $\beta_1$ is typically small.

- This relationship does not hold with other bases. E.g. $10^{0.02} = `r 10^0.02` \not\approx 1 + 0.02$.

- **Example**: From the DC population example above, we had $\beta_1 = `r beta1`$ and $e^{\beta_1} = `r exp(beta1)`$. So, a 3\% increase in population each year, and you can get that either from $\beta_1$ or $e^{\beta_1}$.

## Power-law growth/decline

- $\log(y) = \beta_0 + \beta_1 \log(x)$ represents **power-law growth** if $\beta_1 > 0$ and **power-law decline** if $\beta_1 < 0$.
    - Sub-linear growth if $0 < \beta_1 < 1$.
    - Super-linear growth if $\beta_1 > 1$.

- Exponentiating both sides, we get the relationship
    $$
    y = e^{\beta_0}x^{\beta_1}
    $$

- Interpret $\beta_0$: The value of $y$ when $x = 1$ is $e^{\beta_0}$.

- Interpret $\beta_1$: 
    - When you double $x$, you multiply $y$ by $2^{\beta_1}$.
    - When you multiply $x$ by 10, you multiply $y$ by $10^{\beta_1}$.
    - When you multiply $x$ by 1.1 (10\% increase), you multiply $y$ by $1.1^{\beta_1}$.
    - Choose a multiplier that makes sense for the range of your data. E.g., if it is never the case that one value is 10 times larger than another, don't use that as the interpretation.
    
- This follows from:

    \begin{align}
    y_{old} &= e^{\beta_0}x^{\beta_1}\\
    y_{new} &= e^{\beta_0}(2x)^{\beta_1}\\
            &= e^{\beta_0}2^{\beta_1}x^{\beta_1}\\
            &= 2^{\beta_1}e^{\beta_0}x^{\beta_1}\\
            &= 2^{\beta_1}y_{old}
    \end{align}
    
- **Example**: From the `{Sleuth3}` R package, we have a dataset on 7 islands measuring their land area and the number of species on each island. The goal is to estimate the relationship between these two variables, which has applications in conservation.

    ```{r}
    library(Sleuth3)
    data("case0801")
    ggplot(data = case0801, mapping = aes(x = Area, y = Species)) +
      geom_point()
    ```
    
    ```{r, echo = FALSE}
    coefout <- coef(lm(log(Species) ~ log(Area), data = case0801))
    beta0 <- coefout[[1]]
    beta1 <- coefout[[2]]
    ```
    
    - This relationship is well-approximated by the curve $y = `r exp(beta0)`x^{`r beta1`}$.

    ```{r, echo = FALSE}
    df <- data.frame(x = seq(0, max(case0801$Area), length.out = 100))
    df$y <- exp(beta0) * df$x^beta1
    ggplot() +
      geom_point(data = case0801, aes(x = Area, y = Species)) +
      geom_line(data = df, aes(x = x, y = y))
    ```

    - So islands that are twice as large have $2^{\beta_1} = 2^{`r beta1`} = `r 2^beta1`$ times as many species on average. Or, islands that are twice as large have 19\% more species on average.
    
    - Notice that I didn't use the language "change" or "increase", because those would imply causal connections.
    
- **Exercise**: Prove that the interpretation "when you multiply $x$ by 10, you multiply $y$ by $10^{\beta_1}$" is correct.

    ```{block, eval = FALSE, echo = FALSE}
    Just redo the calculations for the interpretation of doubling, but with 10 instead of 2.
    ```

- **Exercise**: Consider the wine consumption and heart disease data from the `{Sleuth3}` package. The observational units are the counties and the variables are
    - `Wine`: consumption of wine (liters per person per year)
    - `Mortality`: heart disease mortality rate (deaths per 1,000)

    ```{r}
    data("ex0823")
    ggplot(data = ex0823, mapping = aes(x = Wine, y = Mortality)) +
      geom_point()
    ```
 
    - Researchers have determined that this relationship is well-approximated by the following equation.
    
    ```{r, echo = FALSE}
    coefout <- coef(lm(log(Mortality) ~ log(Wine), data = ex0823))
    beta0 <- coefout[[1]]
    beta1 <- coefout[[2]]
    ```
    
    $$
    y = `r exp(beta0)`x^{`r beta1`}
    $$
    
    ```{r, echo = FALSE}
    df <- data.frame(x = seq(min(ex0823$Wine), max(ex0823$Wine), length.out = 100))
    df$y <- exp(beta0) * df$x ^ beta1
    ggplot() +
      geom_point(data = ex0823, mapping = aes(x = Wine, y = Mortality)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```

    1. Interpret `r beta1`.
    
    ```{block, eval = FALSE, echo = FALSE}
    Countries that drink twice as much wine have mortality rates 22% lower on average. This is since 2^-0.3556 = 0.7815.
    ```
    
    2. Interpret `r beta0` (the value of $\beta_0$).
    
    ```{block, eval = FALSE, echo = FALSE}
    The y-intercept of the line of log(Mortality) on log(Wine). You can interpret $exp(\beta_0) = 12.88$ as the average mortality rate of countries that drink 1 liter per year, but this is a stretch since the minimum wine consumption is 2.8 liters per year.
    ```
        
    3. If one country drinks 50\% more wine per person per year than another country, what is the expected difference in mortality rates?
    
    ```{block, echo = FALSE, eval = FALSE}
    `1.5^-0.3556 = 0.8657`, so a mortality rate that is 13\% lower.
    ```
        
## Logging just the predictor variable

- Sometimes, the relationship is of the form
    $$
    y = \beta_0 + \beta_1 \log(x)
    $$

- You intrepret $\beta_1$ with the following:
    - When you double $x$, you add $\beta_1\log(2)$ to $y$.
    - When you multiply $x$ by 10, you add $\beta_1\log(10)$ to $y$.
    - When you multiply $x$ by 1.1 (10\% increase), you add $\beta_1\log(1.1)$ to $y$.
    - Choose a multiplier that makes sense for the range of your data. E.g. if it is never the case that one value is 10 times larger than another, don't use that as the interpretation.
    
- This follows from
    \begin{align}
    y_{old} &= \beta_0 + \beta_1\log(x)\\
    y_{new} &=  \beta_0 + \beta_1\log(2x)\\
            &=  \beta_0 + \beta_1[\log(x) + \log(2)]\\
            &=  \beta_0 + \beta_1\log(x) + \beta_1\log(2)\\
            &= y_{old} + \beta_1\log(2).
    \end{align}
 
```{r, echo = FALSE}
data("mtcars")
mtcars <- mutate(mtcars, log_wt = log(wt))
lm_mt <- lm(mpg ~ log_wt, data = mtcars)
tout <- tidy(lm_mt)
beta0 <- tout$estimate[[1]]
beta1 <- tout$estimate[[2]]
```
 
 - Example: From the `mtcars` dataset, it was determined that the relationship between `mpg` and `wt` was well approximated by the curve $y = `r beta0` + `r beta1`\log(x)$.
    ```{r}
    df <- data.frame(x = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100))
    df$y <- beta0 + beta1 * log(df$x)
    ggplot() +
      geom_point(data = mtcars, mapping = aes(x = wt, y = mpg)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```

- So cars that are 50\% heavier have $-17.0 \times \log(1.5) = -6.9$ worse miles per gallon on average.

## Summary of relationships on different scales

- If relationship is $y = \beta_0 + \beta_1x$, then add $c$ to $x$ means add $c\beta_1$ to $y$.

- If relationship is $y = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means add $\beta_1\log(c)$ to $y$.

- If relationship is $\log(y) = \beta_0 + \beta_1 x$, then add $c$ to $x$ means multiply $y$ by $\exp(c\beta_1)$.

- If relationship is $\log(y) = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means multiply $y$ by $c^{\beta_1}$.
