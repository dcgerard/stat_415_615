---
title: "Probability and Simulation"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
---

```{r setup, include=FALSE}
set.seed(3)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

# Learning Objectives

- Basic Probability
- Normal/$t$-distributions
- Simulation in R
- Appendix A of KNNL.

# Probability, the normal and $t$ distributions.

- A **random variable** is a variable whose value is a numerical outcome of a random process. We denote random variables with letters, like $Y$.

- In practice, this "random process" is sampling a unit from a population of units and observing that unit's value of the variable. E.g., we sample birth weights of babies born in the United States, then birth weight a random variable.

- The **mean** of a random variable is average value of a very large sample of individuals. The notation for the mean of a random variable is $E[Y]$.

- Properties:
    - If $a$ and $b$ are constants (not random variables), then $E[a + bY] = a + bE[Y]$.
    - If $X$ and $Y$ are both random variables, then $E[X + Y] = E[X] + E[Y]$.

- The **variance** of a random variable is the average squared deviation from the mean of this random variable. It measures how spread out the values are in a population. The notation for the variance of a random variable is $var(Y)$. Specifically
    $$
    var(Y) = E[(Y - E[Y])^2]
    $$

- Properties:
    - If $a$ and $b$ are constants, then $var(a + bY) = b^2var(Y)$
    
- The standard deviation of a random variable is the square root of its variance.
    $$
    sd(Y) = \sqrt{var(Y)}
    $$

- The **distribution** of a random variable is the possible values of a random variable and how often it takes those values. 
  
- A **density** describes the distribution of a *quantitative* variable. You can think of it as approximating a histogram. It is a curve where
  - The area under the curve between any two points is approximately the probability of being between those two points.
  - The total area under the curve is 1 (something must happen).
  - The curve is never negative (can't have negative probabilities).
  
- The density of birth weights in America:

    ![](./02_figs/birthweights.png)\ 
  
- The distribution of many variables in Statistics approximate the **normal distribution**.
    - If you know the mean and standard deviation of a normal distribution, then you know the whole distribution.
    - Larger standard deviation implies more spread out (larger and smaller values are both more likely).
    - Mean determines where the data are centered.

- Normal densities with different means.
    ```{r, echo=FALSE}
    library(ggplot2)
    library(ggthemes)
    x <- seq(-10, 10, length = 100)
    y1 <- dnorm(x = x, mean = 0, sd = 2)
    y2 <- dnorm(x = x, mean = -4, sd = 2)
    y3 <- dnorm(x = x, mean = 4, sd = 2)
    dfdat <- data.frame(x = rep(x, 3), 
                        y = c(y1, y2, y3), 
                        z = factor(rep(c(1, 2, 3), each = length(x))))
    
    ggplot(data = dfdat, mapping = aes(x = x, y = y, color = z, lty = z)) +
      geom_line(lwd=1) + 
      ggthemes::scale_color_colorblind() +
      theme(legend.position="none") +
      ylab("f(x)") +
      xlab("x")
    ```

- Normal densities with different standard deviations
    ```{r, echo=FALSE}
    x <- seq(-10, 10, length = 100)
    y1 <- dnorm(x = x, mean = 0, sd = 1)
    y2 <- dnorm(x = x, mean = 0, sd = 2)
    y3 <- dnorm(x = x, mean = 0, sd = 4)
    dfdat <- data.frame(x = rep(x, 3), 
                        y = c(y1, y2, y3), 
                        z = factor(rep(c(1, 2, 3), each = length(x))))
    
    ggplot(data = dfdat, mapping = aes(x = x, y = y, color = z, lty = z)) +
      geom_line(lwd=1) + 
      ggthemes::scale_color_colorblind() +
      theme(legend.position="none") +
      ylab("f(x)") +
      xlab("x")
    ```
  
- Density Function (height of curve, **NOT** probability of a value).
    ```{r}
    dnorm(x = 2, mean = 1, sd = 1)
    ```
    
    ```{r, echo = FALSE}
    x <- seq(-2, 4, length = 100)
    y <- dnorm(x, mean = 1, sd = 1)
    dftemp <- data.frame(x = x, y = y)
    ggplot(data = dftemp, mapping = aes(x = x, y = y)) +
      geom_line() +
      ylab("f(x)") +
      geom_segment(dat = data.frame(x = 2, xend = 2, y = 0, 
                                    yend = dnorm(x = 2, mean = 1, sd = 1)), 
                   aes(x = x, y = y, xend = xend, yend = yend), lty = 2, col = 2)
    ```
    
- Random Generation (generate samples from a given normal distribution).

    ```{r}
    samp <- rnorm(n = 1000, mean = 1, sd = 1)
    head(samp)
    ```
    
    ```{r, echo = FALSE}
    ggplot(data.frame(samp), aes(x = samp)) +
      geom_histogram(color = "blacK", fill = "white", bins = 20)
    ```
  
- Cumulative Distribution Function (probability of being less than or equal to some value).

    ```{r}
    pnorm(q = 2, mean = 1, sd = 1)
    ```
    
    ```{r, echo = FALSE}
    x <- seq(-2, 4, length = 500)
    y <- dnorm(x, mean = 1, sd = 1)
    polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                         y = c(0, y[x < 2], 0, 0))
    ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +
      geom_line() +
      ylab("f(x)") +
      geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
                   fill = "blue", alpha = 1/4)
    ```
    
- Quantile function (find value that has a given the probability of being less than or equal to it).
  
    ```{r}
    qnorm(p = 0.8413, mean = 1, sd = 1)
    ```
    
    ```{r, echo = FALSE}
    x <- seq(-2, 4, length = 500)
    y <- dnorm(x, mean = 1, sd = 1)
    polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                         y = c(0, y[x < 2], 0, 0))
    ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +
      geom_line() +
      ylab("f(x)") +
      geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
                   fill = "blue", alpha = 1/4) +
      annotate(geom = "text", x = 0.5, y = 0.1, label = "0.8413", color = "black")
    ```  
    
- **Exercise**: Use `rnorm()` to generate 10,000 random draws from a normal distribution with mean 5 and standard deviation 2. What proportion are less than 3? Can you think up a way to approximate this proportion using a different function?
  
    ```{r, eval=FALSE, echo=FALSE}
    samp <- rnorm(n = 10000, mean = 5, sd = 2)
    mean(samp < 3)
    pnorm(q = 3, mean = 5, sd = 2)
    ```
  
- **Exercise**: In Hong Kong, human male height is approximately normally distributed with mean 171.5 cm and standard deviation 5.5 cm. What proportion of the Hong Kong population is between 170 cm and 180 cm?
    
    ```{r, echo = FALSE, eval = FALSE}
    pless180 <- pnorm(q = 180, mean = 171.5, sd = 5.5)
    pless170 <- pnorm(q = 170, mean = 171.5, sd = 5.5)
    pbetween <- pless180 - pless170
    pbetween
    ```
    
- A property of the normal distribution is that if $X \sim N(\mu, \sigma^2)$ and $Z = (X - \mu) / \sigma$, then $Z \sim N(0, 1)$.

- **Exercise**: Use `rnorm()` and `qqplot()` to demonstrate this property. That is, simulate 1000 values of $X$ with some mean different than 0 and some variance different than 1. Then transform those $X$ values to $Z$. Then simulate some other variable $W$ from $N(0, 1)$. Use `qqplot()` to show that $W$ and $Z$ follow the same distribution.

    ```{r, echo = FALSE, eval = FALSE}
    mu <- 1
    sigma <- 2
    X <- rnorm(n = 1000, mean = mu, sd = sigma)
    Z <- (X - mu) / sigma
    W <- rnorm(1000)
    qqplot(Z, W)
    abline(0, 1)
    
    ks.test(Z, W)
    ```

- The $t$-distribution shows up a lot in Statistics. 
    - It is also bell-curved but has "thicker tails" (more extreme observations are more likely). 
    - It is always centered at 0. 
    - It only has one parameter, called the "degrees of freedom", which determines how thick the tails are.
    - Smaller degrees of freedom mean thicker tails, larger degrees of freedom means thinner tails.
    - If the degrees of freedom is large enough, the $t$-distribution is approximately the same as a normal distribution with mean 0 and variance 1.
      
- $t$-distributions with different degrees of freedom:

    ```{r, echo=FALSE}
    x <- seq(-4, 4, length = 200)
    data.frame(df = as.factor(c(rep(1, length(x)), rep(5, length(x)), rep(Inf, length(x)))),
               x = c(x, x, x),
               y = c(dt(x = x, df = 1),
                     dt(x = x, df = 5),
                     dt(x = x, df = Inf))) ->
      dfdat
    ggplot(dfdat, mapping = aes(x = x, y = y, color = df, lty = df)) +
      geom_line() +
      scale_color_colorblind() +
      ylab("f(x)")
    ```
    
- Density, distribution, quantile, and random generation functions also exist for the $t$-distribution.
    ```{r, eval = FALSE}
    dt()
    pt()
    qt()
    rt()
    ```

# Covariance/correlation

- The covariance between two random variables, $X$ and $Y$, is a measure of the strength of the linear association between these variables. It is defined as
    $$
    cov(X, Y) = E[(X - E[X])(Y-E[Y])]
    $$

- Covariance is related to correlation by
    $$
    cor(X, Y) = \frac{cov(X, Y)}{sd(X)sd(Y)}
    $$
