---
title: "Probability and Simulation"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
urlcolor: "blue"
---

```{r setup, include=FALSE}
set.seed(3)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

# Learning Objectives

- Basic Probability
- Normal/$t$-distributions
- Simulation in R

# Probability, the normal and $t$ distributions.

- **Distribution**: The possible values of a variable and how often it takes those values. 
  
- A **density** describes the distribution of a *quantitative* variable. You can think of it as approximating a histogram. It is a curve where
  - The area under the curve between any two points is approximately the probability of being between those two points.
  - The total area under the curve is 1 (something must happen).
  - The curve is never negative (can't have negative probabilities).
  
- The density of birth weights in America:

    ![](./02_figs/birthweights.png)\ 
  
- The distribution of many variables in Statistics approximate the **normal distribution**.
    - If you know the mean and standard deviation of a normal distribution, then you know the whole distribution.
    - Larger standard deviation implies more spread out (larger and smaller values are both more likely).
    - Mean determines where the data are centered.

- Normal densities with different means.
    ```{r, echo=FALSE}
    library(ggplot2)
    library(ggthemes)
    x <- seq(-10, 10, length = 100)
    y1 <- dnorm(x = x, mean = 0, sd = 2)
    y2 <- dnorm(x = x, mean = -4, sd = 2)
    y3 <- dnorm(x = x, mean = 4, sd = 2)
    dfdat <- data.frame(x = rep(x, 3), 
                        y = c(y1, y2, y3), 
                        z = factor(rep(c(1, 2, 3), each = length(x))))
    
    ggplot(data = dfdat, mapping = aes(x = x, y = y, color = z, lty = z)) +
      geom_line(lwd=1) + 
      ggthemes::scale_color_colorblind() +
      theme(legend.position="none") +
      ylab("f(x)") +
      xlab("x")
    ```

- Normal densities with different standard deviations
    ```{r, echo=FALSE}
    x <- seq(-10, 10, length = 100)
    y1 <- dnorm(x = x, mean = 0, sd = 1)
    y2 <- dnorm(x = x, mean = 0, sd = 2)
    y3 <- dnorm(x = x, mean = 0, sd = 4)
    dfdat <- data.frame(x = rep(x, 3), 
                        y = c(y1, y2, y3), 
                        z = factor(rep(c(1, 2, 3), each = length(x))))
    
    ggplot(data = dfdat, mapping = aes(x = x, y = y, color = z, lty = z)) +
      geom_line(lwd=1) + 
      ggthemes::scale_color_colorblind() +
      theme(legend.position="none") +
      ylab("f(x)") +
      xlab("x")
    ```
  
- Density Function (height of curve, **NOT** probability of a value).
    ```{r}
    dnorm(x = 2, mean = 1, sd = 1)
    ```
    
    ```{r, echo = FALSE}
    x <- seq(-2, 4, length = 100)
    y <- dnorm(x, mean = 1, sd = 1)
    qplot(x, y, geom = "line", ylab = "f(x)") +
      geom_segment(dat = data.frame(x = 2, xend = 2, y = 0, 
                                    yend = dnorm(x = 2, mean = 1, sd = 1)), 
                   aes(x = x, y = y, xend = xend, yend = yend), lty = 2, col = 2)
    ```
    
- Random Generation (generate samples from a given normal distribution).

    ```{r}
    samp <- rnorm(n = 1000, mean = 1, sd = 1)
    head(samp)
    ```
    
    ```{r, echo = FALSE}
    qplot(samp, geom = "histogram", fill = I("white"), color = I("black"), bins = 20)
    ```
  
- Cumulative Distribution Function (probability of being less than or equal to some value).

    ```{r}
    pnorm(q = 2, mean = 1, sd = 1)
    ```
    
    ```{r, echo = FALSE}
    x <- seq(-2, 4, length = 500)
    y <- dnorm(x, mean = 1, sd = 1)
    polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                         y = c(0, y[x < 2], 0, 0))
    qplot(x, y, geom = "line", ylab = "f(x)") +
      geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
                   fill = "blue", alpha = 1/4)
    ```
    
- Quantile function (find value that has a given the probability of being less than or equal to it).
  
    ```{r}
    qnorm(p = 0.8413, mean = 1, sd = 1)
    ```
    
    ```{r, echo = FALSE}
    x <- seq(-2, 4, length = 500)
    y <- dnorm(x, mean = 1, sd = 1)
    polydf <- data.frame(x = c(min(x), x[x < 2], 2, 0), 
                         y = c(0, y[x < 2], 0, 0))
    qplot(x, y, geom = "line", ylab = "f(x)") +
      geom_polygon(data = polydf, mapping = aes(x = x, y = y), 
                   fill = "blue", alpha = 1/4) +
      annotate(geom = "text", x = 0.5, y = 0.1, label = "0.8413", color = "black")
    ```  
    
- **Exercise**: Use `rnorm()` to generate 10,000 random draws from a normal distribution with mean 5 and standard deviation 2. What proportion are less than 3? Can you think up a way to approximate this proportion using a different function?
  
    ```{r, eval=FALSE, echo=FALSE}
    samp <- rnorm(n = 10000, mean = 5, sd = 2)
    mean(samp < 3)
    pnorm(q = 3, mean = 5, sd = 2)
    ```
  
- **Exercise**: In Hong Kong, human male height is approximately normally distributed with mean 171.5 cm and standard deviation 5.5 cm. What proportion of the Hong Kong population is between 170 cm and 180 cm?
    
    ```{r, echo = FALSE, eval = FALSE}
    pless180 <- pnorm(q = 180, mean = 171.5, sd = 5.5)
    pless170 <- pnorm(q = 170, mean = 171.5, sd = 5.5)
    pbetween <- pless180 - pless170
    pbetween
    ```
    
- A property of the normal distribution is that if $X \sim N(\mu, \sigma^2)$ and $Z - (X - \mu) / \sigma$, then $Z \sim N(0, 1)$.

- **Exercise**: Use `rnorm()` and `qqplot()` to demonstrate this property. That is, simulate 1000 values of $X$ with some mean different than 0 and some variance different than 1. Then transform those $X$ values to $Z$. Then simulate some other variable $W$ from $N(0, 1)$. Use `qqplot()` to show that $W$ and $Z$ follow the same distribution.

    ```{r, echo = FALSE, eval = FALSE}
    mu <- 1
    sigma <- 2
    X <- rnorm(n = 1000, mean = mu, sd = sigma)
    Z <- (X - mu) / sigma
    W <- rnorm(100)
    qqplot(Z, W)
    abline(0, 1)
    
    ks.test(Z, W)
    ```

- The $t$-distribution shows up a lot in Statistics. 
    - It is also bell-curved but has "thicker tails" (more extreme observations are more likely). 
    - It is always centered at 0. 
    - It only has one parameter, called the "degrees of freedom", which determines how thick the tails are.
    - Smaller degrees of freedom mean thicker tails, larger degrees of freedom means thinner tails.
    - If the degrees of freedom is large enough, the $t$-distribution is approximately the same as a normal distribution with mean 0 and variance 1.
      
- $t$-distributions with different degrees of freedom:

    ```{r, echo=FALSE}
    x <- seq(-4, 4, length = 200)
    data.frame(df = as.factor(c(rep(1, length(x)), rep(5, length(x)), rep(Inf, length(x)))),
               x = c(x, x, x),
               y = c(dt(x = x, df = 1),
                     dt(x = x, df = 5),
                     dt(x = x, df = Inf))) ->
      dfdat
    ggplot(dfdat, mapping = aes(x = x, y = y, color = df, lty = df)) +
      geom_line() +
      scale_color_colorblind() +
      ylab("f(x)")
    ```
    
- Density, distribution, quantile, and random generation functions also exist for the $t$-distribution.
    ```{r, eval = FALSE}
    dt()
    pt()
    qt()
    rt()
    ```

# Simulation as a way of life

- **Simulation study**: Generating random data sets to evaluate properties of the sample.

- Simulation is vital in applied statistics for:
    1. Better understanding of variation in data (humans are naturally bad at this).
    2. Evaluating the properties of estimators and hypothesis tests across many samples. This is very important for sample size calculations.
    3. Representing uncertainty in predictions and forecasts.

- R functions for simulation:
    - `rnorm()`, `rt()`, etc: Simulate from a specific distribution.
        ```{r}
        rnorm(n = 10, mean = 1, sd = 2)
        ```
    - `sample()`: Obtain a random sample (with or without replacement) from a finite set of predefined values.
        ```{r}
        sample(c("Heads", "Tails", "Edge"), size = 10, replace = TRUE, prob = c(0.45, 0.45, 0.1))
        ```
    - `replicate()`: Replicate some code a certain number of times, and collect the output from each replication.
        ```{r}
        replicate(n = 10,expr = {
          x <- rnorm(10)
          mean(x)
        }) 
        ```
    - for-loops: A standard way to iteratively apply code.
        ```{r}
        n <- 10
        for (i in 1:n) {
          ## code goes here, i increases by 1 each iteration
          print(i)
        }
        ```
    
        
- Suppose your boss asks you if she should use the mean or the median. She has some trial data, but plans on collecting about 250 new values. She gives you this vector of observed values
    ```{r}
    x <- c(1.31, 2.42, 1.76, 0.71, 3.07, 0.86, 2.78, 1.26, 2.79, 12.91)
    ```

- Your first thought is to generate from the normal distribution using the estimated mean and standard deviation, and then compare the median to the mean using some metric.
    ```{r}
    xbar <- mean(x)
    s <- sd(x)
    sampsize <- 250
    meanvec <- replicate(n = 1000, expr = {
      xsim <- rnorm(n = sampsize, mean = xbar, sd = s)
      mean(xsim)
    })
    
    qplot(meanvec, main = "Mean", bins = 30) +
      geom_vline(xintercept = xbar, lty = 2, col = 2)
    
    medvec <- replicate(n = 1000, expr = {
      xsim <- rnorm(n = sampsize, mean = xbar, sd = s)
      median(xsim)
    })
    
    qplot(medvec, main = "Median", bins = 30) +
      geom_vline(xintercept = xbar, lty = 2, col = 2)
    ```

- They both look to be unbiased. But when you compare the mean squared error, the mean does better (lower mean squared error):
    ```{r}
    # Mean performance
    mean((meanvec - xbar)^2)
    
    # Median performance
    mean((medvec - xbar)^2)
    ```

- Which is a performance improvement of `r (1 - mean((meanvec - xbar)^2) / mean((medvec - xbar)^2))*100`\% in the mean squared error.

- This assumes that you are simulating under the normal model. But it is a little iffy on whether the observed data are actually normal.

    ```{r}
    qplot(x, bins = 10)
    qqnorm(x)
    qqline(x)
    ```

- You decide to stress-test. What if that single right value is an outlier? Let's calculate the mean and standard deviation of the 9 values near 0.
    ```{r}
    xbar <- mean(x[x < 5])
    s <- sd(x[x < 5])
    ```
    
    And let's simulate from a distribution that is normal 90\% of the time and that outlying point 10\% of the time.
    
    ```{r}
    outval <- max(x)
    meanvec <- replicate(n = 1000, expr = {
      outlier <- sample(x = c(TRUE, FALSE), size = sampsize, replace = TRUE, prob = c(0.1, 0.9))
      xsim <- rep(NA, length.out = sampsize)
      for (i in 1:sampsize) {
        xsim[[i]] <- ifelse(outlier[[i]], outval, rnorm(n = 1, mean = xbar, sd = s))
      }
      mean(xsim)
    })
    
    medvec <- replicate(n = 1000, expr = {
      outlier <- sample(x = c(TRUE, FALSE), size = sampsize, replace = TRUE, prob = c(0.1, 0.9))
      xsim <- rep(NA, length.out = sampsize)
      for (i in 1:sampsize) {
        xsim[[i]] <- ifelse(outlier[[i]], outval, rnorm(n = 1, mean = xbar, sd = s))
      }
      median(xsim)
    })
    ```

- The median is looking really good now:
    
    ```{r}
    xbar
    mean(meanvec)
    mean(medvec)
    ```
  
    So you might state that the median is able to capture the center of the bulk of the observations, even in the presence of a large number of extreme values. This is at a reduced efficiency (if the normal model were correct) of about 30\%.
    
- This is an example of using simulation to study the robustness of an estimator to model misspecification. We wouldn't actually run this for evaluating the mean versus the median, but this type of technique becomes vital for more complicated models.

- **Exercise**: About 48\% of US adults are men, and about 52\% of US adults are women. The heights of men are approximately normally distributed with mean 69.1 inches and standard deviation 2.9 inches. The heights of women are approximately normally distributed with mean 63.7 inches and standard deviation 2.7 inches. Simulate the heights of a random sample of 1000 adults and plot them.

    ```{r, eval = FALSE, echo = FALSE}
    male <- sample(x = c(TRUE, FALSE), size = 1000, replace = TRUE, prob = c(0.48, 0.52))
    height <- rep(NA, length.out = 1000)
    for (i in 1:1000) {
      height[[i]] <- ifelse(male[[i]], rnorm(1, 69.1, 2.9), rnorm(1, 63.7, 2.7))
    }
    qplot(height, fill = male, bins = 30)
    ```

