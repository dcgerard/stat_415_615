---
title: "Linear Algebra"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
urlcolor: "blue"
---

```{r setup, include=FALSE}
library(latex2exp)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Introduce/review basic linear algebra.
- Sections 5.1--5.7 of KNNL
- Chapters 2 and 3 of [Linear Algebra for Data Science with examples in R](https://shainarace.github.io/LinearAlgebra/)

# Vectors and Matrices

- A **vector** is a list of numbers.

    $$
    \mathbf{x} = 
    \left(
    \begin{array}{c}
    10\\
    3\\
    5
    \end{array}
    \right)
    $$
    
- In the above vector, the first element is 10, the second is 3, and the third is 5.

- I will generally use bold, lower-case letters to represent vectors.

- A **matrix** is a rectangular array of numbers.

    $$
    \mathbf{A} = 
    \left(
    \begin{array}{cc}
    -1 & 4\\
    11 & -4\\
    2 & 8
    \end{array}
    \right)
    $$
    
- The **dimension** of a matrix is the number of rows and columns of that matrix. So the dimension of the above matrix is $3\times 2$ since it has three rows and two columns.

- We index elements by the rows first, then the columns second. For example, in the above matrix, the $(1, 1)$th element is -1, the $(1, 2)$th element is 4, the $(2, 1)$th element is 11, the $(2,2)$th element is -4, the $(3, 1)$th element is 2, and the $(3,2)$th element is 8.

- I will generally use bold, upper-case letters to represent matrices.

- A vector is just a matrix with one column.

- The **transpose** of a matrix/vector swaps the row and column indices.

    \begin{align}
    \mathbf{x}^T &= (10, 3, 5)\\
    \mathbf{A}^T &= 
    \left(
    \begin{array}{ccc}
    -1 & 11 & 2\\
    4 & -4 & 8
    \end{array}
    \right)
    \end{align}
    
- You can add two matrices together as long as they have the same dimensions. You just sum up the values element-wise:

    $$
    \left(
    \begin{array}{cc}
    1 & 2 \\
    3 & 4
    \end{array}
    \right) 
    +
    \left(
    \begin{array}{cc}
    5 & 6 \\
    7 & 8
    \end{array}
    \right)
    = 
    \left(
    \begin{array}{cc}
    6 & 8 \\
    10 & 12
    \end{array}
    \right)
    $$
    
- You can multiply a matrix by a scalar, where you multiply each element of that matrix by that scalar.
$$
2\mathbf{A} = 2
\left(
\begin{array}{cc}
-1 & 4\\
11 & -4\\
2 & 8
\end{array}
\right)
= 
\left(
\begin{array}{cc}
2 \times -1 & 2 \times 4\\
2 \times 11 & 2 \times -4\\
2 \times 2 & 2 \times8
\end{array}
\right) 
=
\left(
\begin{array}{cc}
-2 & 8\\
22 & -8\\
4 & 16
\end{array}
\right)
$$

    
- You can multiply two matrices together as long as the left matrix has the same number of columns as the number or rows of the right matrix. The rules for matrix multiplication are kinda weird (but they are justified if you ever take linear algebra). In effect, you multiply the *row* of the left matrix with the *column* of the right matrix, and add up the resulting multiplied numbers:

    ![](./04_figs/matmult.gif)\ 
    
- An awesome visualization from [Shaina Race](https://github.com/shainarace) does a better job of illustrating this:

    ![](./04_figs/multlincombanim.gif)\ 
    
- The special case of a matrix multiplied by a vector is below (also from Shaina Race):

    ![](./04_figs/animmatrixvectormult.gif)\ 
    
- **Exercise**: Let
    \begin{align}
    \mathbf{X} &= 
    \left(
    \begin{array}{cc}
    3 & -1\\
    1 & -2
    \end{array}
    \right)\\
    \mathbf{Y} &= 
    \left(
    \begin{array}{cc}
    1 & 2\\
    2 & 1
    \end{array}
    \right)
    \end{align}
    Calculate $\mathbf{X}^T\mathbf{Y}$.
    
    ```{block, eval = FALSE, echo = FALSE}
    \begin{align}
    \left(
    \begin{array}{cc}
    5 & 7\\
    -5 & -4
    \end{array}
    \right)
    \end{align}
    ```

- Note that matrix multiplication is generally *not* commutative. That is
    $$
    \mathbf{A}\mathbf{B} \neq \mathbf{B}\mathbf{A}
    $$
    Indeed, even if $\mathbf{A}\mathbf{B}$ exists does not mean that $\mathbf{B}\mathbf{A}$ exists. E.g. Suppose $\mathbf{A}$ is $2 \times 3$ and $\mathbf{B}$ is $3 \times 4$.
    
- The **identity matrix** is the diagonal matrix with 1's in the diagonal. It is the matrix analogue to the number 1. For example, the $3 \times 3$ diagonal matrix is

    $$
    \mathbf{I}_3 = \left(
    \begin{array}{ccc}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1
    \end{array}
    \right)
    $$
    
- For any matrix $\mathbf{A}$ of dimension $n \times p$, we have
    $$
    \mathbf{I}_n\mathbf{A} = \mathbf{A}\mathbf{I}_p = \mathbf{A}
    $$
    
- **Exercise**: Verify that
    $$
    \left(
    \begin{array}{cc}
    1 & 0 \\
    0 & 1
    \end{array}
    \right)
    \left(
    \begin{array}{cc}
    a & b \\
    c & d
    \end{array}
    \right)
    =
    \left(
    \begin{array}{cc}
    a & b \\
    c & d
    \end{array}
    \right)
    $$
    
- The $n$-vector of all $1$'s is usually denoted $\mathbf{1}_n$
    $$
    \mathbf{1}_n = 
    \left(
    \begin{array}{c}
    1 \\
    1 \\
    \vdots\\
    1
    \end{array}
    \right)
    $$

- The $n\times p$ matrix of all $1$'s is usually denoted $\mathbf{J}_{n\times p}$.
    $$
    \mathbf{J}_{n\times p} = 
    \left(
    \begin{array}{ccc}
    1 & \cdots & 1\\
    \vdots & \ddots & \vdots\\
    1 & \cdots & 1
    \end{array}
    \right)
    $$
    
- The $n$-vector of all $0$'s is usually denoted $\mathbf{0}_n$.
    $$
    \mathbf{0}_n = 
    \left(
    \begin{array}{c}
    0 \\
    0 \\
    \vdots\\
    0
    \end{array}
    \right)
    $$

# Linear Algebra and Simple Linear Regression

- We can represent the simple linear regression model by arranging values in matrices.

- Let
    \begin{align}
    \mathbf{y} &= 
    \left(
    \begin{array}{c}
    Y_1\\
    Y_2\\
    \vdots\\
    Y_n
    \end{array}
    \right),\\
    \mathbf{X} &= 
    \left(
    \begin{array}{cc}
    1 & X_1\\
    1 & X_2\\
    \vdots & \vdots\\
    1 & X_n
    \end{array}
    \right),\\
    \mathbf{\beta} &= 
    \left(
    \begin{array}{c}
    \beta_0\\
    \beta_1
    \end{array}
    \right),\\
    \mathbf{\epsilon} &= 
    \left(
    \begin{array}{c}
    \epsilon_1\\
    \epsilon_2\\
    \vdots\\
    \epsilon_n
    \end{array}
    \right)
    \end{align}

- Then we can represent the simple linear regression model in terms of these matrices
    $$
    \mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}
    $$

