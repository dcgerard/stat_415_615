---
title: "Multiple Linear Regression I"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
---

```{r setup, include=FALSE}
library(latex2exp)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Multiple linear regression model definition and interpretation.
- Indicator variables, transformed variables, interaction effects,
- Inference
- Diagnostics and remedial measures.
- Chapter 6 of KNNL

# Motivation

- Most regression problems have more than one predictor.

- Multiple linear regression explores the relationship between **one response variable** and **multiple predictor variables**.

    - (if you have multiple response variables, this is called "multivariate" regression, and is a separate topic)

- Examples: 
    - Predicting volume of tractor sales in a territory ($Y$) given number of farms ($X_1$) and crop production ($X_2$).
    - Predicting a political party's vote share in a congressional district ($Y$) given that party's vote share in the previous election ($X_1$) and the incumbancy's status of the political party ($X_2$).

- Why include more predictors?
    - You can often get a more accurate prediction of $Y$ if you include more $X$'s.
    - You can control for other variables when assessing the relationship between a response and a predictor of interest.
    
- Controlling is necessary because the strength/sign of a relationship might change once you control for a variable.

- Consider the earnings data described [here](https://dcgerard.github.io/stat_415_615/data.html#Earnings_Data) and available for download [here](https://dcgerard.github.io/stat_415_615/data/earnings.csv).

    ```{r, message=FALSE, echo = FALSE}
    library(tidyverse)
    earnings <- read_csv("https://dcgerard.github.io/stat_415_615/data/earnings.csv")
    ```

    We might be interested in the association between height and earnings. Indeed, there seems to be a slight positive relationship.
    ```{r, echo = FALSE, warning = FALSE, message = FALSE}
    earnings %>%
      mutate(log_earn = log(earn)) %>%
      ggplot(mapping = aes(x = height, y = log_earn)) +
      geom_point(alpha = 0.05) +
      geom_smooth(method = "lm", se = FALSE) +
      xlab("Height") +
      ylab("log-earnings")
    ```
    
    But then we remember that men tend to be taller than women, and men tend to make more money than women, and so this might be the reason why **marginally** we are seeing this association between height and earnings.
    ```{r, echo = FALSE, warning = FALSE, message = FALSE}
    library(ggthemes)
    earnings %>%
      mutate(log_earn = log(earn)) %>%
      ggplot(mapping = aes(x = height, y = log_earn)) +
      geom_point(mapping = aes(color = sex), alpha = 0.05) +
      geom_smooth(method = "lm", se = FALSE) +
      geom_smooth(mapping = aes(color = sex), method = "lm", se = FALSE) +
      xlab("Height") +
      ylab("log-earnings") +
      scale_color_colorblind()
    ```
    
    The relationship appears to still be slightly positive within each sex, but nowhere near as strong.

# Multiple linear regression model: 2 predictors

- The multiple linear regression model for two predictors is
    \begin{align}
    Y_i &= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i\\
    E[\epsilon_i] &= 0\\
    var(\epsilon_i) &= \sigma^2\\
    cov(\epsilon_i, \epsilon_j) &= 0 \text{ for all } i \neq j
    \end{align}
    where

    - $Y_i$ is the value of the response for individual $i$.
    - $X_{i1}$ is the value of the first predictor for individual $i$.
    - $X_{i2}$ is the value of the second predictor for individual $i$.
    - $\epsilon_i$ is the error term for individual $i$ (vertical distance from plane).
    
- This necessarily means that
    $$
    E[Y_i] = \beta_0 + \beta_1 X_{i1} + \beta_2X_{i2}
    $$

- We can visualize this model using KNNL's figure 6.1:

    ![](./figs/plane.png)\ 

- The regression function (the deterministic part) is often called the **regression surface** or the **response surface**.

- **Example**: It's been determined, from the `mtcars` dataset, that the relationship of `drat` ($X_1$) and `wt` ($X_2$) with `mpg` ($Y$) can be described by
    $$
    E[Y] = 30.3 + 1.4 X_1 - 4.8 X_2
    $$
    
    ```{r}
    data("mtcars")
    coef(lm(mpg ~ drat + wt, data = mtcars))
    ```
    
- We interpret multiple linear models the same way we interpret simple linear models while **considering that all predictors except one are fixed.**

- Suppose we are just interested in cars that are exactly 4000 pounds. Then, given a car is 4000 pounds, the regression relationship becomes
    \begin{align}
    E[Y] &= 30.3 + 1.4X_1 - 4.8 \times 4\\
    &= 11.1 + 1.4 X_1.
    \end{align}
    So, given two cars that are the same weight, we expect the car with 1 more `drat` to have 1.4 more miles per gallon, on average.
    
- This interpretation would have held if we considered any other car weight (besides 4000 pounds).

- So $\beta_1$ is interpreted as

    > The expected difference in $Y$ between two individuals that differ by 1 $X_1$ but have the same $X_2$.

- Similarly, $\beta_2$ is interpreted as

    > The expected difference in $Y$ between two individuals that differ by 1 $X_2$ but have the same $X_1$.
    
- In the `mtcars` dataset example, a car that weighs 1000 pounds more but has the same drat will on average have 4.8 worse mpg.

# The model

- The multiple linear regression (MLR) model is of the form

    \begin{align}
    Y_i &= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1} + \epsilon_i\\
    E[\epsilon_i] &= 0\\
    var(\epsilon_i) &= \sigma^2\\
    cov(\epsilon_i, \epsilon_j) &= 0 \text{ for all } i \neq j
    \end{align}
    
- Using summation notation, we can write the top equation by
    $$
    Y_i = \beta_0 + \sum_{j=1}^{p-1}\beta_j X_{ji}
    $$
    
- Why use $p-1$ in the indexing? The $p$th variable is $X_{0i} = 1$ and the linear model can be equivalently written
    \begin{align}
    Y_i &= \beta_0X_{0i} + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_pX_{i,p-1} + \epsilon_i\\
     &= \sum_{j=0}^{p-1}\beta_j X_{ji}
    \end{align}

- The response function
    $$
    E[Y] = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_{p-1}X_{p-1}
    $$
    is a **hyperplane**. You cannot visualize this (since it involves more than three dimensions).


- The interpretation of $\beta_i$ is similar:

    > The expected difference in $Y$ between two individuals that differ by 1 $X_i$ but have the same values of every other predictor.
    
- Notice that I am not using the words "change", "increase", or "decrease". These are forbidden words in this class.

# The flexibility of the linear model

- Fitting a hyperplane through a cloud of points seems restrictive, but it turns out that the multiple linear regression model is extremely flexible by including transformations of variables as covariates.

## Polynomial regression

- Suppose we want to fit a quadratic function to some data. Our model is

    $$
    Y_i = \beta_0 + \beta_1X_i + \beta_2 X_i^2 + \epsilon_i
    $$

```{r, echo = FALSE, eval = FALSE}
tibble(x = seq(0, 1, length.out = 100)) %>%
  mutate(y = 1 + 1.3 * x - x^2) %>%
  ggplot(aes(x = x, y = y)) +
  geom_line() +
  theme_bw() +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank()) +
  xlab("X") +
  ylab("Y") +
  annotate(geom = "text", x = 0.5, y = 1.3, label = TeX("$-\\frac{\\beta_1}{2\\beta_2}$")) +
  geom_vline(xintercept = 1.3 / 2, col = 2) ->
  pl
ggsave(filename = "./figs/quad.pdf", plot = pl, height = 3, width = 3)
```
    
    ![](./figs/quad.png)\ 
    
- We can redefine predictors:
    \begin{align}
    X_{i1} &= X_i\\
    X_{i2} &= X_i^2
    \end{align}

- Then it is easy to see that this is the multiple linear model:
    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \beta_2 X_{i2} + \epsilon_i
    $$

- This induces curvature in the response surface. 

- It is still called a linear model because it is linear in terms of the parameters (the $\beta$'s).


## Transformed variables

- It should be clear that you can include arbitrary transformations of the $X_i$'s (but this may or may not be a good idea).

- Suppose

    $$
    \log(Y_i) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i
    $$

    Then if we define $Y_i' = \log(Y_i)$, we have the MLR
    
    $$
    Y_i' = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i
    $$

- Suppose
    $$
    Y_i = \beta_0 + \beta_1 \log(X_{i1}) + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i
    $$
    
    Then if we define $X_{i1}' = \log(X_{i1})$, we have the MLR
    
    $$
    Y_i = \beta_0 + \beta_1 X_{i1}' + \beta_2 X_{i2} + \beta_3 X_{i3} + \epsilon_i
    $$

- These are result in complicated curved response surfaces.

- It is still called a linear model because it is linear in terms of the parameters (the $\beta$'s).

## Categorical (qualitative) variables

- Categorical variables (such as alive/dead, race, marital status, etc) can be used as predictors in the linear model by using indicator variables.

- An **indicator variable** takes on the value of 1 if a unit belongs to a category and 0 if it does not. 

- E.g., from the `mtcars` dataset, `am` is an indicator variable where
    $$
    X_{i} = 
    \begin{cases}
    1 & \text{ if manual}\\
    0 & \text{ if automatic}
    \end{cases}
    $$

- You could then include this indicator variable into the multiple linear regression model as a predictor. 
    $$
    Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
    $$

- The model for automatic cars is
    $$
    Y_i = \beta_0 + \epsilon_i
    $$
    
- The model for manual cars is
    $$
    Y_i = \beta_0 + \beta_1 + \epsilon_i
    $$
    
- So $\beta_1$ is interpreted as the mean difference between automatic and manual cars.

- If other covariates are included in the model, this interpretation changes to the "mean difference between automatic and manual cars while controlling for other variables".

- If there are $c$ classes (instead of 2), we use $c-1$ indicator variables to represent this categorical variable.

- For example, in the `mpg` dataset from the `{ggplot2}` package, the `drv` variable has values `"f"`, `"4"` and `"r"`. We could create two variables
    \begin{align}
    X_{i1} &= 
    \begin{cases}
    1 & \text{ if forward-wheel drive}\\
    0 & \text{ otherwise}
    \end{cases}\\
    X_{i2} &= 
    \begin{cases}
    1 & \text{ if 4-wheel drive}\\
    0 & \text{ otherwise}
    \end{cases}
    \end{align}

    The multiple linear regression model (using, say, `cty` as the response) becomes
    
    $$
    Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \epsilon_i
    $$
    
- The model for rear-wheel drive cars is
    $$
    Y_i = \beta_0 + \epsilon_i
    $$

- The model for forward-wheel drive cars is
    $$
    Y_i = \beta_0 + \beta_1 + \epsilon_i
    $$

- The model for 4-wheel drive cars is
    $$
    Y_i = \beta_0 + \beta_2 + \epsilon_i
    $$

- $\beta_0$ is interpreted as the mean city mpg for rear-wheel drive cars.

- $\beta_1$ is interpreted as the mean difference in city mpg between rear-wheel drive and forward-wheel drive cars.

- $\beta_2$ is interpreted as the mean difference in city mpg between rear-wheel drive and 4-wheel drive cars.

- **Exercise**: What is the mean difference in city mpg between 4-wheel drive and forward-wheel drive cars?
    ```{block, eval = FALSE, echo = FALSE}
    $\beta_2 - \beta_1$
    ```


- This parameterization allows for a different mean for each class. This is the **exact same** as ANOVA.

- We'll talk more about indicator variables in Chapter 8.

## Interaction effects

- The effect of one predictor may depend on the values of other predictors.

- E.g., in the height/earnings example, we saw that height might have a stronger earnings effect in men than in women (the slope was steeper for men than for women).


- Interaction terms are represented by multiplying predictors with each other:
    $$
    Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} +\beta_3X_{1i}X_{2i} + \epsilon_i
    $$

- We can get back the linear model by setting $X_3 = X_{1i}X_{2i}$. 

- This creates a very complicated and flexible model. We will talk about these interaction terms in chapter 8.

# Matrix Representation

- The matrix representation of the multiple linear regression model is **the exact same** as the matrix representation of the simple linear regression model, once we define the appropriate matrices.

- Let
    \begin{align}
    \mathbf{y} &= 
    \left(
    \begin{array}{c}
    Y_1\\
    Y_2\\
    \vdots\\
    Y_n
    \end{array}
    \right),\\
    \mathbf{X} &= 
    \left(
    \begin{array}{cc}
    1 & X_{11} & X_{12} & \cdots & X_{1,p-1}\\
    1 & X_{21} & X_{22} & \cdots & X_{2,p-1}\\
    \vdots & \vdots & \vdots & & \vdots \\
    1 & X_{n1} & X_{n2} & \cdots & X_{n,p-1}
    \end{array}
    \right),\\
    \mathbf{\beta} &= 
    \left(
    \begin{array}{c}
    \beta_0\\
    \beta_1\\
    \vdots\\
    \beta_{p-1}
    \end{array}
    \right),\\
    \mathbf{\epsilon} &= 
    \left(
    \begin{array}{c}
    \epsilon_1\\
    \epsilon_2\\
    \vdots\\
    \epsilon_n
    \end{array}
    \right)
    \end{align}

- The multiple linear model can be fully expressed with assumptions via
    \begin{align}
    \mathbf{y} &= \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\\
    E[\mathbf{\epsilon}] &= \mathbf{0}_n\\
    cov(\mathbf{\epsilon}) &= \sigma^2\mathbf{I}_n
    \end{align}
    
- For the normal linear model (important only for prediction invtervals), you further assume that $\mathbf{\epsilon}$ is distributed according to the [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution).

- **Exercise**: In the $\mathbf{X}$ matrix. What does the $i$ represent in $X_{ij}$? What does the $j$ represent?
    ```{block, eval = FALSE, echo = FALSE}
    The $i$ represents the unit, the $j$ represents the variable.
    ```


# Estimates and fitted values

$$
Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1} + \epsilon_i
$$

- The $\beta$'s are unknown. So, as in SLR, we estimate them by OLS:
    $$
    \min_{\beta_0,\beta_1,\ldots,\beta_{p-1}}\sum_{i=1}^n[Y_i - (\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots \beta_{p-1}X_{i,p-1})]^2
    $$

- The OLS estimates can be expressed using matrix notation *the exact same way* as in SLR:
    $$
    \hat{\mathbf{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
    $$

- The fitted values and the residuals can be expressed using matrix notation *the exact same way* as in SLR:
    $$
    \hat{\mathbf{Y}} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
    $$
    $$
    \mathbf{e} = (\mathbf{I}_n - \mathbf{H})\mathbf{Y}.
    $$
    
# Inference

- One can show that the OLS estimates are unbiased:
    $$
    E[\hat{\mathbf{\beta}}] = \mathbf{\beta},
    $$
    where this expectation is taken over the sampling distribution of $\hat{\mathbf{\beta}}$.
    
- The estimated covariance matrix of $\mathbf{\beta}$ can be expressed the using matrix notation *the exact same way* as in SLR:
    $$
    s^2(\hat{\mathbf{\beta}}) = MSE(\mathbf{X}^T\mathbf{X})^{-1}
    $$

- **Exercise**: What is the dimension of $s^2(\hat{\mathbf{\beta}})$
    ```{block, eval = FALSE, echo = FALSE}
    $p$ by $p$
    ```

- **Exercise**: How would you get the standard error of $\hat{\beta}_i$ from this covariance matrix? We denote this standard error $s\{\hat{\beta}_i\}$.
    ```{block, eval = FALSE, echo = FALSE}
    Find the $(i, i)$th value of $s^2(\hat{\mathbf{\beta}})$ and take a square root.
    ```

- It can be shown that the sampling distribution of $\hat{\beta}_i$ is given by
    $$
    t_i = \frac{\hat{\beta}_i - \beta_i}{s\{\hat{\beta}_i\}} \sim t(n-p)
    $$
    
- Confidence intervals: Because of this sampling distribution, we have
    $$
    \text{Pr}(\text{qt}(\alpha / 2, n-p) \leq t \leq \text{qt}(1 - \alpha/2, n-p)) = 1 - \alpha,
    $$
    where this probability is taken over the sampling distribution of $t$.
    ```{r, echo = FALSE, warning = FALSE, fig.width = 7}
    dfval <- 10
    uval <- qt(1 - 0.05/2, dfval)
    tibble(x = seq(-4, 4, length.out = 500)) %>%
          mutate(y = dt(x = x, df = dfval)) ->
          tdat
    ggplot(tdat, aes(x = x, y = y)) +
      geom_line() +
      theme_classic() +
      theme(axis.text = element_blank(),
              axis.ticks = element_blank(),
              axis.title = element_blank()) +
      geom_ribbon(data = filter(tdat, x < uval, x > -uval), aes(x = x, ymax = y), ymin = 0, fill = "blue") +
      geom_vline(xintercept = uval, lty = 2, col = 2) +
      geom_vline(xintercept = -uval, lty = 2, col = 2) +
      annotate(x = uval + 1, y = max(tdat$y), geom = "text", label = TeX("qt($1 - \\alpha/2$, n-p)"), size = 5) +
      annotate(x = -uval - 0.7, y = max(tdat$y), geom = "text", label = TeX("qt($\\alpha/2$, n-p)"), size = 5) +
      annotate(x = 0, y = max(tdat$y) / 2, geom = "text", label = TeX("$1 - \\alpha$"), color = "white", size = 10)
    ```
    So a 95\% confidence interval can be found by solving for $\beta_i$ in
    \begin{align}
    &\text{qt}(\alpha / 2, n-p) \leq \frac{\hat{\beta}_i - \beta_i}{s\{\hat{\beta}_i\}} \leq \text{qt}(1 - \alpha/2, n-p)\\
    &\Leftrightarrow  \text{qt}(\alpha / 2, n-p)s\{\hat{\beta}_i\} \leq \hat{\beta}_i - \beta_i \leq \text{qt}(1 - \alpha/2, n-p)s\{\hat{\beta}_i\}\\
        &\Leftrightarrow  \hat{\beta}_i - \text{qt}(\alpha / 2, n-p)s\{\hat{\beta}_i\} \geq \beta_i \geq \hat{\beta}_i - \text{qt}(1 - \alpha/2, n-p)s\{\hat{\beta}_i\}.
    \end{align}
    Noting that $\text{qt}(\alpha / 2, n-p) = -\text{qt}(1 - \alpha/2, n-p)$, we can write this interval as
    $$
    \hat{\beta}_i \pm \text{qt}(1 - \alpha/2, n-p)s\{\hat{\beta}_i\}
    $$

- This sampling distribution can also be used to test
    - $H_0: \beta_i = 0$ versus
    - $H_A: \beta_i \neq 0$.
    Under the null, we have
    $$
    t^* = \frac{\hat{\beta}_i}{s\{\hat{\beta}_i\}} \sim t(n-p).
    $$
    So to test against $H_0$, we can compare $t^* = \frac{\hat{\beta}_i}{s\{\hat{\beta}_i\}}$ to a $t(n-p)$ distribution and see how extreme it is.
    $$
    p-\text{value} = 2*\text{pt}(-|t^*|, n-p).
    $$

    ```{r, echo=FALSE, message = FALSE}
    tibble(x = seq(-4, 4, length.out = 500)) %>%
      mutate(y = dt(x = x, df = 2)) ->
      tdat
    ggplot(tdat, aes(x = x, y = y)) +
      geom_line() +
      theme_classic() +
      theme(axis.text = element_blank(),
              axis.ticks = element_blank(),
              axis.title = element_blank()) +
      geom_ribbon(data = filter(tdat, x > 2.5), aes(x = x, ymax = y), ymin = 0, fill = "blue") +
      geom_ribbon(data = filter(tdat, x < -2.5), aes(x = x, ymax = y), ymin = 0, fill = "blue") +
      geom_vline(xintercept = 2.5, lty = 2, col = 2)
    ```
    
    
# Estimation and Inference in R

- 















