---
title: 'MLR III: Special Predictors'
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
---

```{r setup, include=FALSE}
library(latex2exp)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Incorporating/interpreting quadratic terms.
- Incorporating/interpreting interaction effects.
- Incorporationg/interpreting categorical variables (through incators).
- Chapter 8 of KNNL.

# Quadratic Terms

- Consider the Muscle Mass data which explores the association between age and muscle mass. You can read about it [here](https://dcgerard.github.io/stat_415_615/data.html#Muscle_Mass).

    ```{r, message = FALSE}
    library(tidyverse)
    library(broom)
    muscle <- read_csv("https://dcgerard.github.io/stat_415_615/data/muscle.csv")
    glimpse(muscle)
    ```

- These data look like a quadratic fit could help
    ```{r}
    qplot(x = age, y = mass, data = muscle) +
      geom_smooth(se = FALSE)
    
    lm_musc <- lm(mass ~ age, data = muscle)
    a_musc <- augment(lm_musc)
    qplot(x = .fitted, y = .resid, data = a_musc) +
      geom_hline(yintercept = 0, lty = 2, col = 2)
    ```
    
- A quadratice regression model with one predictor variable is
    $$
    Y_i = \beta_0 + \beta_1 X_i + \beta_{2}X_{i}^2 + \epsilon_i
    $$
    with the usual assumptions on the errors.

- We fit this in R by first creating a new variable, say `age2`, with contains `age` squared.
    ```{r}
    muscle <- mutate(muscle, age2 = age^2)
    glimpse(muscle)
    ```

- We then fit a multiple linear regression model using `age` and `age2` as predictors.

    ```{r}
    lm_musc2 <- lm(mass ~ age + age2, data = muscle)
    tidy(lm_musc2)
    ```

- The estimated regression surface is
    $$
    \hat{Y} = 207.360 - 2.964X + 0.015X^2
    $$
    
- The $p$-value corresponding to `age2` is a test for the quadratic term (linear regression as the null versus quadratic regression as the alternative). The $p$-value in this case (0.08) says that we only have week evidence of a quadratic relationship.

- **Exercise**: Write out the null and alternative models associated with the $p$-value of 0.08109.

    ```{block, eval = FALSE, echo = FALSE}
    - $H_0: Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$
    - $H_A: Y_i = \beta_0 + \beta_1 X_i + \beta_2X_i^2 + \epsilon_i$
    ```


- **Exercise**: What are the null and alternative models associated with the $p$-value of 0.004535?

    ```{block, eval = FALSE, echo = FALSE}
    - $H_0: Y_i = \beta_0 + \beta_1 X_i^2 + \epsilon_i$
    - $H_A: Y_i = \beta_0 + \beta_1 X_i + \beta_2X_i^2 + \epsilon_i$
    ```

- It's possible to fit higher order polynomials. E.g. a cubic polynomial
    $$
    Y_i = \beta_0 + \beta_1 X_i + \beta_{2}X_{i}^2 + \beta_3 X_{i}^3 + \epsilon_i
    $$
    We would do this via
    
    ```{r}
    muscle <- mutate(muscle, age3 = age^3)
    lm_m3 <- lm(mass ~ age + age2 + age3, data = muscle)
    tidy(lm_m3)
    ```
    However, it is rarely a good idea to fit terms higher than quadratic. This is because
    
    1. They tend to be sensative to overfitting and
    2. They are hard to interpret.
    
    So at that point, you should just fit a cubic spline to these data, since it will be equally uninterpretable.
    
- If you include a quadratic term you should **always** include the linear term as well.

- That is, you should **never** fit the model
    $$
    Y_i = \beta_0 + \beta_1 X_i^2 + \epsilon_i$
    $$
    even if the $p$-value is very high.
    
- Why? This follows the same logic as always including the intercept term in the model. Lower order terms are thought to provide more basic information on the relationship, so you should include them.

- More generally, if you do end up using a cubic term, you should always include both linear and quadratic terms in the model, etc...


- When there are multiple predictors in the model, it is usual to denote quadratic coefficients with repeat indices. E.g.
    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \beta_{11}X_{i1}^2 + \beta_{2}X_{i2} + \beta_{22}X_{i2}^2 + \beta_{12}X_{i1}X_{i2} + \epsilon_i
    $$
    
- **Exericse**: Write out a model that contains two predictors, $X_{i1}$ and $X_{i2}$, where only $X_{i2}$ is quadratic (and so the model is linear in $X_{i2}$). Use the repeated indexing that we just introduced.

    ```{block, eval = FALSE, echo = FALSE}
    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \beta_{11}X_{i1}^2 + \beta_{2}X_{i2} + \epsilon_i
    $$
    ```

# Categorical Variables

## Two classes

- An innovation in the insurance industry was introduced, and a researcher wanted to study what factors affect how quickly different insurance firms adopted this new innovation. Variables include
    - `months`: How long, in months, it took the firm to adopt the new innovation.
    - `size`: The amount of total assets of the insurance firm, in millions of dollars.
    - `type`: The type of firm. Either a mutual company (`"mutual"`) or a stock company (`"stock"`).
    
    You can load these data into R via:
    ```{r, message = FALSE}
    firm <- read_csv("https://dcgerard.github.io/stat_415_615/data/firm.csv")
    glimpse(firm)
    ```

- Recall that we deal with categorical variables by creating indicators
    $$
    X_{i2} =
    \begin{cases}
    1 & \text{ if stock company}\\
    0 & \text{ otherwise}
    \end{cases}
    $$

- If a categorical variable has $c$ classes, then we need to use $c-1$ indicator variables to represent this categorical variable. 

- Here, $c = 2$ (for `"stock"` and `"mutual"`), so we only need $c-1=1$ indicator variable.

- Let $Y_i$ be the number of months elapsed for company $i$, and $X_{i1}$ be the size of the firm in millions of dollars. Then we will fit the following model.

    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i
    $$
    
- The interpretation of $\beta_1$ is as usual. Firms that are the same type, but are 10 million dollars larger, tended to take $10 \times \beta_1$ months less to innovate.
    
- To interpret $\beta_2$, consider that the model for mutual companies is
    $$
    Y_i = \beta_0 + \beta_1X_{i1} + \epsilon_i
    $$
    while the model for stock companies is
    $$
    Y_i = \beta_0 + \beta_2 + \beta_1X_{i1} + \epsilon_i
    $$
    where $\beta_0$ and $\beta_1$ are the same in both instances.
    
- This means that $\beta_2$ is the expected difference in months between mutual and stock companies that are about the same size.

- Let's visualize this model
    ```{r, echo = FALSE, eval = FALSE}
    lm(months ~ size + type, data = firm)
    
    firm %>%
      rename(Type = type) %>%
      ggplot(mapping = aes(x = size, y = months, color = Type)) +
      geom_point() +
      theme_classic() +
      geom_abline(slope = -0.102, intercept = 33.874, color = "#005AB5", lwd = 1.5) +
      geom_abline(slope = -0.102, intercept = 41.93, color = "#DC3220", lwd = 1.5) +
      xlab("Size of Firm (millions of dollars)") +
      ylab("Number of Months Elapsed") +
      scale_color_manual(values = c("#005AB5", "#DC3220")) ->
      pl
      ## ggsave(filename = "./figs/cat_interp.pdf", plot = pl, height = 4, width = 5.5)
    ```
    
    ```{r,echo = FALSE}
    knitr::include_graphics(path = "./figs/cat_interp.png")
    ```

- In R, you fit this model by first converting the categorical variable into a **factor** with `factor()`. You specify the order of the levels by the `levels` argument, where the first level is the **reference level** (the one that does not have an indicator for it).
    ```{r}
    firm <- mutate(firm, type = factor(type, levels = c("mutual", "stock")))
    ```
    Then you can use the new factor variable in `lm()`. It will automatically create $c-1$ indicator variables to fit.
    ```{r}
    lm_firm <- lm(months ~ size + type, data = firm)
    tidy(lm_firm, conf.int = TRUE)
    ```
    We conclude that stock companies tended to take 8 months longer than mutual companies to adopt the new innovation (95\% CI of 5 to 11 months longer), adjusting for company size. Companies of the same type that had \$10 million more assets tended to take 1 fewer month to adopt the new innovation (95\% CI of 0.8 to 1.2 fewer months).
    
- If you wanted to change which level is the reference level of a factor variable, you could use `fct_relevel()` from the `{forcats}` package (apart of the tidyverse).
    ```{r}
    firm <- mutate(firm, type = fct_relevel(type, "stock", "mutual"))
    lm_firm <- lm(months ~ size + type, data = firm)
    tidy(lm_firm, conf.int = TRUE)
    ```


- Why not fit two separate regressions (one for each firm)?
    1. Enforces interpretability of $\beta_2$ parameter since otherwise $\beta_1$ would differ between firms.
    2. Use all of the data to estimate $\beta_1$.
    3. Use all of the data to estimate $\sigma^2$.
    
## More than two classes

- If a categorical variable has $c$ classes, then you use $c-1$ indicator variables to represent this variable.


- The `mpg` dataset's `drv` variable has classes `"f"`, `"4"`, and `"r"`.
    ```{r}
    data("mpg")
    unique(mpg$drv)
    ```

- We can represent this categorical variable with two indicator variables:

    \begin{align}
    X_{i1} &= 
    \begin{cases}
    1 & \text{ if forward-wheel drive}\\
    0 & \text{ otherwise}
    \end{cases}\\
    X_{i2} &= 
    \begin{cases}
    1 & \text{ if 4-wheel drive}\\
    0 & \text{ otherwise}
    \end{cases}
    \end{align}
    
- Suppose we want to explore the association between log-`cty` with `displ` and `drv`. Let $Y$ be the city miles per gallon, $X_1$ and $X_2$ be the indicator variables for forward- and 4-wheel drive cars, and let $X_3$ be the car's engine displacement (in liters). Then our model is
    $$
    \log(Y_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i
    $$

- The model for rear-wheel drive cars is
    $$
    \log(Y_i) = \beta_0 + \beta_3X_{i3} + \epsilon_i
    $$
    The model for forward-wheel drive cars is
    $$
    \log(Y_i) = \beta_0 + \beta_1 + \beta_3X_{i3} + \epsilon_i
    $$
    The model for 4-wheel drive cars is
    $$
    \log(Y_i) = \beta_0 + \beta_2 + \beta_3X_{i3} + \epsilon_i
    $$

- Interpretation:
    - $\beta_1$ is the average difference in city mpg between rear-wheel and forward drive cars of about the same engine size.
    
    - $\beta_2$ is the average difference in city mpg between rear-wheel and 4-wheel drive cars of about the same engine size.
    
    - $\beta_2 - \beta_1$ is the average difference in city mpg between 4-wheel and forward wheel drive cars of about the same engine size.
    
    - $\beta_3$ is the average difference in city mpg between cars that are of the same type, but have 1 liter different engine displacement.

- We can visualize this model below:

    ```{r, echo = FALSE, eval = FALSE}
    ggplot(mpg, aes(x = displ, y = log(cty), color = drv)) +
      xlim(range(mpg$displ)) +
      ylim(range(log(mpg$cty))) +
      xlab("Engine Displacement (liters)") +
      ylab("log(City mpg)") +
      geom_abline(slope = -0.14256, intercept = 3.37044+0.3, color = "#005AB5", lwd = 1.5) +
      geom_abline(slope = -0.14256, intercept = 3.37044 - 0.1, color = "#DC3220", lwd = 1.5) +
      geom_abline(slope = -0.14256, intercept = 3.37044 - 0.3, color = "#FFC107", lwd = 1.5) +
      theme_classic() +
      theme(axis.text.y = element_blank(),
            axis.ticks.y = element_blank()) ->
      pl
    # ggsave(filename = "./figs/cat_interp_3.pdf", plot = pl, height = 4, width = 5.5)
    ```

    ```{r,echo = FALSE}
    knitr::include_graphics(path = "./figs/cat_interp3.png")
    ```

- We fit this model in R by first selecting the order of the levels of `drv` to include rear-wheel drive cars as the reference class.
    ```{r}
    mpg <- mutate(mpg, 
                  l_cty = log(cty),
                  drv = factor(drv, levels = c("r", "f", "4")))
    ```
    Then we run `lm()`.
    ```{r}
    lm_mpg <- lm(l_cty ~ drv + displ, data = mpg)
    tidy(lm_mpg, conf.int = TRUE)
    ```

- The estimated regression surface is
    $$
    \log(\text{city mpg}) = 3.37 - 0.026\times \text{forward} - 0.158 \times \text{4-wheel} - 0.142 \times \text{displacement}
    $$

- We obtained confidence intervals for each of these coefficients, but how do we obtain a confidence interval for $\beta_2 - \beta_1$ (the difference in average mpg between forward- and 4-wheel drive cars of the same size)? There are some third-party packages that do this. But an easy way is to just hange which class is the reference

    ```{r, message=FALSE}
    mpg <- mutate(mpg, drv = fct_relevel(drv, "f", "r", "4"))
    lm_mpg_2 <- lm(l_cty ~ drv + displ, data = mpg)
    tidy(lm_mpg_2, conf.int = TRUE)
    ```
    So this confidence interval is -0.175 to -0.089.


# Interaction Effects

- An **interaction** between two variables means that the slope with respect to one variable changes with the value of the second variable.

- The way we represent this is by multiplying the two variables together.
    $$
    Y_{i} = \beta_0 + \beta_1X_{i1} + \beta_{2}X_{i2} + \beta_{12}X_{i1}X_{i2} + \epsilon_i
    $$

- The slope with respect to $X_1$ when $X_2$ is fixed is $\beta_1 + \beta_{12}X_2$

- The slope with respect to $X_2$ when $X_1$ is fixed is $\beta_2 + \beta_{12}X_1$

- NOTE: $\beta_1$ and $\beta_2$ **no longer** represent the expected difference in $Y$ given a difference of $X_1$ or $X_2$, respectively.

- The association between $X_1$ and $Y$ depends on the level of $X_2$. Likewise, the association between $X_2$ and $Y$ depends on the level of $X_1$.

- When $\beta_{12}$ is positive, this is called an interaction of the **reinforcement** or **synergistic** type. The slope is more positive for more positive levels of $X_1$ or $X_2$.

- When $\beta_{12}$ is negative, this is called an interaction of the **interference** or **antagonistic** type. The slope is less positive for more positive levels of $X_1$ or $X_2$.

- XYZ Plots demonstrating synergistic and antagonistic interactions.

- XYZ Demonstrating difficulty in interpretation.

- XYZ Interactions and categorical variables.
