---
title: "MLR: Practical Considerations"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
---

```{r setup, include=FALSE}
library(latex2exp)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Choice of scaling predictors for interpretation.
- Multicollinearity
- Effect sizes, practical significance, and statistical significance.
- Chapter 12 from ROS
- Chapter 7 from KNNL

<!-- # Steps of an analysis -->

<!-- - For **any analysis** where you are applying a model, the steps you should take are: -->
<!--     1. Exploratory data analysis -->
<!--     2. Choose the form of the model. -->
<!--     3. Fit the model to the data. -->
<!--     4. Assess how well the model describes the data. -->
<!--     5. Use the model to address the question of interest. -->

<!-- - You should be performing each of these steps when you apply a model to data. -->

# Scales

## Interpretable Scales

- Recall [earnings data](https://dcgerard.github.io/stat_415_615/data.html#Earnings_Data) exploring the relationship between height and earnings.

    ```{r, message=FALSE}
    library(tidyverse)
    library(broom)
    earnings <- read_csv("https://dcgerard.github.io/stat_415_615/data/earnings.csv")
    earnings <- mutate(earnings,
                       height_mm = height * 25.4,
                       height_in = height,
                       height_mi = height / 63360)
                       
    tidy(lm(earn ~ height_mm, data = earnings))
    tidy(lm(earn ~ height_in, data = earnings))
    tidy(lm(earn ~ height_mi, data = earnings))
    ```

- The estimated regression line depends on the units for $X$ and $Y$.
    - earnings = -85,000 + 36 * height(millimeters)
    - earnings = -85,000 + 1600 * height(inches)
    - earnings = -85,000 + 101,001,000 * height(miles)
    
- Does this mean that height in miles is a stronger predictor of earnings (since its coefficient estimate is larger?)
    - NO! ðŸ¤¦
    - The scale of height was chosen arbitrarily. These three equations all reflect the same underlying information.
    
- By itself, **the size of the regression coefficients tells you nothing about the importance of a predictor** because this coefficient can change if you change the units of the predictor.

- Better to choose a scale which is in the most interpretable units.

- A 1 millimeter difference in height is not a meaningful difference, in terms of the distribution of human heights.

- A 1 mile difference in height is unrealistic.

- A 1 inch difference in height is more reasonable.

    ```{r}
    qplot(x = height_in, data = earnings, bins = 20)
    ```

- You can use the standard deviation of the variables to help guide you. If the standard deviation is between 0.5 and 10, then you have a good scale. But this is a rough rule of thumb.

    ```{r}
    sd(earnings$height_mm)
    sd(earnings$height_in)
    sd(earnings$height_mi)
    ```
    
- What if you are logging variables?
    - Recall, we interpret log-relationships in terms of *multiplicative* changes. So being twice as tall is just as good in inches (84 inches versus 42 inches) as it is in miles (0.00133 miles versus 0.00066 miles). 
    - So choosing the units does not really matter.
    - But choosing the *size* of the multiplicative difference is important (50\% larger versus twice as large versus ten times as large, etc). 
    - E.g., there are very few humans who are twice as large as other humans. So a better multiplicative difference would be 1.05 (5\% taller).
    
    ```{r}
    earnings <- mutate(earnings, log_height = log(height))
    tidy(lm(earn ~ log_height, data = earnings))
    ```
    
    Individuals that are 5\% taller tend to make $106362 * \log(1.05) = \$5189$ more on average.
    
    - You can choose a good multiplier by seeing if the standard deviation of the log of that base is between 0.5 and 10 (but this is a rough rule of thumb).

    ```{r}
    sd(log(earnings$height, base = 10)) # bad
    sd(log(earnings$height, base = 2)) # bad
    sd(log(earnings$height, base = 1.05)) # good
    sd(log(earnings$height, base = 1.01)) # good
    ```


- **Exercise**: A study was exploring the association between a country's wine consumption (liters per person per year) and mortality rate (deaths per 1000 individuals). Researchers found that a power-law relationship was a fine approximation. Based on the below plot alone, choose a good multiplier to interpret the coefficient of a regression of log-mortality on log-wine. Verify your result using our rule of thumb above.

    ```{r}
    library(Sleuth3)
    data("ex0823")
    wine <- ex0823
    qplot(x = Wine, y = Mortality, data = wine)
    ```
    
    ```{block, echo = FALSE, eval = TRUE}
    2 and 10 would be the best.
    ```

    ```{r, echo = FALSE, eval = FALSE}
    sd(log(wine$Wine, base = 1.1))
    sd(log(wine$Wine, base = 1.5))
    sd(log(wine$Wine, base = 2))
    sd(log(wine$Wine, base = 10))
    ```

- **Exercise**: XYZ Have them determine a good scale when it is a linear relationship, not a power-law.

## Z-score scales

- The idea behind using standard deviations is that roughly (i.e. generally only exactly if normal) 68\% of observations will be within 1 standard deviation of the mean. So this difference represents something meaningful.

- Above, I suggested to use standard deviations to guide your scaling. An alternative choice is to scale explicitely *by* standard deviations.

- The $Z$-score for a variable is its value, minus its mean, divided by its standard deviation.
    $$
    Z_i = \frac{X_i - \bar{X}}{s_x}
    $$

- $Z_i$ is now in units of *standard deviations of $X_i$ from its mean*. 
    - $Z_i = 1$ means individual $i$ has an $X_i$ value that is one standard deviation above the mean.
    - $Z_i = -1$ means individual $i$ has an $X_i$ value that is one standard deviation below the mean.
    - $Z_i = 2$ means individual $i$ has an $X_i$ value that is two standard deviations above the mean.
    - etc

- **Example**: XYZ demonstrate $Z$-scores here on a real dataset.
    
- If you do this to all variables in your model, you can use the following multiple regression model:
    $$
    Y_i = \beta_0^* + \beta_1^*Z_{i1} + \beta_2^* Z_{i2} + \cdots + \beta_{p-1}^*Z_{i, p-1} + \epsilon_i
    $$
- If we run a linear regression under this model, then the interpretation of coefficient $\beta_i^*$ is:
    > Individuals that are 1 standard deviation larger in X_i tend to have $\beta_1^*$ higher $Y_i$ on average, adjusting for all other predictors in our model.

- **Example**: XYZ Fit a multiple linear regression model, then scale. Provide interpretation in terms of model.

- Note: If $X_i^* = \frac{X_i - a}{b}$, then $\beta_i^* = b\beta_i$ and $\hat{\beta}_i^* = b\hat{\beta}_i$.

- **Exercise**: XYZ Let them fit both with and without $Z$-score scaling. Compare coefficients. Provide interpretation using both models. See if the coefficients of other variables change when we scale a different variable.

# Multicollinearity

## Be careful about comparisons

- Sometimes, no data points exist that are one unit larger $X_1$ but the same $X_2$, because of multicollinearity. Thus, this interpretation is suspect. 

- Multicollinearity therefore also messes with interpretation.

# Practical versus statistical significance

- Tests can have tiny $p$-values, but the effect sizes might be small.
    - This can occur if we have a large sample size when two variables have a weak relationship.

- Tests can have large $p$-values, but the effect sizes might be huge.
    - This can occur if we have a small sample size when two variables have a strong relationship.
    
- So **never** use $p$-values to describe the strength of the relationship between two variables. 
    - This is what other statistics like $R^2$, partial $R^2$, sums of squares, and Cohen's $d$ are meant for.

- Only use $p$-values to describe the strength of the **evidence that there is a relationship** between two variables.

# Comparing effect sizes to residual standard error

- It is always a good idea to compare the effect sizes to the residual standard error.
    - Recall: Residual standard error is the estimated population standard deviation.
    - Recall: Residual standard error is the square root of the MSE.

- **Example**: Recall the [earnings data](https://dcgerard.github.io/stat_415_615/data.html#Earnings_Data). Let's read it in and fit a model for log-earnings on height:

    ```{r, message = FALSE}
    library(tidyverse)
    earnings <- read_csv("https://dcgerard.github.io/stat_415_615/data/earnings.csv")
    earnings <- mutate(earnings, log_earn = log(earn))
    earnings <- filter(earnings, is.finite(log_earn))
    qplot(x = height, y = log_earn, data = earnings)
    lm_earn <- lm(log_earn ~ height, data = earnings)
    ```
    
    ```{r}
    library(broom)
    tidy(lm_earn)
    ```
    
    The $p$-value is **tiny**. So is this a huge effect? Not really. Let's discuss.
    
    Individuals that are a whole foot taller earn about $0.05704 \times 12 = 0.6845$ log-dollars more. This corresponds to about twice as much money ($e^0.6845 = 1.98). This seems large. However, let's look at the residual standard deviation.
    
    ```{r}
    glance(lm_earn)
    ```
    
    The residual SD is 0.8772. This is the average variability about the regression line and is larger even when comparing folks that are a full foot different in height.
    
    If we compare prediction intervals between a 5'2'' individual and a 6'2'' individual (ignoring the appropriateness of the normal model for now), we have
    
    ```{r}
    predict(object = lm_earn, 
            newdata = data.frame(height = c(5.2 * 12, 6.2 * 12)),
            interval = "prediction") %>%
      exp()
    ```
    So it is true that the taller individual is expected to make twice as much money (\$12,938 versus \$25,652), but the range of individuals at each level is huge (\$2312 to \$72,387 and \$4579 to \$143,718). And this is on the larger side of comparisons between individuals (most individuals are less than a foot different).
    



