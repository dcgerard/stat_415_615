---
title: "MLR: Practical Considerations"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
---

```{r setup, include=FALSE}
library(latex2exp)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3, fig.width = 4)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Choice of scaling predictors for interpretation.
- Multicollinearity
- Effect sizes, practical significance, and statistical significance.
- Chapter 12 from ROS
- Section 7.6 from KNNL

<!-- # Steps of an analysis -->

<!-- - For **any analysis** where you are applying a model, the steps you should take are: -->
<!--     1. Exploratory data analysis -->
<!--     2. Choose the form of the model. -->
<!--     3. Fit the model to the data. -->
<!--     4. Assess how well the model describes the data. -->
<!--     5. Use the model to address the question of interest. -->

<!-- - You should be performing each of these steps when you apply a model to data. -->

# Scales

## Interpretable Scales

- Recall [earnings data](https://dcgerard.github.io/stat_415_615/data.html#Earnings_Data) exploring the relationship between height and earnings.

    ```{r, message=FALSE}
    library(tidyverse)
    library(broom)
    earnings <- read_csv("https://dcgerard.github.io/stat_415_615/data/earnings.csv")
    earnings <- mutate(earnings,
                       height_mm = height * 25.4,
                       height_in = height,
                       height_mi = height / 63360)
                       
    tidy(lm(earn ~ height_mm, data = earnings))
    tidy(lm(earn ~ height_in, data = earnings))
    tidy(lm(earn ~ height_mi, data = earnings))
    ```

- The estimated regression line depends on the units for $X$ and $Y$.
    - earnings = -85,000 + 36 * height(millimeters)
    - earnings = -85,000 + 1600 * height(inches)
    - earnings = -85,000 + 101,001,000 * height(miles)
    
- Does this mean that height in miles is a stronger predictor of earnings (since its coefficient estimate is larger?)
    - NO! ðŸ¤¦
    - The scale of height was chosen arbitrarily. These three equations all reflect the same underlying information.
    
- By itself, **the size of the regression coefficients tells you nothing about the importance of a predictor** because this coefficient can change if you change the units of the predictor.

- **Exercise**: When exploring the effect of the percent of of the population with bachelor's degrees and percent unemployment on percent below the poverty line in the [County Demographic Information](https://dcgerard.github.io/stat_415_615/data.html#County_Demographic_Information) data, researchers obtained the following output:
    ```{r, message = FALSE}
    cdi <- read_csv("https://dcgerard.github.io/stat_415_615/data/cdi.csv")
    cdi <- select(cdi, poverty, bachelors, unemployment)
    lmout <- lm(poverty ~ bachelors + unemployment, data = cdi)
    tidy(lmout)
    ```
    
    Which has a bigger effect on `poverty`: `bachelors` or `unemployment` and why?
    
    ```{block, eval = FALSE, echo = FALSE}
    You cannot tell based on the given information.
    ```

- You should choose a scale which is in the most interpretable units.

- A 1 millimeter difference in height is not a meaningful difference, in terms of the distribution of human heights.

- A 1 mile difference in height is unrealistic.

- A 1 inch difference in height is more reasonable.

    ```{r}
    qplot(x = height_in, data = earnings, bins = 20)
    ```

- You can use the standard deviation of the variables to help guide you. If the standard deviation is between 0.5 and 10, then you have a good scale. But this is a rough rule of thumb.

    ```{r}
    sd(earnings$height_mm)
    sd(earnings$height_in)
    sd(earnings$height_mi)
    ```
    
- What if you are logging variables?
    - Recall, we interpret log-relationships in terms of *multiplicative* changes. So being twice as tall is just as good in inches (84 inches versus 42 inches) as it is in miles (0.00133 miles versus 0.00066 miles). 
    - So choosing the units does not really matter.
    - But choosing the *size* of the multiplicative difference is important (50\% larger versus twice as large versus ten times as large, etc). 
    - E.g., there are very few humans who are twice as large as other humans. So a better multiplicative difference would be 1.05 (5\% taller).
    
    ```{r}
    earnings <- mutate(earnings, log_height = log(height))
    tidy(lm(earn ~ log_height, data = earnings))
    ```
    
    Individuals that are 5\% taller tend to make $106362 * \log(1.05) = \$5189$ more on average.
    
    - You can choose a good multiplier by seeing if the standard deviation of the log of that base is between 0.5 and 10 (but this is a rough rule of thumb).

    ```{r}
    sd(log(earnings$height, base = 10)) # bad
    sd(log(earnings$height, base = 2)) # bad
    sd(log(earnings$height, base = 1.05)) # good
    sd(log(earnings$height, base = 1.01)) # good
    ```

- **Exercise**: A study was exploring the association between a country's wine consumption (liters per person per year) and mortality rate (deaths per 1000 individuals). Researchers found that a power-law relationship was a fine approximation. Based on the below plot alone, choose a good multiplier to interpret the coefficient of a regression of log-mortality on log-wine. Verify your result using our rule of thumb above.

    ```{r}
    library(Sleuth3)
    data("ex0823")
    wine <- ex0823
    qplot(x = Wine, y = Mortality, data = wine)
    ```
    
    ```{block, echo = FALSE, eval = TRUE}
    2 and 10 would be the best.
    ```

    ```{r, echo = FALSE, eval = FALSE}
    sd(log(wine$Wine, base = 1.1))
    sd(log(wine$Wine, base = 1.5))
    sd(log(wine$Wine, base = 2))
    sd(log(wine$Wine, base = 10))
    ```

- **Exercise**: World record mile time progressions from 1913 to 1999. Data are from Figure A.1 of ROS. Variables include
    - `year`: The date (in years) of the new world record.
    - `seconds`: The new world record (in seconds).
    ```{r, message = FALSE}
    mile <- read_csv("https://dcgerard.github.io/stat_415_615/data/mile.csv")
    glimpse(mile)
    ```
    
    The default scale of the $X$ variable is in years. Is this the best scale? If not, transform the $X$-variable to a more appropriate scale, fit a regression of world record on year, then provide an interpretation for the slope.
    
    ```{r, echo = FALSE, eval = FALSE}
    sd(mile$year)
    ```
    ```{block, eval = FALSE, echo = FALSE}
    Maybe decades would be better.
    ```
    ```{r, echo = FALSE, eval = FALSE}
    mile <- mutate(mile, year_dec = year / 10)
    qplot(x = year_dec, y = seconds, data = mile) +
      geom_smooth(method = "lm", se = FALSE)
    lm_mile <- lm(seconds ~ year_dec, data = mile)
    tidy(lm_mile, conf.int = TRUE)
    ```
    
    ```{block, eval = FALSE, echo = FALSE}
    Each decade, the world record tended to be about 3.9 seconds lower (95\% confidence interval of 3.7 to 4.2 seconds lower).
    ```

## Z-score scales

- The idea behind using standard deviations is that roughly (i.e. generally only exactly if normal) 68\% of observations will be within 1 standard deviation of the mean. So this difference represents something meaningful.

- Above, I suggested to use standard deviations to *guide* your scaling. Alternatively, we could scale explicitly *by* standard deviations.

- The $Z$-score for a variable is its value, minus its mean, divided by its standard deviation.
    $$
    Z_i = \frac{X_i - \bar{X}}{s_x}
    $$

- $Z_i$ is now in units of *standard deviations of $X_i$ from its mean*. 
    - $Z_i = 1$ means individual $i$ has an $X_i$ value that is one standard deviation above the mean.
    - $Z_i = -1$ means individual $i$ has an $X_i$ value that is one standard deviation below the mean.
    - $Z_i = 2$ means individual $i$ has an $X_i$ value that is two standard deviations above the mean.
    - etc

- **Example**: For the heights and earnings dataset, let's create $Z$-scores.

    ```{r}
    earnings <- mutate(earnings, 
                       height_z = (height - mean(height)) / sd(height),
                       earn_z = (earn - mean(earn)) / sd(earn))
    glimpse(select(earnings, earn_z, height_z))
    ```
    Centering and scaling does not change the underlying information.
    ```{r}
    qplot(x = height, y = earn, data = earnings) + geom_smooth(method = "lm", se = FALSE)
    qplot(x = height_z, y = earn_z, data = earnings) + geom_smooth(method = "lm", se = FALSE)
    ```
    But this will make interpretation a little easier.
    ```{r}
    lm_earn_z <- lm(earn_z ~ height_z, data = earnings)
    tidy(lm_earn_z, conf.int = TRUE)
    ```
    So a 1 standard deviation difference in height corresponds to 0.27 standard deviations higher earnings.
        
- If you do this to all variables in your model, you can use the following multiple regression model:
    $$
    Y_i = \beta_0^* + \beta_1^*Z_{i1} + \beta_2^* Z_{i2} + \cdots + \beta_{p-1}^*Z_{i, p-1} + \epsilon_i
    $$
- If we run a linear regression under this model, then the interpretation of coefficient $\beta_i^*$ is:
    > Individuals that are 1 standard deviation larger in X_i tend to have $\beta_1^*$ higher $Y_i$ on average, adjusting for all other predictors in our model.

- **Example**: XYZ Fit a multiple linear regression model, then scale. Provide interpretation in terms of model.

- Note: If $X_i^* = \frac{X_i - a}{b}$, then $\beta_i^* = b\beta_i$ and $\hat{\beta}_i^* = b\hat{\beta}_i$.

- **Exercise**: XYZ Let them fit both with and without $Z$-score scaling. Compare coefficients. Provide interpretation using both models. See if the coefficients of other variables change when we scale a different variable.

# Multicollinearity

- **Multicollinearity**: Correlation between predictors.

- Multicollinearity is the rule, not the exception.
    - In observational studies, you will always have some correlation between predictors.
    - But too much correlation can be a really bad thing (how much is too much is calculated in Chapter 10).

- The consequences of multicollinearity:
    1. Estimates of coefficients change based on what other predictors are in the model. 
    2. Extra sums of squares change based on what other predictors are in the model.
    3. Standard errors are inflated.
    4. Fits and predictions and MSE are less affected.
    5. The significance of some predictors can be masked.
    6. Interpretation becomes more difficult.
    
- We will demonstrate all of these issues through the body fat example with variables
    - $X_1$ `triceps`: Triceps skinfold thickness.
    - $X_2$ `thigh`: Thigh circumference.
    - $X_3$ `midarm`: Midarm circumference
    - $Y$ `fat`: Body fat.
    
    ```{r, message = FALSE}
    body <- read_csv("https://dcgerard.github.io/stat_415_615/data/body.csv")
    glimpse(body)
    ```
    
- These data exhibit multicollinearity
    ```{r, message=FALSE}
    library(GGally)
    ggpairs(data = body)
    ```

- Notice that $X_1$ = triceps and $X_2$ = thigh are highly correlated with each other, but these are less correlated with $X_3$ = midarm.

- Let's fit a bunch of models
    ```{r}
    lm_tr <- lm(fat ~ triceps, data = body)
    lm_th <- lm(fat ~ thigh, data = body)
    lm_tr_th <- lm(fat ~ triceps + thigh, data = body)
    lm_tr_th_m <- lm(fat ~ triceps + thigh + midarm, data = body)
    ```

## Coefficient Estimates Change

```{r}
select(tidy(lm_tr), term, estimate)
select(tidy(lm_th), term, estimate)
select(tidy(lm_tr_th), term, estimate)
select(tidy(lm_tr_th_m), term, estimate)
```

- Coefficient estimates change based on what predictors are in the model.

| Variables in Model | $\hat{\beta}_1$ | $\hat{\beta}_2$ |
|:------------------:|:---------------:|:---------------:|
|        $X_1$       |       0.86      |        -        |
|        $X_2$       |        -        |       0.86      |
|     $X_1, X_2$     |       0.22      |       0.66      |
|   $X_1, X_2, X_3$  |       4.33      |      -2.86      |



- ðŸ˜² Look how much they change!

- **Exercise**: What is the model for each row in the above table?

    ```{block, eval = FALSE, echo = FALSE}
    - Row 1
        $$
        Y_i = \beta_0 + \beta_1 X_{i1} + \epsilon_i
        $$
        
    - Row 2
        $$
        Y_i = \beta_0 + \beta_2 X_{i2} + \epsilon_i
        $$
        
    - Row 3
        $$
        Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2X_{i2} + \epsilon_i
        $$
        
    - Row 4
        $$
        Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2X_{i2} + \beta_3 X_{i3} + \epsilon_i
        $$
    ```

- We saw that $X_1$ and $X_2$ were highly correlated, so it makes sense that those coefficients change when they are included together in the model.

- But $X_3$ was not highly correlated with either $X_1$ nor $X_2$, so why did the coefficients change so much?

- It turns out that, although $X_3$ is not highly correlated with $X_1$ and is not highly correlated with $X_2$, it is highly correlated with the combination of $X_1$ and $X_2$. 

- We can measure the strength of the association between $X_3$ and the combination of $X_1$ and $X_2$ by the multiple $R^2$ value of regression of $X_3$ on $X_1$ and $X_2$

    ```{r}
    glance(lm(midarm ~ triceps + thigh, data = body))$r.squared
    ```

- So, you cannot detect multicollinearity by just looking at pairwise correlations between predictors.
    - Chapter 10 provides more sophisticated measures of multicollinearity.
    
    
- Key Takeaway: a regression coefficient does not reflect any inherent effect of a particular predictor on the response, but only a marginal or partial effect given whatever other predictors are in the model.

## Extra Sums of Squares Change

```{r, message=FALSE}
library(car)
Anova(lm_tr)
Anova(lm_tr_th)
Anova(lm_tr_th_m)
```


- Extra sums of squares will differ based on what other predictors are in the model
    - $SSR(X_1) = 352$
    - $SSR(X_1|X_2) = 3.5$
    - $SSR(X_1|X_2, X_3) = 12.7$

- Recall, the extra sum of squares is how much the error sum of squares is reduced when we include a predictor in a model with other predictors already in the model.

- The extra sum of squares can increase or decrease depending on what other variables are in the model.

- Key Takeaway: there is no unique sum of squares that can be ascribed to any one predictor as reflecting its effect in reducing the total variation in $Y$ --- this depends on what other predictors are in the model.

## Standard Errors are Larger

```{r}
select(tidy(lm_tr), term, std.error)
select(tidy(lm_th), term, std.error)
select(tidy(lm_tr_th), term, std.error)
select(tidy(lm_tr_th_m), term, std.error)
```

| Variables in Model | $s\{\hat{\beta}_1\}$ | $s\{\hat{\beta}_2\}$ |
|:------------------:|:--------------------:|:--------------------:|
|        $X_1$       |         0.13         |           -          |
|        $X_2$       |           -          |         0.11         |
|     $X_1, X_2$     |         0.30         |         0.29         |
|   $X_1, X_2, X_3$  |         3.02         |         2.58         |

- Standard error increases as you include correlated predictors in the model.

- Why? Consider the case of perfectly correlated variables where $X_{i1} = X_{i2}$ for all $i$. Then for an number $a$
    \begin{align}
    \hat{Y}_i &= \hat{\beta}_0 + \hat{\beta}_1X_{i1} + \hat{\beta}_2X_{i2}\\
    &= \hat{\beta}_0 + (\hat{\beta}_1-a)X_{i1} + (\hat{\beta}_2 + a)X_{i2}
    \end{align}

    - So it is impossible to tell $\hat{\beta}_1$ and $\hat{\beta}_2$ apart. 
    
    - We don't know what effect to attribute to $X_1$ and what to attribute to $X_2$.
    


## Fitted Values, Predictions, and MSE are Relatively Stable

- The major issue with multicollinearity is that we can't tell what effect comes from which predictor.

- But the effect of multicollinearity on fitted values, predictions, and MSE is much more models.

- Confidence intervals for mean have about the same width (predictor levels chosen to have about the same mean estimate in each model)
    ```{r}
    df1 <- data.frame(triceps = 25)
    df2 <- data.frame(triceps = 25, thigh = 50)
    df3 <- data.frame(triceps = 25, thigh = 50, midarm = 28.8)
    predict(lm_tr, newdata = df1, interval = "confidence")
    predict(lm_tr_th, newdata = df2, interval = "confidence")
    predict(lm_tr_th_m, newdata = df3, interval = "confidence")
    ```

- Prediction intervals have about the same width
    ```{r}
    predict(lm_tr, newdata = df1, interval = "prediction")
    predict(lm_tr_th, newdata = df2, interval = "prediction")
    predict(lm_tr_th_m, newdata = df3, interval = "prediction")
    ```

- Residual standard deviation (square root of MSE) is about the same in all models

    ```{r}
    glance(lm_tr)$sigma
    glance(lm_tr_th)$sigma
    glance(lm_tr_th_m)$sigma
    ```

- Why? Consider the case of perfectly correlated variables where $X_{i1} = X_{i2}$ for all $i$. Then for an number $a$
    \begin{align}
    \hat{Y}_i &= \hat{\beta}_0 + \hat{\beta}_1X_{i1} + \hat{\beta}_2X_{i2}\\
    &= \hat{\beta}_0 + (\hat{\beta}_1-a)X_{i1} + (\hat{\beta}_2 + a)X_{i2}
    \end{align}
    
    - The estimated mean value does not change no matter what value of $a$ is provided (so predictions would also be robust to change).
    
    - This means the residuals would also not change no matter what value of $a$ is provided (so the MSE would be more robust to change).

- This is not to say that you should just include every predictor in the model. This is called "overfitting" and can cause bad issues (machine learning will discuss this).
    - The point is just that predictions are more *robust* to issues of multicollinearity.

## Masking Significance

- The $p$-values for all three variables are all large, indicating a lack of evidence to include any of them.
    ```{r}
    tidy(lm_tr_th_m)
    ```

- But the overall $F$-test indicates strong evidence that we should include at least one of them:
    ```{r}
    glance(lm_tr_th_m)$p.value
    ```

- The $t$-test for $H_0: \beta_1 = 0$ compares the two models:
    - $H_0: Y_i = \beta_0 + \beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i$
    - $H_A: Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i$
    
    and if $X_2$ is highly correlated with $X_1$, and $X_2$ is already in the model, then why do we also need to have $X_1$ in the model?
    
- The overall $F$-test compares the two models:
    - $H_0: Y_i = \beta_0 + \epsilon_i$
    - $H_A: Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i$

    So this test says that we should have at least one of these variables in the model.
    
- Key point: Just because the $p$-values from the $t$-tests are large does *not* mean that none of the predictors are associated with $Y$.

## The Difficulty of Interpretation

- Sometimes, no data points exist that are one unit larger $X_1$ but the same $X_2$, because of multicollinearity. Thus, this interpretation is suspect. 

- E.g.

- Multicollinearity therefore also messes with interpretation.

## Remedies for Multicollinearity

1. Drop some of the correlated predictor variables.

2. Develop "composite" predictors that combine the correlated predictors.
    - Do this manually.
    - Use [principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA).

3. Apply [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) (Chapter 10).

## OK, that sucked, but what about when there is no multicollinearity?

- Researchers were studying the effect of work crew size ($X_1$) and level of bonus pay ($X_2$) on crew productivity ($Y$).

    ```{r}
    crew <- tibble::tribble(
      ~size, ~bonus, ~productivity,
          4,      2,            42,
          4,      2,            39,
          4,      3,            48,
          4,      3,            51,
          6,      2,            49,
          6,      2,            53,
          6,      3,            61,
          6,      3,            60
      )
    ```
    
- `size` and `bonus` are both uncorrelated, by design.
    ```{r}
    cor(crew$size, crew$bonus)
    ```

- Let's fit these models:
    ```{r}
    lm_s <- lm(productivity ~ size, data = crew)
    lm_b <- lm(productivity ~ bonus, data = crew)
    lm_bs <- lm(productivity ~ bonus + size, data = crew)
    ```


- Notice that the estimated coefficients are unchanged when we add more predictors.   
    ```{r}
    tidy(lm_s)
    tidy(lm_b)
    tidy(lm_bs)
    ```
    
- The interpretation of the coefficients still changes based on what variables are in the model (controlling for the other predictor), but the effect size is estimated to be the same regardless.
    
- Did you also notice that the standard errors did not get inflated?

- The extra sums of squares are the same. That is
    $$
    SSR(X_1) = SSR(X_1|X_2)\\
    SSR(X_2) = SSR(X_2|X_1)
    $$

    so the relative reduction in the variation in $Y$ by including $X_1$ is the same no matter if $X_2$ is in the model.

    ```{r}
    Anova(lm_b)
    Anova(lm_s)
    Anova(lm_bs)
    ```

- Thus, it is relatively straightforward to determine if a variable should be included in a model, because this question may be answered without looking at other variables.

- Conclusion: When designing an experiment, make sure your predictors are all uncorrelated.

# Practical versus statistical significance

- Tests can have tiny $p$-values, but the effect sizes might be small.
    - This can occur if we have a large sample size when two variables have a weak relationship.

- Tests can have large $p$-values, but the effect sizes might be huge.
    - This can occur if we have a small sample size when two variables have a strong relationship.
    
- So **never** use $p$-values to describe the strength of the relationship between two variables. 
    - This is what other statistics like $R^2$, partial $R^2$, sums of squares, and Cohen's $d$ are meant for.

- Only use $p$-values to describe the strength of the **evidence that there is a relationship** between two variables.




