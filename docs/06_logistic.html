<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2022-10-05" />

<title>Logistic Regression</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Logistic Regression</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2022-10-05</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Logistic Regression for Binary Response Variables</li>
<li>Chapter 20 of the <em>Statistical Sleuth</em></li>
<li>Sections 14.1–14.10 of KNNL</li>
</ul>
<pre class="r"><code>library(tidyverse)
library(Sleuth3)
library(broom)
library(sandwich)
library(lmtest)</code></pre>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<ul>
<li><p>Lots of regression problems involve response variables that are
<strong>binary</strong> — the only possible values are either 0 or
1.</p>
<ul>
<li>Alive (1) vs dead (0)</li>
<li>Success (1) vs failure (0)</li>
<li>Presence (1) vs absence (0)</li>
</ul></li>
<li><p>It is arbitrary and unimportant which variable you encode as 1 vs
0.</p>
<ul>
<li>E.g. we could have coded “dead” as 1 and “alive” as 0.</li>
<li>Just keep track of the coding for interpretation purposes.</li>
</ul></li>
<li><p>E.g., either survival (<span class="math inline">\(Y_i =
1\)</span>) or death (<span class="math inline">\(Y_i = 0\)</span>) of
individual <span class="math inline">\(i\)</span> from the Donner Party
of 1846.</p>
<pre class="r"><code>data(&quot;case2001&quot;)
donner &lt;- case2001
glimpse(donner)</code></pre>
<pre><code>## Rows: 45
## Columns: 3
## $ Age    &lt;int&gt; 23, 40, 40, 30, 28, 40, 45, 62, 65, 45, 25, 28, 28, 23, 22, 23,…
## $ Sex    &lt;fct&gt; Male, Female, Male, Male, Male, Male, Female, Male, Male, Femal…
## $ Status &lt;fct&gt; Died, Survived, Survived, Died, Died, Died, Died, Died, Died, D…</code></pre></li>
<li><p>We often want to quantify the association between a quantitative
predictor <span class="math inline">\(x\)</span> and the binary response
<span class="math inline">\(y\)</span>. E.g. older folks tended to die
at a higher frequency:</p>
<pre class="r"><code>qplot(x = Age, y = Status, data = donner, geom = &quot;point&quot;)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
<div id="linear-probability-model-lpm" class="section level1">
<h1>Linear Probability Model (LPM)</h1>
<ul>
<li><p>If your goal is <em>not</em> prediction, then it is <strong>not
entirely incorrect</strong> to just use linear regression <span
class="citation">(Hellevik 2009; Gomila 2021)</span>.</p>
<ul>
<li>I am hedging here since this is a controversial statement.</li>
</ul></li>
<li><p>Let <span class="math inline">\(Y_i \sim Bern(p)\)</span>. That
is <span class="math inline">\(Y_i\)</span> is 1 with probability <span
class="math inline">\(p\)</span> and is 0 with probability <span
class="math inline">\(1-p\)</span>. Then</p>
<ul>
<li><span class="math inline">\(E[Y_i] = p\)</span></li>
<li><span class="math inline">\(var(Y_i) = p(1-p)\)</span></li>
</ul></li>
<li><p>Suppose <span class="math inline">\(Y_i\)</span> is 1 for
“success” and “0” for failure. Suppose we have the model</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots +
\beta_{p-1}X_{i,p-1} + \epsilon_i.
  \]</span></p></li>
<li><p>Since <span class="math inline">\(E[Y_i]\)</span> is the
probability that <span class="math inline">\(Y_i\)</span> is 1, that
mean’s we can interpret the regression line as the probability that
<span class="math inline">\(Y_i\)</span> is 1.</p></li>
<li><p>Interpreting the regression coefficients: “Individuals that are 1
unit larger in <span class="math inline">\(X_{ik}\)</span> are <span
class="math inline">\(\beta_k \times 100\)</span> percentage points more
probable to have a <span class="math inline">\(Y\)</span> value of 1,
adjusting for other variables.”</p></li>
<li><p>Let’s apply this to the Donner party data</p>
<pre class="r"><code>donner &lt;- mutate(donner, Survived = if_else(Status == &quot;Survived&quot;, 1, 0))
lmout &lt;- lm(Survived ~ Age, data = donner)
lmout</code></pre>
<pre><code>## 
## Call:
## lm(formula = Survived ~ Age, data = donner)
## 
## Coefficients:
## (Intercept)          Age  
##      0.8692      -0.0134</code></pre></li>
<li><p>Thus, individuals that were one year older were 1.3 percentage
points less likely to survive.</p></li>
<li><p>Bernoulli random variables are always heteroscedastic since their
variance is a function of the mean <span
class="math inline">\(p(1-p)\)</span>. So if you use the LPM, you
<em>need</em> to use heteroscedastic robust standard errors.</p>
<pre class="r"><code>cout &lt;- coeftest(x = lmout, vcov. = vcovHC)
tidy(cout)</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)   0.869    0.171        5.08 0.00000772
## 2 Age          -0.0134   0.00413     -3.23 0.00235</code></pre></li>
<li><p>Residual plots are useless here, so never show them:</p>
<pre class="r"><code>aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-6-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="issues" class="section level2">
<h2>Issues</h2>
<ul>
<li><p>Econometricians and psychometricians love the LPM because of its
ease of interpretability.</p></li>
<li><p>Statisticians typically do not like it (though I’m
agnostic).</p></li>
<li><p>The possible values of <span class="math inline">\(\beta_0 +
\beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1}\)</span>
are any real numbers between <span
class="math inline">\(-\infty\)</span> and <span
class="math inline">\(\infty\)</span>, but the probability of <span
class="math inline">\(Y_i\)</span> being 1 can only be between 0 and
1.</p></li>
<li><p>So it is possible (and typical in many datasets) to have
probability estimates that are negative or greater than 1. This makes
folks squeamish.</p></li>
<li><p>E.g., the point-wise confidence bands from the Donner party make
no sense:</p>
<pre class="r"><code>ggplot(donner, aes(x = Age, y = Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-7-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>When the probability of a “1” is between about 0.2 and 0.8, then
the linear and logistic (see below) models produce about the same
results.</p>
<ul>
<li>This is a <strong>conditional</strong> probability. So all values of
<span class="math inline">\(E[Y|X]\)</span> must be between 0.2 and 0.8
for all values of <span class="math inline">\(X\)</span> for this to
hold.</li>
<li>I.e., you <strong>cannot</strong> just look at the marginal
proportions of 1’s and 0’s and use this to say if the LPM is good.</li>
</ul></li>
<li><p>Let’s look at the differences in the Donner party example:</p>
<pre class="r"><code>ggplot(donner, aes(x = Age, y = Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) +
  geom_smooth(method = &quot;glm&quot;, 
              se = FALSE,
              method.args = list(family = &quot;binomial&quot;),
              color = &quot;red&quot;, 
              lty = 2,
              formula = y ~ x)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-8-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
</div>
<div id="logistic-regression-model" class="section level1">
<h1>Logistic Regression Model</h1>
<ul>
<li><p>Let <span class="math inline">\(p_i\)</span> be the probability
that individual <span class="math inline">\(i\)</span> is 1. I.e. <span
class="math inline">\(E[Y_i] = p_i\)</span>. Then our model is <span
class="math display">\[
  \text{logit}(p_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots
\beta_{p-1}X_{i,p-1}
  \]</span></p></li>
<li><p>Here, <span class="math inline">\(\text{logit}(\cdot)\)</span> is
the “logit” function, <span class="math display">\[
  \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right).
  \]</span></p></li>
<li><p>The inverse of the logit function is the “expit” function
(confusingly, also known as the “logistic” function) <span
class="math display">\[
  \text{expit}(\eta) = \frac{e^{\eta}}{1 + e^{\eta}}.
  \]</span></p></li>
<li><p>So we can equivalently write this model as <span
class="math display">\[
  p_i = \text{expit}\left(\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} +
\cdots \beta_{p-1}X_{i,p-1}\right)
  \]</span></p></li>
<li><p>The <span class="math inline">\(\text{expit}(\cdot)\)</span>
function takes a number from <span
class="math inline">\(-\infty\)</span> to <span
class="math inline">\(\infty\)</span> and places it between 0 and 1.
Thus, it forces the probabilities to be between 0 and 1.</p>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="generalized-linear-model" class="section level2">
<h2>Generalized Linear Model</h2>
<ul>
<li><p>This is an example of a “generalized linear model”. Let <span
class="math inline">\(Y_i\)</span> follow any distribution we specify
Let <span class="math inline">\(\mu_i\)</span> be the mean of <span
class="math inline">\(Y_i\)</span>. Then a generalized linear model
is</p>
<p><span class="math display">\[
  g(\mu_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots
\beta_{p-1}X_{i,p-1}
  \]</span></p></li>
<li><p>Here, <span class="math inline">\(g(\cdot)\)</span> is called the
“link function”. The model is linear on the link scale (hence
“generalized linear model”).</p></li>
<li><p>In logistic regression, <span class="math inline">\(Y_i \sim
Bern(\mu_i)\)</span> and <span class="math inline">\(g(\mu_i) =
\text{logit}(\mu_i)\)</span>.</p></li>
<li><p>Another example is probit regression (more popular in econ) where
<span class="math inline">\(g(\mu_i) = \Phi^{-1}(\mu_i)\)</span>, the
standard normal quantile function (<code>qnorm()</code>). This gives you
almost identical results (except in the tails), but statisicians don’t
like the probit because it’s less interpretable than the logit.</p>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-10-1.png" width="480" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Technical, unimportant note</strong>: The above plots the
quantile function of a <span class="math inline">\(N(0,
\pi^2/3)\)</span> distribution for the probit to account for the
difference in variances between the normal and logit distributions. This
makes the two curves comparable.</li>
</ul></li>
<li><p>In <a
href="https://en.wikipedia.org/wiki/Poisson_regression">log-linear
models</a> (which we might discuss later), we model counts with <span
class="math inline">\(Y_i \sim Poi(\mu_i)\)</span> (<a
href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson
Distribution</a>) and <span class="math inline">\(g(\mu_i) =
\log(\mu_i)\)</span>.</p></li>
</ul>
</div>
</div>
<div id="estimation" class="section level1">
<h1>Estimation</h1>
<div id="maximum-likelihood-estimation" class="section level2">
<h2>Maximum Likelihood Estimation</h2>
<ul>
<li><p>We don’t use least squares for estimation. Rather, we use Maximum
Likelihood.</p></li>
<li><p>The probability that <span class="math inline">\(Y_i = 1\)</span>
is <span class="math inline">\(p_i\)</span> and the probability that
<span class="math inline">\(Y_i = 0\)</span> is <span
class="math inline">\(1 - p_i\)</span>.</p></li>
<li><p>Let <span class="math inline">\(y_i \in \{0,1\}\)</span> be the
observed value of the random variable <span
class="math inline">\(Y_i\)</span>. Then we can succinctly write the
probabilities of <span class="math inline">\(Y_i\)</span> as <span
class="math display">\[
  Pr(Y_i = y_i) = p_i^{y_i}(1-p_i)^{1 - y_i}
  \]</span> Go ahead and plug in <span class="math inline">\(y_i =
0\)</span> and <span class="math inline">\(y_i = 1\)</span> to see
this.</p></li>
<li><p>So the probability of our data given <span
class="math inline">\(p_i\)</span> is <span class="math display">\[
  Pr(Y_1 = y_1, Y_2 = y_2, \ldots, Y_n = y_n) = \prod_{i=1}^n
p_i^{y_i}(1-p_i)^{1-y_i}
  \]</span></p></li>
<li><p>Recall that <span class="math display">\[
  \text{logit}(p_i) = \beta_0 + \beta_1X_{i1} + \cdots
\beta_{p-1}X_{i,p-1}.
  \]</span></p></li>
<li><p>So the probability of the data is a function of <span
class="math inline">\(\beta_0,\beta_1,\ldots,\beta_{p-1}\)</span>.</p></li>
<li><p>IDEA: Choose the <span class="math inline">\(\beta_k\)</span>’s
that make our observed data as probable as possible.</p>
<ul>
<li>PHILOSOPHY: we observed our data (that’s why we have it), so it is
probably likely to have been generated. Choose the parameters that would
maximize the probability of seeing the data we actually saw.</li>
</ul></li>
<li><p>The objective function is called the “likelihood”.</p></li>
<li><p>The resulting estimates are called the “maximum likelihood
estimates” (MLE’s).</p></li>
<li><p>Typically, we actually maximize the log of the
likelihood</p></li>
</ul>
<p><span class="math display">\[
L(\mathbf{\beta}) = \sum_{i=1}^n [y_i\log(p_i) + (1-y_i)\log(1-p_i)]
\]</span></p>
<ul>
<li><p>Below, the expit curve is morphing according to different values
of <span class="math inline">\(\beta_0\)</span> and <span
class="math inline">\(\beta_1\)</span>. The text by the points is the
probability of the those observed points, which in this case is just the
height (when <span class="math inline">\(Y_i = 1\)</span>) or one minus
the height (when <span class="math inline">\(Y_i = 0\)</span>) of the
expit function. The likelihood is the product of those probabilities,
and the log-likelihood is the sum of the log of those probabilities.</p>
<p><img src="06_figs/logistic_mle.gif" /> </p></li>
</ul>
</div>
<div id="implementation-in-r" class="section level2">
<h2>Implementation in R</h2>
<ul>
<li><p>You should transform your binary variable to have 0’s and 1’s.
You should call your new variable the value when <span
class="math inline">\(Y_i = 1\)</span>. E.g. in the Donner party we
created the “Survived” variable.</p>
<pre class="r"><code>donner &lt;- mutate(donner, Survived = if_else(Status == &quot;Survived&quot;, 1, 0))</code></pre></li>
<li><p>Let <span class="math inline">\(Y_i\)</span> be an indicator for
survival for individual <span class="math inline">\(i\)</span>, let
<span class="math inline">\(X_{i1}\)</span> be the age of individual
<span class="math inline">\(i\)</span>, and let <span
class="math inline">\(X_{i2}\)</span> be an indicator for male Then we
fit the model <span class="math display">\[\begin{align}
  Y_i &amp;\sim Bern(p_i)\\
  \text{logit}(p_i) &amp;= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2}
  \end{align}\]</span></p></li>
<li><p>We fit all generalized linear models in R with
<code>glm()</code>. The arguments are</p>
<ul>
<li><code>formula</code>: The same idea as in <code>lm()</code>.
Response variable on the left of the tilde, explanatory variables on the
right of the tilde.</li>
<li><code>data</code>: The data frame where the variables are.</li>
<li><code>family</code>: The distribution of <span
class="math inline">\(Y_i\)</span>.
<ul>
<li><code>gaussian</code> will assume normal errors (like in
<code>lm()</code>).</li>
<li><code>binomial</code> in this case will fit the bernoulli model
since <span class="math inline">\(Bern(p_i) = Bin(1, p_i)\)</span>.</li>
</ul></li>
<li>You can specify the link function here as well. So for probit models
you would do <code>binomial(link = "probit")</code>.</li>
</ul></li>
<li><p>Let’s fit the model</p>
<pre class="r"><code>glm_out &lt;- glm(Survived ~ Age + Sex, data = donner, family = binomial)</code></pre></li>
<li><p>You again use <code>tidy()</code> from the <code>{broom}</code>
package to get estimates / standard errors</p>
<pre class="r"><code>tidy(glm_out)</code></pre>
<pre><code>## # A tibble: 3 × 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)   3.23      1.39        2.33  0.0198
## 2 Age          -0.0782    0.0373     -2.10  0.0359
## 3 SexMale      -1.60      0.755      -2.11  0.0345</code></pre></li>
<li><p>So the estimated regression surface is <span
class="math display">\[
  \text{logit}(p_i) = 3.23 -0.08X_{1} - 1.60X_{2}
  \]</span></p></li>
<li><p><strong>Exercise</strong>: What if <span
class="math inline">\(X_2\)</span> would have been an indicator for
female? What would the coefficient estimate of <span
class="math inline">\(\beta_2\)</span> be?</p></li>
</ul>
</div>
<div id="bias-reduction" class="section level2">
<h2>Bias reduction</h2>
<ul>
<li><p>For small sample sizes, the MLE’s of the logistic regression
coefficients can be biased. A quick way to reduce this bias is through
the <a
href="https://cran.r-project.org/package=brglm2"><code>{brglm2}</code></a>
package, which uses the method of <span class="citation">Firth
(1993)</span>.</p></li>
<li><p>Let’s try this out on the Donner party example</p>
<pre class="r"><code>library(brglm2)
glm_out2 &lt;- glm(Survived ~ Age + Sex,
                data = donner, 
                family = binomial, 
                method = brglm2::brglm_fit)
tidy(glm_out2, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 3 × 7
##   term        estimate std.error statistic p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   2.80      1.28        2.18  0.0292    0.283  5.31    
## 2 Age          -0.0669    0.0346     -1.94  0.0528   -0.135  0.000821
## 3 SexMale      -1.43      0.725      -1.98  0.0482   -2.85  -0.0116</code></pre></li>
<li><p>This actually resulted in some moderate differences!</p></li>
</ul>
</div>
</div>
<div id="interpretation" class="section level1">
<h1>Interpretation</h1>
<div id="odds" class="section level2">
<h2>Odds</h2>
<ul>
<li><p>The interpretation of the logistic regression model requires you
to be familiar with odds.</p></li>
<li><p>As I write this, the Baltimore Ravens have 3 to 2 odds of
defeating the Miami Dolphins tomorrow. This means that the odds of a
Ravens victory are <span class="math inline">\(3/2 =
1.5\)</span>.</p></li>
<li><p>The odds of an event are the probability of that event divided by
the probability of not that event.</p></li>
<li><p>Let <span class="math inline">\(p\)</span> be the probability of
an event, and let <span class="math inline">\(\omega\)</span> be the
odds of the same event. Then we have the relations: <span
class="math display">\[
  \omega = \frac{p}{1-p}\\
  p = \frac{\omega}{1+\omega}
  \]</span></p></li>
<li><p>So odds are just another description of probabilities.</p></li>
<li><p>In the football example above, the Ravens’ probability of wining
is 0.6, so the odds are 0.6/0.4 = 3/2 = 1.5.</p></li>
<li><p><strong>Exercise</strong>: The Chiefs have 2 to 1 odds over the
Chargers tomorrow. What’s the probability of a Chief’s win?</p></li>
<li><p><strong>Exercise</strong>: As I write this, FiveThirtyEight has
the Democrats with a 2/3 probability of keeping the Senate in the 2022
midterm election. What are the odds of a Democrat victory?</p></li>
</ul>
</div>
<div id="interpreting-logistic-regression-model" class="section level2">
<h2>Interpreting Logistic Regression Model</h2>
<ul>
<li><p>Let <span class="math inline">\(\omega_{old}\)</span> be the odds
of an individual. We can write the logistic regression model as <span
class="math display">\[
  \omega_{old} = \exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)
  \]</span></p></li>
<li><p>Suppose that a different individual has the exact same predictor
values except one unit higher <span class="math inline">\(X_1\)</span>.
Then <span class="math display">\[\begin{align}
  \omega_{new} &amp;= \exp\left(\beta_0 + \beta_1(X_{1} + 1) +
\beta_2X_{2} + \cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp\left(\beta_0 + \beta_1X_{1} + \beta_1 + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp(\beta_1)\exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp(\beta_1)\omega_{old}
  \end{align}\]</span></p></li>
<li><p>Thus, the odds of the second individual are <span
class="math inline">\(e^{\beta_1}\)</span> times as large as the odds of
the first individual.</p></li>
<li><p>Different way to say this: The odds ratio for the two individuals
is <span class="math inline">\(e^{\beta_1}\)</span>.</p></li>
<li><p>Recall the estimated regression relationship from the Donner
party example: <span class="math display">\[
  \text{logit}(p_i) = 3.23 -0.08X_{1} - 1.60X_{2}
  \]</span></p></li>
<li><p>So individuals of the same sex that are 10 years younger have
about twice the odds of surviving (<span class="math inline">\(e^{0.08}
\times 10 = 2.186\)</span>). Also, a woman’s odds of survival were about
5 times that of a man of the same age (<span
class="math inline">\(e^{1.6} = 4.95\)</span>).</p></li>
<li><p><strong>Exercise</strong>: The nocturia data set, described <a
href="https://dcgerard.github.io/stat_415_615/data.html#Nocturia">here</a>
and dowloadable from <a
href="https://dcgerard.github.io/stat_415_615/data/nocturia.csv">here</a>,
contains patient covariates that are believed to be associated with
whether the individual has nocturia (wakes up to pee). Download the
data, the fit of logistic regression model of nocturia on age. Interpret
the resulting coefficients.</p></li>
</ul>
</div>
</div>
<div id="retrospective-studies" class="section level1">
<h1>Retrospective studies</h1>
<ul>
<li><p>In a <strong>prospective</strong> study, the explanatory
variables are fixed and we imagine <span
class="math inline">\(Y_i\)</span> being sampled given <span
class="math inline">\(X_i\)</span>.</p>
<ul>
<li>E.g. we might choose <span class="math inline">\(200\)</span>
patients and observe their cancer status.</li>
</ul></li>
<li><p>In a <strong>retrospective</strong> study, we fix <span
class="math inline">\(Y_i\)</span> and later observe their <span
class="math inline">\(X_i\)</span>.</p>
<ul>
<li>E.g. we might choose <span class="math inline">\(n = 100\)</span>
cancer patients and <span class="math inline">\(n = 100\)</span>
controls.</li>
<li>This is typically done with the probability of one group is really
small and you want more samples of that group.</li>
</ul></li>
<li><p>The LPM model (and probit model) are <strong>not</strong> valid
in retrospective studies, but the logistic regression model
<strong>is</strong> valid. So in retrospective studies you
<strong>must</strong> use logistic regression.</p></li>
<li><p>The reason is that the odds ratios for prospective studies and
retrospective studies are the same, even though the probabilities are
not.</p></li>
<li><p>You can interpret slopes in the usual way in retrospective
studies, but</p>
<ul>
<li>You cannot interpret the <span
class="math inline">\(Y\)</span>-intercept.</li>
<li>You cannot interpret <strong>any</strong> fixed estimate of the
probability.</li>
<li>You can <strong>only</strong> interpret the slope (the odds
ratio).</li>
</ul></li>
</ul>
<div id="proof" class="section level3">
<h3>Proof</h3>
<ul>
<li><p>Let’s suppose that we have model <span class="math display">\[
Pr(Y = 1|X) = \text{expit}(\beta_0 + \beta_1X) = \frac{e^{\beta_0 +
\beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
\]</span></p></li>
<li><p>Let’s introduce a binary variable <span
class="math inline">\(Z\)</span> which is 1 if a unit is sampled, and 0
if it is not sampled. Let <span class="math display">\[
  Pr(Z = 1|Y = 1) = \pi_1\\
  Pr(Z = 1|Y = 0) = \pi_0\\
  \]</span></p></li>
<li><p>So we sample a <span class="math inline">\(Y_i = 1\)</span>
individual with probability <span class="math inline">\(\pi_1\)</span>
and a <span class="math inline">\(Y_i = 0\)</span> individual with
probability <span class="math inline">\(\pi_0\)</span>. This is a
retrospective design, where the <span
class="math inline">\(\pi\)</span>’s are <strong>not</strong> functions
of the <span class="math inline">\(X_i\)</span>’s.</p></li>
<li><p>We can only model the data we observe, so we want <span
class="math inline">\(Pr(Y = 1|Z=1, X)\)</span>, which we can calculate
using <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes
theorem</a> <span class="math display">\[\begin{align}
  Pr(Y = 1|Z=1, X) &amp;= \frac{Pr(Z = 1|Y = 1, X) Pr(Y = 1|X)}{Pr(Z =
1|Y = 1, X) Pr(Y = 1|X) + Pr(Z = 1|Y = 0, X) Pr(Y = 0|X)}\\
  &amp;= \frac{\pi_1 Pr(Y = 1|X)}{\pi_1 Pr(Y = 1|X) + \pi_0 Pr(Y =
0|X)}\\
  &amp;= \frac{\pi_1 \text{expit}(\beta_0 + \beta_1X)}{\pi_1
\text{expit}(\beta_0 + \beta_1X) + \pi_0 (1 - \text{expit}(\beta_0 +
\beta_1X))}\\
  &amp;= \frac{\pi_1 \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 +
\beta_1X}}}{\pi_1 \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 +
\beta_1X}} + \pi_0 \frac{1}{1 + e^{\beta_0 + \beta_1X}}}\\
  &amp;= \frac{\pi_1 e^{\beta_0 + \beta_1X}}{\pi_1 e^{\beta_0 +
\beta_1X} + \pi_0}\\
  &amp;= \frac{\frac{\pi_1}{\pi_0} e^{\beta_0 +
\beta_1X}}{\frac{\pi_1}{\pi_0} e^{\beta_0 + \beta_1X} + 1}\\
  &amp;= \frac{e^{(\beta_0 + \log(\pi_1 / \pi_0)) +
\beta_1X}}{e^{(\beta_0 + \log(\pi_1 / \pi_0)) + \beta_1X} + 1}\\
  &amp;= \frac{e^{\tilde{\beta}_0 + \beta_1X}}{e^{\tilde{\beta}_0 +
\beta_1X} + 1}\\
  &amp;= \text{expit}(\tilde{\beta}_0 + \beta_1X),
  \end{align}\]</span> where <span class="math inline">\(\tilde{\beta}_0
= \beta_0 + \log(\pi_1 / \pi_0)\)</span>.</p></li>
<li><p>Thus, in a retrospective study, we are estimating <span
class="math inline">\(\beta_1\)</span> and <span
class="math inline">\(\tilde{\beta}_0\)</span> (not <span
class="math inline">\(\beta_0\)</span>).</p></li>
<li><p>So, it is impossible to estimate <span
class="math inline">\(\beta_0\)</span> (and hence, impossible to
estimate the probability of <span class="math inline">\(Y_i = 1\)</span>
at any value of <span class="math inline">\(X\)</span>).</p></li>
<li><p><strong>BUT</strong> it is possible to estimate <span
class="math inline">\(\beta_1\)</span> (and thus, the relationship
between variables).</p></li>
</ul>
</div>
</div>
<div id="inference" class="section level1">
<h1>Inference</h1>
<ul>
<li>(Essentially) all maximum likelihood estimates enjoy good
properties</li>
</ul>
<ol style="list-style-type: decimal">
<li>Mostly unbiased.</li>
<li>Equations for standard deviation calculations are available.</li>
<li>Sampling distribution of MLE’s is approximately normal for moderate
to large sample sizes.</li>
</ol>
<ul>
<li><p>That means that we can take take the same approaches as in linear
regression to run hypothesis tests and obtain confidence
intervals.</p></li>
<li><p>For large samples, we have that the MLE’s have the following
sampling distribution: <span class="math display">\[
\frac{\hat{\beta}_i - \beta_i}{s(\hat{\beta}_i)} \sim N(0, 1)
\]</span></p></li>
<li><p>Hypothesis test:</p>
<ul>
<li><span class="math inline">\(H_0: \beta_i = 0\)</span></li>
<li><span class="math inline">\(H_A: \beta_i \neq 0\)</span></li>
</ul></li>
<li><p><span class="math inline">\(z^* =
\frac{\hat{\beta}_i}{s(\hat{\beta}_i)}\)</span></p></li>
<li><p>Under the null, <span class="math inline">\(z^*\)</span>
asymptotically follows a standard normal distribution. Use this to
calculate a <span class="math inline">\(p\)</span>-value in the usual
way.</p>
<p><img src="06_hyp/logistic_hyp.png" /> </p></li>
<li><p>We can also use this property to obtain a confidence interval for
<span class="math inline">\(\beta_i\)</span>.</p></li>
<li><p>Let <span class="math inline">\(u = \text{qnorm}(1 - \alpha /
2)\)</span>, which is 1.96 when <span class="math inline">\(\alpha =
0.05\)</span>. <span class="math display">\[
  Pr\left(-u \leq \frac{\hat{\beta}_i - \beta_i}{s(\hat{\beta}_i)} \leq
u\right) = 0.95
  \]</span></p></li>
<li><p>Solving for <span class="math inline">\(\beta_i\)</span>, we get
the following <span class="math inline">\((1 - \alpha)\times
100\%\)</span> confidence interval: <span class="math display">\[
  \hat{\beta}_i \pm \text{qnorm}(1 - \alpha / 2) s(\hat{\beta}_i)
  \]</span></p></li>
<li><p>We do not use the <span
class="math inline">\(t\)</span>-distribution here because the <span
class="math inline">\(t\)</span> results from assuming normal error
terms, which is not the case here.</p></li>
<li><p>In R, you get these quantities by <code>tidy()</code>.</p>
<pre class="r"><code>glm_out &lt;- glm(Survived ~ Age + Sex, data = donner, family = binomial)
tidy(glm_out, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 3 × 7
##   term        estimate std.error statistic p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   3.23      1.39        2.33  0.0198    0.851    6.43  
## 2 Age          -0.0782    0.0373     -2.10  0.0359   -0.162   -0.0141
## 3 SexMale      -1.60      0.755      -2.11  0.0345   -3.23    -0.195</code></pre></li>
</ul>
</div>
<div id="point-wise-estimation" class="section level1">
<h1>Point-wise estimation</h1>
<ul>
<li><p>You again obtain point-wise estimates for log-odds by
<code>predict()</code>.</p>
<pre class="r"><code>glm_out &lt;- glm(Survived ~ Age + Sex, data = donner, family = binomial)
newdf &lt;- data.frame(Age = c(25, 30), Sex = c(&quot;Male&quot;, &quot;Female&quot;))
predict(glm_out, newdata = newdf)</code></pre>
<pre><code>##       1       2 
## -0.3220  0.8843</code></pre></li>
<li><p>If you want the point-wise estimates for the probabilities, use
<code>type = "response"</code></p>
<pre class="r"><code>predict(glm_out, newdata = newdf, type = &quot;response&quot;)</code></pre>
<pre><code>##      1      2 
## 0.4202 0.7077</code></pre></li>
<li><p>To calculate point-wise confidence intervals, you should do this
on the logit-scale manually, then back-transform. Here is a function
that will do all of that for you.</p>
<pre class="r"><code>#&#39; Point-wise confidence intervals for GLM objects.
#&#39; 
#&#39; Some documentation was borrowed from stats::predict().
#&#39; 
#&#39; @param object The output of stats::glm()
#&#39; @param newdata An optional data frame in which to look for
#&#39;     variables with which to predict. If omitted, the fitted 
#&#39;     values are used.
#&#39; @param interval Type of interval calculation.
#&#39; @param level Tolerance/confidence level.
#&#39; 
#&#39; @return A data frame, with `newdata` column-bound to the fits and 
#&#39;     the lower and upper ranges of the intervals.
#&#39; 
#&#39; @author David Gerard
confglm &lt;- function(object, 
                    newdata = NULL, 
                    type = c(&quot;link&quot;, &quot;response&quot;), 
                    level = 0.95) {
  # Check arguments ----
  stopifnot(inherits(object, &quot;glm&quot;))
  type &lt;- match.arg(type)
  stopifnot(level &gt;= 0, level &lt;= 1, length(level) == 1)
  if (is.null(newdata)) {
    newdata &lt;- as.data.frame(stats::model.frame(object))[-1]
  }

  ## predict ----
  object$family
  pout &lt;- stats::predict(object = object, 
                         newdata = newdata,
                         se.fit = TRUE)
  alpha &lt;- 1 - level
  z &lt;- stats::qnorm(1 - alpha / 2)
  newdata$fit &lt;- pout$fit
  newdata$lwr &lt;- pout$fit - z * pout$se.fit
  newdata$upr &lt;- pout$fit + z * pout$se.fit

  ## Inverse transformation ----
  if (type == &quot;response&quot;) {
    fam &lt;- family(object)
    newdata$fit &lt;- fam$linkinv(newdata$fit)
    newdata$lwr &lt;- fam$linkinv(newdata$lwr)
    newdata$upr &lt;- fam$linkinv(newdata$upr)
  }

  return(newdata)
}</code></pre>
<pre class="r"><code>confglm(object = glm_out, newdata = newdf, type = &quot;response&quot;)</code></pre>
<pre><code>##   Age    Sex    fit    lwr    upr
## 1  25   Male 0.4202 0.2408 0.6235
## 2  30 Female 0.7077 0.4228 0.8890</code></pre></li>
</ul>
</div>
<div id="prediction" class="section level1">
<h1>Prediction</h1>
<ul>
<li><p>To predict a response value at a given set of explanatory values,
you obtain the estimated probability at those levels, like above</p>
<pre class="r"><code>newdf &lt;- data.frame(Age = c(25, 30), Sex = c(&quot;Male&quot;, &quot;Female&quot;))
predict(glm_out, newdata = newdf, type = &quot;response&quot;)</code></pre>
<pre><code>##      1      2 
## 0.4202 0.7077</code></pre></li>
<li><p>We might then predict the 25 year-old male to die (since his
estimated probability of survival is only 0.42) and the 30 year-old
female to survive (since her estimated probability of survival is
0.71).</p></li>
<li><p>In general, you choose some threshold <span
class="math inline">\(c\)</span> and predict <span
class="math inline">\(Y_i = 1\)</span> if the estimated probability is
above <span class="math inline">\(c\)</span>, and predict <span
class="math inline">\(Y_i = 0\)</span> if the estimated probability is
below <span class="math inline">\(c\)</span>.</p></li>
<li><p>You might think to always choose <span class="math inline">\(c =
0.5\)</span>, but it’s more complicated than that. You typically need to
calibrate <span class="math inline">\(c\)</span> by balancing desired
levels of <a
href="https://en.wikipedia.org/wiki/Precision_and_recall">recall and
precision</a>. See also <a
href="https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion">here</a>.</p></li>
<li><p>You will learn more about this in Machine Learning next
semester.</p></li>
</ul>
</div>
<div id="drop-in-deviance-test-aka-the-likelihood-ratio-test"
class="section level1">
<h1>Drop-in-deviance Test (AKA the Likelihood Ratio Test)</h1>
<ul>
<li><p>A <strong>saturated model</strong> is one where every single
observation has its own paramter, so the data are fit exactly.</p></li>
<li><p>In logistic regression the likelihood of the saturated model is 1
(since every observation is perfectly observed) and so the
log-likelihood of the saturated model is 0.</p></li>
<li><p>The <strong>deviance</strong> of a model is -2 times the
difference between the saturated model and the model under
consideration. So in logistic regression, the deviance is <span
class="math inline">\(-2L\)</span> where <span
class="math inline">\(L\)</span> is the maximized
log-likelihood.</p></li>
<li><p><code>glm()</code> returns the deviance.</p>
<pre class="r"><code>glance(glm_out) %&gt;%
  select(deviance)</code></pre>
<pre><code>## # A tibble: 1 × 1
##   deviance
##      &lt;dbl&gt;
## 1     51.3</code></pre></li>
<li><p>We can calculate -2 times the log-likelihood manually to verify
this:</p>
<pre class="r"><code>donner &lt;- mutate(donner, prob = predict(glm_out, type = &quot;response&quot;))
donner &lt;- mutate(donner, prob_obs = if_else(Survived == 1, prob, 1 - prob))
-2 * sum(log(donner$prob_obs))</code></pre>
<pre><code>## [1] 51.26</code></pre></li>
<li><p>The deviance is a generalization of the error sum of squares from
linear regression.</p></li>
<li><p>If the reduced model is correct, the drop in deviance between two
models follows a chi-squared distribution with degrees of freedom
corresponding to the difference in the number of parameters between the
two models.</p></li>
<li><p>Let’s test</p>
<ul>
<li><span class="math inline">\(H_0: \beta_1 = \beta_2 = 0\)</span></li>
<li>$H_A: $ either <span class="math inline">\(\beta_1 \neq 0\)</span>
or <span class="math inline">\(\beta_2 \neq 0\)</span>.</li>
</ul></li>
<li><p>The reduced model: <span class="math display">\[
  \text{logit}(p_i) = \beta_0\\
  Y_i \sim \text{Bern}(p_i)
  \]</span></p>
<pre class="r"><code>g_red &lt;- glm(Survived ~ 1, data = donner, family = binomial)</code></pre></li>
<li><p>The full model: <span class="math display">\[
  \text{logit}(p_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2}\\
  Y_i \sim \text{Bern}(p_i)
  \]</span></p>
<pre class="r"><code>g_ful &lt;- glm(Survived ~ Age + Sex, data = donner, family = binomial)</code></pre></li>
<li><p>Use <code>anova()</code> to test differences, using
<code>test = "LRT"</code> to specify that you should obtain a <span
class="math inline">\(p\)</span>-value using the likelihood ratio
test.</p>
<pre class="r"><code>anova(g_red, g_ful, test = &quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Survived ~ 1
## Model 2: Survived ~ Age + Sex
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1        44       61.8                     
## 2        42       51.3  2     10.6   0.0051</code></pre></li>
<li><p><strong>NOTE</strong>: Unlike in linear models, the
drop-in-deviance test is <strong>not</strong> equivalent to the <span
class="math inline">\(z\)</span>-test (“Wald-test”) when we are just
testing for one coefficient.</p>
<ul>
<li>Below I run a drop-in-deviance and a Wald test for <span
class="math inline">\(H_0: \beta_1 = 0\)</span>. The <span
class="math inline">\(p\)</span>-values are different.</li>
</ul>
<pre class="r"><code>g_red &lt;- glm(Survived ~ Sex, data = donner, family = binomial)
g_ful &lt;- glm(Survived ~ Age + Sex, data = donner, family = binomial)
anova(g_red, g_ful, test = &quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Survived ~ Sex
## Model 2: Survived ~ Age + Sex
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1        43       57.3                     
## 2        42       51.3  1     6.03    0.014</code></pre>
<pre class="r"><code>tidy(g_ful) %&gt;%
  filter(term == &quot;Age&quot;) %&gt;%
  select(term, p.value)</code></pre>
<pre><code>## # A tibble: 1 × 2
##   term  p.value
##   &lt;chr&gt;   &lt;dbl&gt;
## 1 Age    0.0359</code></pre>
<ul>
<li>The drop-in-deviance and Wald tests give the same results for large
samples, but for small samples the drop-in-deviance test tends to work
better.</li>
</ul></li>
</ul>
</div>
<div id="model-checking" class="section level1">
<h1>Model Checking</h1>
<ul>
<li><p>Residual plots are useless.</p>
<ul>
<li>But we don’t need to check for constant variance or outliers, two of
the big uses of residual plots.</li>
</ul></li>
<li><p>We should evaluate the logistic model.</p>
<ul>
<li>Compare to a more complicated model.</li>
<li>Compare to a model with cut explanatory variables.</li>
<li>Non-parametric smoother.</li>
</ul></li>
<li><p>Fit a more complicated model and see if there are any
deficiencies:</p>
<pre class="r"><code>glm_out &lt;- glm(Survived ~ Sex + Age,
               data = donner,
               family = binomial)
donner &lt;- mutate(donner, Age2 = Age^2)
g_big &lt;- glm(Survived ~ Sex + Age + Age2 + Sex * Age + Sex * Age2, 
             data = donner, 
             family = binomial)
anova(glm_out, g_big, test = &quot;LRT&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Survived ~ Sex + Age
## Model 2: Survived ~ Sex + Age + Age2 + Sex * Age + Sex * Age2
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
## 1        42       51.3                     
## 2        39       45.4  3      5.9     0.12</code></pre>
<ul>
<li>Moderate to weak evidence of a model mispecification, but we would
be fine ignoring this I think.</li>
</ul></li>
<li><p><a
href="https://en.wikipedia.org/wiki/Hosmer%E2%80%93Lemeshow_test">Hosmer-Lemeshow</a>
goodness-of-fit test for logistic regression.</p>
<ul>
<li>Basically, divides the points into bins (via their estimated
probabilities of 1) and compares a group-wise probability model to the
logistic model.</li>
<li>Null is fitted model, alternative is deviation from fitted
model.</li>
<li>Large <span class="math inline">\(p\)</span>-values indicate no
evidence of lack-of-fit.</li>
</ul>
<pre class="r"><code>glmtoolbox::hltest(glm_out)</code></pre>
<pre><code>## 
##    The Hosmer-Lemeshow goodness-of-fit test
## 
##  Group Size Observed Expected
##      1    5        1   0.2932
##      2    4        2   0.9107
##      3    5        1   1.6521
##      4    5        2   1.8479
##      5    6        1   2.5212
##      6    6        1   2.6721
##      7    6        5   3.5606
##      8    5        4   3.9842
##      9    3        3   2.5581
## 
##          Statistic =  9.32 
## degrees of freedom =  7 
##            p-value =  0.23</code></pre></li>
<li><p>The residuals should still have mean 0, so plot the residuals
versus fits with a non-parametric smoother.</p>
<pre class="r"><code>glm_out &lt;- glm(Survived ~ Sex + Age,
               data = donner,
               family = binomial)
aout &lt;- augment(glm_out)

qplot(x = .fitted, y = .std.resid, data = aout) +
  geom_hline(yintercept = 0) +
  geom_smooth()</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-35-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-firth1993bias" class="csl-entry">
Firth, David. 1993. <span>“<span class="nocase">Bias reduction of
maximum likelihood estimates</span>.”</span> <em>Biometrika</em> 80 (1):
27–38. <a
href="https://doi.org/10.1093/biomet/80.1.27">https://doi.org/10.1093/biomet/80.1.27</a>.
</div>
<div id="ref-gomila2021logistic" class="csl-entry">
Gomila, Robin. 2021. <span>“Logistic or Linear? Estimating Causal
Effects of Experimental Treatments on Binary Outcomes Using Regression
Analysis.”</span> <em>Journal of Experimental Psychology: General</em>
150 (4): 700. <a
href="https://doi.org/10.1037/xge0000920">https://doi.org/10.1037/xge0000920</a>.
</div>
<div id="ref-hellevik2009linear" class="csl-entry">
Hellevik, Ottar. 2009. <span>“Linear Versus Logistic Regression When the
Dependent Variable Is a Dichotomy.”</span> <em>Quality &amp;
Quantity</em> 43 (1): 59–74. <a
href="https://doi.org/10.1007/s11135-007-9077-3">https://doi.org/10.1007/s11135-007-9077-3</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
