<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2022-09-14" />

<title>Logistic Regression</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Logistic Regression</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2022-09-14</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Logistic Regression for Binary Response Variables</li>
<li>Chapters 20 and 21 of the <em>Statistical Sleuth</em></li>
</ul>
<pre class="r"><code>library(tidyverse)
library(Sleuth3)
library(broom)
library(sandwich)
library(lmtest)</code></pre>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<ul>
<li><p>Lots of regression problems involve response variables that are
<strong>binary</strong> — the only possible values are either 0 or
1.</p>
<ul>
<li>Alive (1) vs dead (0)</li>
<li>Success (1) vs failure (0)</li>
<li>Presence (1) vs absence (0)</li>
</ul></li>
<li><p>It is arbitrary and unimportant which variable you encode as 1 vs
0.</p>
<ul>
<li>E.g. we could have coded “dead” as 1 and “alive” as 0.</li>
<li>Just keep track of the coding for interpretation purposes.</li>
</ul></li>
<li><p>E.g., either survival (<span class="math inline">\(Y_i =
1\)</span>) or death (<span class="math inline">\(Y_i = 0\)</span>) of
individual <span class="math inline">\(i\)</span> from the Donner Party
of 1846.</p>
<pre class="r"><code>data(&quot;case2001&quot;)
donner &lt;- case2001
glimpse(donner)</code></pre>
<pre><code>## Rows: 45
## Columns: 3
## $ Age    &lt;int&gt; 23, 40, 40, 30, 28, 40, 45, 62, 65, 45, 25, 28, 28, 23, 22, 23,…
## $ Sex    &lt;fct&gt; Male, Female, Male, Male, Male, Male, Female, Male, Male, Femal…
## $ Status &lt;fct&gt; Died, Survived, Survived, Died, Died, Died, Died, Died, Died, D…</code></pre></li>
<li><p>We often want to quantify the association between a quantitative
predictor <span class="math inline">\(x\)</span> and the binary response
<span class="math inline">\(y\)</span>. E.g. older folks tended to die
at a higher frequency:</p>
<pre class="r"><code>qplot(x = Age, y = Status, data = donner, geom = &quot;point&quot;)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
<div id="linear-probability-model-lpm" class="section level1">
<h1>Linear Probability Model (LPM)</h1>
<ul>
<li><p>If your goal is <em>not</em> prediction, then it is not entirely
incorrect to just use linear regression <span class="citation">(Hellevik
2009; Gomila 2021)</span>.</p></li>
<li><p>Let <span class="math inline">\(Y_i \sim Bern(p)\)</span>. That
is <span class="math inline">\(Y_i\)</span> is 1 with probability <span
class="math inline">\(p\)</span> and is 0 with probability <span
class="math inline">\(1-p\)</span>. Then</p>
<ul>
<li><span class="math inline">\(E[Y_i] = p\)</span></li>
<li><span class="math inline">\(var(Y_i) = p(1-p)\)</span></li>
</ul></li>
<li><p>Suppose <span class="math inline">\(Y_i\)</span> is 1 for
“success” and “0” for failure. Suppose we have the model</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots +
\beta_{p-1}X_{i,p-1} + \epsilon_i.
  \]</span></p></li>
<li><p>Since <span class="math inline">\(E[Y_i]\)</span> is the
probability that <span class="math inline">\(Y_i\)</span> is 1, that
mean’s we can interpret the regression line as the probability that
<span class="math inline">\(Y_i\)</span> is 1.</p></li>
<li><p>Interpreting the regression coefficients: “Individuals that are 1
unit larger in <span class="math inline">\(X_{ik}\)</span> are <span
class="math inline">\(\beta_k \times 100\)</span> percentage points more
probable to have a <span class="math inline">\(Y\)</span> value of 1,
adjusting for other variables.”</p></li>
<li><p>Let’s apply this to the Donner party data</p>
<pre class="r"><code>donner &lt;- mutate(donner, Survived = if_else(Status == &quot;Survived&quot;, 1, 0))
lmout &lt;- lm(Survived ~ Age, data = donner)
lmout</code></pre>
<pre><code>## 
## Call:
## lm(formula = Survived ~ Age, data = donner)
## 
## Coefficients:
## (Intercept)          Age  
##      0.8692      -0.0134</code></pre></li>
<li><p>Thus, individuals that were one year older were 1.3 percentage
points less likely to survive.</p></li>
<li><p>Bernoulli random variables are always heteroscedastic since their
variance is a function of the mean <span
class="math inline">\(p(1-p)\)</span>. So if you use the LPM, you
<em>need</em> to use heteroscedastic robust standard errors.</p>
<pre class="r"><code>cout &lt;- coeftest(x = lmout, vcov. = vcovHC)
tidy(cout)</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)   0.869    0.171        5.08 0.00000772
## 2 Age          -0.0134   0.00413     -3.23 0.00235</code></pre></li>
<li><p>Residual plots are kind of useless here:</p>
<pre class="r"><code>aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-6-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="issues" class="section level2">
<h2>Issues</h2>
<ul>
<li><p>Econometricians and psychologists love the LPM because of its
ease of interpretability.</p></li>
<li><p>Statisticians typically do not like it (though I’m
agnostic).</p></li>
<li><p>The possible values of <span class="math inline">\(\beta_0 +
\beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1}\)</span>
are any real numbers between <span
class="math inline">\(-\infty\)</span> and <span
class="math inline">\(\infty\)</span>, but the probability of <span
class="math inline">\(Y_i\)</span> being 1 can only be between 0 and
1.</p></li>
<li><p>So it is possible (and typical in many datasets) to have
probability estimates that are negative or greater than 1. This makes
folks squeamish.</p></li>
<li><p>E.g., the point-wise confidence bands from the Donner party make
no sense:</p>
<pre class="r"><code>ggplot(donner, aes(x = Age, y = Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-7-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>When the probability of a “1” is between about 0.2 and 0.8, then
the linear and logistic (see below) models produce about the same
results.</p></li>
<li><p>Let’s look at the differences in the Donner party example:</p>
<pre class="r"><code>ggplot(donner, aes(x = Age, y = Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) +
  geom_smooth(method = &quot;glm&quot;, 
              se = FALSE,
              method.args = list(family = &quot;binomial&quot;),
              color = &quot;red&quot;, 
              lty = 2,
              formula = y ~ x)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-8-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
</div>
<div id="model" class="section level1">
<h1>Model</h1>
<ul>
<li><p>Let <span class="math inline">\(p_i\)</span> be the probability
that individual <span class="math inline">\(i\)</span> is 1. I.e. <span
class="math inline">\(E[Y_i] = p_i\)</span>. Then our model is <span
class="math display">\[
  \text{logit}(p_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots
\beta_{p-1}X_{i,p-1}
  \]</span></p></li>
<li><p>Here, <span class="math inline">\(\text{logit}(\cdot)\)</span> is
the “logit” function, <span class="math display">\[
  \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right).
  \]</span></p></li>
<li><p>The inverse of the logit function is the “expit” function (or
“logistic”) <span class="math display">\[
  \text{expit}(\eta) = \frac{e^{\eta}}{1 + e^{\eta}}.
  \]</span></p></li>
<li><p>So we can equivalently write this model as <span
class="math display">\[
  p_i = \text{expit}\left(\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} +
\cdots \beta_{p-1}X_{i,p-1}\right)
  \]</span></p></li>
<li><p>The <span class="math inline">\(\text{expit}(\cdot)\)</span>
function takes a number from <span
class="math inline">\(-\infty\)</span> to <span
class="math inline">\(\infty\)</span> and places it between 0 and 1.
Thus, it forces the probabilities to be between 0 and 1.</p>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="generalized-linear-model" class="section level2">
<h2>Generalized Linear Model</h2>
<ul>
<li><p>This is an example of a “generalized linear model”. Let <span
class="math inline">\(Y_i\)</span> follow any distribution we specify
Let <span class="math inline">\(\mu_i\)</span> be the mean of <span
class="math inline">\(Y_i\)</span>. Then a generalized linear model
is</p>
<p><span class="math display">\[
  g(\mu_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots
\beta_{p-1}X_{i,p-1}
  \]</span></p></li>
<li><p>Here, <span class="math inline">\(g(\cdot)\)</span> is called the
“link function”. The model is linear on the link scale (hence
“generalized linear model”).</p></li>
<li><p>In logistic regression, <span class="math inline">\(Y_i \sim
Bern(\mu_i)\)</span> and <span class="math inline">\(g(\mu_i) =
\text{logit}(\mu_i)\)</span>.</p></li>
<li><p>Another example is probit regression (more popular in econ) where
<span class="math inline">\(g(\mu_i) = \Phi^{-1}(\mu_i)\)</span>, the
standard normal quantile function (<code>qnorm()</code>). This gives you
similar results, but statisicians don’t like it because it’s less
interpretable than the logit.</p>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-10-1.png" width="480" style="display: block; margin: auto;" /></p></li>
<li><p>In <a
href="https://en.wikipedia.org/wiki/Poisson_regression">log-linear
models</a> (which we might later), we model counts with <span
class="math inline">\(Y_i \sim Poi(\mu_i)\)</span> (<a
href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson
Distribution</a>) and <span class="math inline">\(g(\mu_i) =
\log(\mu_i)\)</span>.</p></li>
</ul>
</div>
</div>
<div id="interpretation" class="section level1">
<h1>Interpretation</h1>
<div id="odds" class="section level2">
<h2>Odds</h2>
<ul>
<li><p>The interpretation of the logistic regression model requires you
to be familiar with odds.</p></li>
<li><p>As I write this, the Baltimore Ravens have 3 to 2 odds of
defeating the Miami Dolphins tomorrow. This means that the odds of a
Ravens victory are <span class="math inline">\(3/2 =
1.5\)</span>.</p></li>
<li><p>The odds of an event are the probability of that event divided by
the probability of not that event.</p></li>
<li><p>Let <span class="math inline">\(p\)</span> be the probability of
an event, and let <span class="math inline">\(\omega\)</span> be the
odds of the same event. Then we have the relations: <span
class="math display">\[
  \omega = \frac{p}{1-p}\\
  p = \frac{\omega}{1+\omega}
  \]</span></p></li>
<li><p>So odds are just another description of probabilities.</p></li>
<li><p>In the football example above, the Ravens’ probability of wining
is 0.6, so the odds are 0.6/0.4 = 3/2 = 1.5.</p></li>
<li><p><strong>Exercise</strong>: The Chiefs have 2 to 1 odds over the
Chargers tomorrow. What’s the probability of a Chief’s win?</p></li>
</ul>
</div>
<div id="interpreting-logistic-regression-model" class="section level2">
<h2>Interpreting Logistic Regression Model</h2>
<ul>
<li><p>Let <span class="math inline">\(\omega_{old}\)</span> be the odds
of an individual. We can write the logistic regression model as <span
class="math display">\[
  \omega_{old} = \exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)
  \]</span></p></li>
<li><p>Suppose that a different individual has the exact same predictor
values except one unit higher <span class="math inline">\(X_1\)</span>.
Then <span class="math display">\[\begin{align}
  \omega_{new} &amp;= \exp\left(\beta_0 + \beta_1(X_{1} + 1) +
\beta_2X_{2} + \cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp\left(\beta_0 + \beta_1X_{1} + \beta_1 + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp(\beta_1)\exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp(\beta_1)\omega_{old}
  \end{align}\]</span></p></li>
<li><p>Thus, the odds of the second individual are <span
class="math inline">\(e^{\beta_1}\)</span> times as large as the odds of
the first individual.</p></li>
<li><p>Different way to say this: The odds ratio for the two individuals
is <span class="math inline">\(e^{\beta_1}\)</span>.</p></li>
</ul>
</div>
</div>
<div id="bias-reduction" class="section level1">
<h1>Bias reduction</h1>
<ul>
<li>For small sample sizes, the MLE’s of the logistic regression
coefficients can be biased. A quick way to reduce this bias is through
the <a
href="https://cran.r-project.org/package=brglm2"><code>{brglm2}</code></a>
package, which uses the method of <span class="citation">Firth
(1993)</span>.</li>
</ul>
<p>Their example:</p>
<pre class="r"><code>## The lizards example from ?brglm::brglm
data(&quot;lizards&quot;, package = &quot;brglm2&quot;)
# Fit the model using maximum likelihood
lizardsML &lt;- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards)
# Mean bias-reduced fit:
lizardsBR &lt;- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards,
                 method = brglm2::brglmFit)
tidy(lizardsML)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term         estimate std.error statistic      p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)     1.94      0.341     5.69  0.0000000123
## 2 height&gt;=5ft     1.13      0.257     4.40  0.0000111   
## 3 diameter&gt;2in   -0.763     0.211    -3.61  0.000306    
## 4 lightshady     -0.847     0.322    -2.63  0.00858     
## 5 timemidday      0.227     0.250     0.908 0.364       
## 6 timelate       -0.737     0.299    -2.46  0.0137</code></pre>
<pre class="r"><code>tidy(lizardsBR)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term         estimate std.error statistic      p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)     1.90      0.337     5.64  0.0000000173
## 2 height&gt;=5ft     1.11      0.254     4.35  0.0000137   
## 3 diameter&gt;2in   -0.754     0.210    -3.58  0.000338    
## 4 lightshady     -0.818     0.319    -2.57  0.0103      
## 5 timemidday      0.228     0.249     0.916 0.360       
## 6 timelate       -0.727     0.297    -2.45  0.0145</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-firth1993bias" class="csl-entry">
Firth, David. 1993. <span>“<span class="nocase">Bias reduction of
maximum likelihood estimates</span>.”</span> <em>Biometrika</em> 80 (1):
27–38. <a
href="https://doi.org/10.1093/biomet/80.1.27">https://doi.org/10.1093/biomet/80.1.27</a>.
</div>
<div id="ref-gomila2021logistic" class="csl-entry">
Gomila, Robin. 2021. <span>“Logistic or Linear? Estimating Causal
Effects of Experimental Treatments on Binary Outcomes Using Regression
Analysis.”</span> <em>Journal of Experimental Psychology: General</em>
150 (4): 700. <a
href="https://doi.org/10.1037/xge0000920">https://doi.org/10.1037/xge0000920</a>.
</div>
<div id="ref-hellevik2009linear" class="csl-entry">
Hellevik, Ottar. 2009. <span>“Linear Versus Logistic Regression When the
Dependent Variable Is a Dichotomy.”</span> <em>Quality &amp;
Quantity</em> 43 (1): 59–74. <a
href="https://doi.org/10.1007/s11135-007-9077-3">https://doi.org/10.1007/s11135-007-9077-3</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
