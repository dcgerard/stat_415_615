<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2022-09-16" />

<title>Logistic Regression</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Logistic Regression</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2022-09-16</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Logistic Regression for Binary Response Variables</li>
<li>Chapters 20 and 21 of the <em>Statistical Sleuth</em></li>
</ul>
<pre class="r"><code>library(tidyverse)
library(Sleuth3)
library(broom)
library(sandwich)
library(lmtest)</code></pre>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<ul>
<li><p>Lots of regression problems involve response variables that are
<strong>binary</strong> — the only possible values are either 0 or
1.</p>
<ul>
<li>Alive (1) vs dead (0)</li>
<li>Success (1) vs failure (0)</li>
<li>Presence (1) vs absence (0)</li>
</ul></li>
<li><p>It is arbitrary and unimportant which variable you encode as 1 vs
0.</p>
<ul>
<li>E.g. we could have coded “dead” as 1 and “alive” as 0.</li>
<li>Just keep track of the coding for interpretation purposes.</li>
</ul></li>
<li><p>E.g., either survival (<span class="math inline">\(Y_i =
1\)</span>) or death (<span class="math inline">\(Y_i = 0\)</span>) of
individual <span class="math inline">\(i\)</span> from the Donner Party
of 1846.</p>
<pre class="r"><code>data(&quot;case2001&quot;)
donner &lt;- case2001
glimpse(donner)</code></pre>
<pre><code>## Rows: 45
## Columns: 3
## $ Age    &lt;int&gt; 23, 40, 40, 30, 28, 40, 45, 62, 65, 45, 25, 28, 28, 23, 22, 23,…
## $ Sex    &lt;fct&gt; Male, Female, Male, Male, Male, Male, Female, Male, Male, Femal…
## $ Status &lt;fct&gt; Died, Survived, Survived, Died, Died, Died, Died, Died, Died, D…</code></pre></li>
<li><p>We often want to quantify the association between a quantitative
predictor <span class="math inline">\(x\)</span> and the binary response
<span class="math inline">\(y\)</span>. E.g. older folks tended to die
at a higher frequency:</p>
<pre class="r"><code>qplot(x = Age, y = Status, data = donner, geom = &quot;point&quot;)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
<div id="linear-probability-model-lpm" class="section level1">
<h1>Linear Probability Model (LPM)</h1>
<ul>
<li><p>If your goal is <em>not</em> prediction, then it is not entirely
incorrect to just use linear regression <span class="citation">(Hellevik
2009; Gomila 2021)</span>.</p></li>
<li><p>Let <span class="math inline">\(Y_i \sim Bern(p)\)</span>. That
is <span class="math inline">\(Y_i\)</span> is 1 with probability <span
class="math inline">\(p\)</span> and is 0 with probability <span
class="math inline">\(1-p\)</span>. Then</p>
<ul>
<li><span class="math inline">\(E[Y_i] = p\)</span></li>
<li><span class="math inline">\(var(Y_i) = p(1-p)\)</span></li>
</ul></li>
<li><p>Suppose <span class="math inline">\(Y_i\)</span> is 1 for
“success” and “0” for failure. Suppose we have the model</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots +
\beta_{p-1}X_{i,p-1} + \epsilon_i.
  \]</span></p></li>
<li><p>Since <span class="math inline">\(E[Y_i]\)</span> is the
probability that <span class="math inline">\(Y_i\)</span> is 1, that
mean’s we can interpret the regression line as the probability that
<span class="math inline">\(Y_i\)</span> is 1.</p></li>
<li><p>Interpreting the regression coefficients: “Individuals that are 1
unit larger in <span class="math inline">\(X_{ik}\)</span> are <span
class="math inline">\(\beta_k \times 100\)</span> percentage points more
probable to have a <span class="math inline">\(Y\)</span> value of 1,
adjusting for other variables.”</p></li>
<li><p>Let’s apply this to the Donner party data</p>
<pre class="r"><code>donner &lt;- mutate(donner, Survived = if_else(Status == &quot;Survived&quot;, 1, 0))
lmout &lt;- lm(Survived ~ Age, data = donner)
lmout</code></pre>
<pre><code>## 
## Call:
## lm(formula = Survived ~ Age, data = donner)
## 
## Coefficients:
## (Intercept)          Age  
##      0.8692      -0.0134</code></pre></li>
<li><p>Thus, individuals that were one year older were 1.3 percentage
points less likely to survive.</p></li>
<li><p>Bernoulli random variables are always heteroscedastic since their
variance is a function of the mean <span
class="math inline">\(p(1-p)\)</span>. So if you use the LPM, you
<em>need</em> to use heteroscedastic robust standard errors.</p>
<pre class="r"><code>cout &lt;- coeftest(x = lmout, vcov. = vcovHC)
tidy(cout)</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)   0.869    0.171        5.08 0.00000772
## 2 Age          -0.0134   0.00413     -3.23 0.00235</code></pre></li>
<li><p>Residual plots are kind of useless here:</p>
<pre class="r"><code>aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-6-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="issues" class="section level2">
<h2>Issues</h2>
<ul>
<li><p>Econometricians and psychologists love the LPM because of its
ease of interpretability.</p></li>
<li><p>Statisticians typically do not like it (though I’m
agnostic).</p></li>
<li><p>The possible values of <span class="math inline">\(\beta_0 +
\beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1}\)</span>
are any real numbers between <span
class="math inline">\(-\infty\)</span> and <span
class="math inline">\(\infty\)</span>, but the probability of <span
class="math inline">\(Y_i\)</span> being 1 can only be between 0 and
1.</p></li>
<li><p>So it is possible (and typical in many datasets) to have
probability estimates that are negative or greater than 1. This makes
folks squeamish.</p></li>
<li><p>E.g., the point-wise confidence bands from the Donner party make
no sense:</p>
<pre class="r"><code>ggplot(donner, aes(x = Age, y = Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, formula = y ~ x)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-7-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>When the probability of a “1” is between about 0.2 and 0.8, then
the linear and logistic (see below) models produce about the same
results.</p></li>
<li><p>Let’s look at the differences in the Donner party example:</p>
<pre class="r"><code>ggplot(donner, aes(x = Age, y = Survived)) +
  geom_point() +
  geom_smooth(method = &quot;lm&quot;, se = FALSE, formula = y ~ x) +
  geom_smooth(method = &quot;glm&quot;, 
              se = FALSE,
              method.args = list(family = &quot;binomial&quot;),
              color = &quot;red&quot;, 
              lty = 2,
              formula = y ~ x)</code></pre>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-8-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
</div>
<div id="model" class="section level1">
<h1>Model</h1>
<ul>
<li><p>Let <span class="math inline">\(p_i\)</span> be the probability
that individual <span class="math inline">\(i\)</span> is 1. I.e. <span
class="math inline">\(E[Y_i] = p_i\)</span>. Then our model is <span
class="math display">\[
  \text{logit}(p_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots
\beta_{p-1}X_{i,p-1}
  \]</span></p></li>
<li><p>Here, <span class="math inline">\(\text{logit}(\cdot)\)</span> is
the “logit” function, <span class="math display">\[
  \text{logit}(p_i) = \log\left(\frac{p_i}{1-p_i}\right).
  \]</span></p></li>
<li><p>The inverse of the logit function is the “expit” function (or
“logistic”) <span class="math display">\[
  \text{expit}(\eta) = \frac{e^{\eta}}{1 + e^{\eta}}.
  \]</span></p></li>
<li><p>So we can equivalently write this model as <span
class="math display">\[
  p_i = \text{expit}\left(\beta_0 + \beta_1X_{i1} + \beta_2X_{i2} +
\cdots \beta_{p-1}X_{i,p-1}\right)
  \]</span></p></li>
<li><p>The <span class="math inline">\(\text{expit}(\cdot)\)</span>
function takes a number from <span
class="math inline">\(-\infty\)</span> to <span
class="math inline">\(\infty\)</span> and places it between 0 and 1.
Thus, it forces the probabilities to be between 0 and 1.</p>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="generalized-linear-model" class="section level2">
<h2>Generalized Linear Model</h2>
<ul>
<li><p>This is an example of a “generalized linear model”. Let <span
class="math inline">\(Y_i\)</span> follow any distribution we specify
Let <span class="math inline">\(\mu_i\)</span> be the mean of <span
class="math inline">\(Y_i\)</span>. Then a generalized linear model
is</p>
<p><span class="math display">\[
  g(\mu_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots
\beta_{p-1}X_{i,p-1}
  \]</span></p></li>
<li><p>Here, <span class="math inline">\(g(\cdot)\)</span> is called the
“link function”. The model is linear on the link scale (hence
“generalized linear model”).</p></li>
<li><p>In logistic regression, <span class="math inline">\(Y_i \sim
Bern(\mu_i)\)</span> and <span class="math inline">\(g(\mu_i) =
\text{logit}(\mu_i)\)</span>.</p></li>
<li><p>Another example is probit regression (more popular in econ) where
<span class="math inline">\(g(\mu_i) = \Phi^{-1}(\mu_i)\)</span>, the
standard normal quantile function (<code>qnorm()</code>). This gives you
similar results, but statisicians don’t like it because it’s less
interpretable than the logit.</p>
<p><img src="06_logistic_files/figure-html/unnamed-chunk-10-1.png" width="480" style="display: block; margin: auto;" /></p></li>
<li><p>In <a
href="https://en.wikipedia.org/wiki/Poisson_regression">log-linear
models</a> (which we might later), we model counts with <span
class="math inline">\(Y_i \sim Poi(\mu_i)\)</span> (<a
href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson
Distribution</a>) and <span class="math inline">\(g(\mu_i) =
\log(\mu_i)\)</span>.</p></li>
</ul>
</div>
</div>
<div id="estimation" class="section level1">
<h1>Estimation</h1>
<div id="maximum-likelihood-estimation" class="section level2">
<h2>Maximum Likelihood Estimation</h2>
</div>
<div id="practical-use-in-r" class="section level2">
<h2>Practical Use in R</h2>
<ul>
<li><p>Let <span class="math inline">\(Y_i\)</span> be an indicator for
survival for individual <span class="math inline">\(i\)</span>, let
<span class="math inline">\(X_{i1}\)</span> be the age of individual
<span class="math inline">\(i\)</span>, and let <span
class="math inline">\(X_{i2}\)</span> be an indicator for male Then we
fit the model <span class="math display">\[\begin{align}
  Y_i &amp;\sim Bern(p_i)\\
  \text{logit}(p_i) &amp;= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2}
  \end{align}\]</span></p></li>
<li><p>We fit all generalized linear models in R with
<code>glm()</code>. The arguments are</p>
<ul>
<li><code>formula</code>: The same idea as in <code>lm()</code>.
Response variable on the left of the tilde, explanatory variables on the
right of the tilde.</li>
<li><code>data</code>: The data frame where the variables are.</li>
<li><code>family</code>: The distribution of <span
class="math inline">\(Y_i\)</span>.
<ul>
<li><code>gaussian</code> will assume normal errors (like in
<code>lm()</code>).</li>
<li><code>binomial</code> in this case will fit the bernoulli model
since <span class="math inline">\(Bern(p_i) = Bin(1, p_i)\)</span>.</li>
</ul></li>
<li>You can specify the link function here as well. So for probit models
you would do <code>binomial(link = "probit")</code>.</li>
</ul></li>
<li><p>Let’s fit the model</p>
<pre class="r"><code>glm_out &lt;- glm(Survived ~ Age + Sex, data = donner, family = binomial)</code></pre></li>
<li><p>You again use <code>tidy()</code> from the <code>{broom}</code>
package to get estimates / standard errors</p>
<pre class="r"><code>tidy(glm_out)</code></pre>
<pre><code>## # A tibble: 3 × 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)   3.23      1.39        2.33  0.0198
## 2 Age          -0.0782    0.0373     -2.10  0.0359
## 3 SexMale      -1.60      0.755      -2.11  0.0345</code></pre></li>
<li><p>So the estimated regression surface is <span
class="math display">\[
  \text{logit}(p_i) = 3.23 -0.08X_{1} - 1.60X_{2}
  \]</span></p></li>
<li><p><strong>Exercise</strong>: What if <span
class="math inline">\(X_2\)</span> would have been an indicator for
female? What would the coefficient estimate of <span
class="math inline">\(\beta_2\)</span> be?</p></li>
</ul>
</div>
</div>
<div id="interpretation" class="section level1">
<h1>Interpretation</h1>
<div id="odds" class="section level2">
<h2>Odds</h2>
<ul>
<li><p>The interpretation of the logistic regression model requires you
to be familiar with odds.</p></li>
<li><p>As I write this, the Baltimore Ravens have 3 to 2 odds of
defeating the Miami Dolphins tomorrow. This means that the odds of a
Ravens victory are <span class="math inline">\(3/2 =
1.5\)</span>.</p></li>
<li><p>The odds of an event are the probability of that event divided by
the probability of not that event.</p></li>
<li><p>Let <span class="math inline">\(p\)</span> be the probability of
an event, and let <span class="math inline">\(\omega\)</span> be the
odds of the same event. Then we have the relations: <span
class="math display">\[
  \omega = \frac{p}{1-p}\\
  p = \frac{\omega}{1+\omega}
  \]</span></p></li>
<li><p>So odds are just another description of probabilities.</p></li>
<li><p>In the football example above, the Ravens’ probability of wining
is 0.6, so the odds are 0.6/0.4 = 3/2 = 1.5.</p></li>
<li><p><strong>Exercise</strong>: The Chiefs have 2 to 1 odds over the
Chargers tomorrow. What’s the probability of a Chief’s win?</p></li>
</ul>
</div>
<div id="interpreting-logistic-regression-model" class="section level2">
<h2>Interpreting Logistic Regression Model</h2>
<ul>
<li><p>Let <span class="math inline">\(\omega_{old}\)</span> be the odds
of an individual. We can write the logistic regression model as <span
class="math display">\[
  \omega_{old} = \exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)
  \]</span></p></li>
<li><p>Suppose that a different individual has the exact same predictor
values except one unit higher <span class="math inline">\(X_1\)</span>.
Then <span class="math display">\[\begin{align}
  \omega_{new} &amp;= \exp\left(\beta_0 + \beta_1(X_{1} + 1) +
\beta_2X_{2} + \cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp\left(\beta_0 + \beta_1X_{1} + \beta_1 + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp(\beta_1)\exp\left(\beta_0 + \beta_1X_{1} + \beta_2X_{2} +
\cdots \beta_{p-1}X_{p-1}\right)\\
  &amp;= \exp(\beta_1)\omega_{old}
  \end{align}\]</span></p></li>
<li><p>Thus, the odds of the second individual are <span
class="math inline">\(e^{\beta_1}\)</span> times as large as the odds of
the first individual.</p></li>
<li><p>Different way to say this: The odds ratio for the two individuals
is <span class="math inline">\(e^{\beta_1}\)</span>.</p></li>
<li><p>Recall the estimated regression relationship from the Donner
party example: <span class="math display">\[
  \text{logit}(p_i) = 3.23 -0.08X_{1} - 1.60X_{2}
  \]</span></p></li>
<li><p>So individuals of the same sex that are 10 years younger have
about twice the odds of surviving (<span class="math inline">\(e^{0.08}
\times 10} = 2.186\)</span>). Also, a woman’s odds of survival were
about 5 times that of a man of the same age (<span
class="math inline">\(e^{1.6} = 4.95\)</span>).</p></li>
<li><p><strong>Exercise</strong>: The nocturia data set, described <a
href="https://dcgerard.github.io/stat_415_615/data.html#Nocturia">here</a>
and dowloadable from <a
href="https://dcgerard.github.io/stat_415_615/data/nocturia.csv">here</a>,
contains patient covariates that are believed to be associated with
whether the individual has nocturia (wakes up to pee). Download the
data, the fit of logistic regression model of nocturia on age. Interpret
the resulting coefficients.</p></li>
</ul>
</div>
</div>
<div id="retrospective-studies" class="section level1">
<h1>Retrospective studies</h1>
<ul>
<li><p>In a <strong>prospective</strong> study, the explanatory
variables are fixed and we imagine <span
class="math inline">\(Y_i\)</span> being sampled given <span
class="math inline">\(X_i\)</span>.</p>
<ul>
<li>E.g. we might choose <span class="math inline">\(200\)</span>
patients and observe their cancer status.</li>
</ul></li>
<li><p>In a <strong>retrospective</strong> study, we fix <span
class="math inline">\(Y_i\)</span> and later observe their <span
class="math inline">\(X_i\)</span>.</p>
<ul>
<li>E.g. we might choose <span class="math inline">\(n = 100\)</span>
cancer patients and <span class="math inline">\(n = 100\)</span>
controls.</li>
<li>This is typically done with the probability of one group is really
small and you want more samples of that group.</li>
</ul></li>
<li><p>The LPM model (and probit model) are <strong>not</strong> valid
in retrospective studies, but the logistic regression model
<strong>is</strong> valid. So in retrospective studies you
<strong>must</strong> use logistic regression.</p></li>
<li><p>The reason is that the odds ratios for prospective studies and
retrospective studies are the same, even though the probabilities are
not.</p></li>
<li><p>You can interpret slopes in the usual way in retrospective
studies, but you cannot interpret the <span
class="math inline">\(Y\)</span>-intercept, or <strong>any</strong>
fixed estimate of the probability. In these studies,
<strong>only</strong> the slope (the odds ratio) is
interpretable.</p></li>
</ul>
</div>
<div id="bias-reduction" class="section level1">
<h1>Bias reduction</h1>
<ul>
<li>For small sample sizes, the MLE’s of the logistic regression
coefficients can be biased. A quick way to reduce this bias is through
the <a
href="https://cran.r-project.org/package=brglm2"><code>{brglm2}</code></a>
package, which uses the method of <span class="citation">Firth
(1993)</span>.</li>
</ul>
<p>Their example:</p>
<pre class="r"><code>## The lizards example from ?brglm::brglm
data(&quot;lizards&quot;, package = &quot;brglm2&quot;)
# Fit the model using maximum likelihood
lizardsML &lt;- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards)
# Mean bias-reduced fit:
lizardsBR &lt;- glm(cbind(grahami, opalinus) ~ height + diameter +
                   light + time, family = binomial(logit), data = lizards,
                 method = brglm2::brglmFit)
tidy(lizardsML)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term         estimate std.error statistic      p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)     1.94      0.341     5.69  0.0000000123
## 2 height&gt;=5ft     1.13      0.257     4.40  0.0000111   
## 3 diameter&gt;2in   -0.763     0.211    -3.61  0.000306    
## 4 lightshady     -0.847     0.322    -2.63  0.00858     
## 5 timemidday      0.227     0.250     0.908 0.364       
## 6 timelate       -0.737     0.299    -2.46  0.0137</code></pre>
<pre class="r"><code>tidy(lizardsBR)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term         estimate std.error statistic      p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)     1.90      0.337     5.64  0.0000000173
## 2 height&gt;=5ft     1.11      0.254     4.35  0.0000137   
## 3 diameter&gt;2in   -0.754     0.210    -3.58  0.000338    
## 4 lightshady     -0.818     0.319    -2.57  0.0103      
## 5 timemidday      0.228     0.249     0.916 0.360       
## 6 timelate       -0.727     0.297    -2.45  0.0145</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-firth1993bias" class="csl-entry">
Firth, David. 1993. <span>“<span class="nocase">Bias reduction of
maximum likelihood estimates</span>.”</span> <em>Biometrika</em> 80 (1):
27–38. <a
href="https://doi.org/10.1093/biomet/80.1.27">https://doi.org/10.1093/biomet/80.1.27</a>.
</div>
<div id="ref-gomila2021logistic" class="csl-entry">
Gomila, Robin. 2021. <span>“Logistic or Linear? Estimating Causal
Effects of Experimental Treatments on Binary Outcomes Using Regression
Analysis.”</span> <em>Journal of Experimental Psychology: General</em>
150 (4): 700. <a
href="https://doi.org/10.1037/xge0000920">https://doi.org/10.1037/xge0000920</a>.
</div>
<div id="ref-hellevik2009linear" class="csl-entry">
Hellevik, Ottar. 2009. <span>“Linear Versus Logistic Regression When the
Dependent Variable Is a Dichotomy.”</span> <em>Quality &amp;
Quantity</em> 43 (1): 59–74. <a
href="https://doi.org/10.1007/s11135-007-9077-3">https://doi.org/10.1007/s11135-007-9077-3</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
