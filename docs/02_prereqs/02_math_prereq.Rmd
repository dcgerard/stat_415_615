---
title: "Math Prerequisites"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
urlcolor: "blue"
---

```{r setup, include=FALSE}
library(latex2exp)
library(broom)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center")
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Learning Objectives

- Minimum mathematics required to complete this course.
- Summations/averages
- Equations for lines
- Logarithms/exponentials
- Chapter 3 of [ROS](https://avehtari.github.io/ROS-Examples/)

# Motivation

- Mathematics are the building blocks of linear regression.

- You must be proficient with linear equations and summations to implement and understand linear models.

- Log/exponential transformations are important for the practice and interpretation of many types of linear models.

- Here, we will provide a brief mathematical review.

# Weighted Averages

- Consider the following data on USA, Mexico, and Canada:

    | Label  | Population $N_j$  | Average Age $\bar{y}_j$|
    |--------|-------------------|------------------------|
    | USA    | 310 Million       | 36.8                   |
    | Mexico | 112 Million       | 26.7                   |
    | Canada | 34 Million        | 40.7                   |
    
- What is the average age for these three countries?

- Proportion USA: $\frac{310}{310 + 112 + 34} = 0.68$.

- Proportion Mexico: $\frac{112}{310 + 112 + 34} = 0.25$.

- Proportion Canada: $\frac{34}{310 + 112 + 34} = 0.07$.

- So the USA contributes 68\% of the population, mexico contributes 25\% of the population, and Canada contributes 7\% of the population. To find the overall average age, we calculate:

$$
0.68 \times 36.8 + 0.25 \times 26.7 + 0.07 \times 40.7 = 34.6
$$

- We can equivalently write this as

$$
\frac{310 \times 36.8 + 112 \times 26.7 + 34 \times 40.7}{310 + 112 + 34} = \frac{310}{456}\times 36.8 + \frac{112}{456}\times 26.7 + \frac{34}{456}\times 40.7
$$

- The proportions 0.68, 0.25, and 0.07 are called the **weights**. When the weights sum to one, the overall sumation is called a **weighted average**.

- In **summation notation** (using the capital-sigma), we would write:

    \begin{align}
    \text{weighted average} &= \sum_{j=1}^n w_j y_j\\
    &= w_1y_1 + w_2y_2 + \cdots w_ny_n
    \end{align}
    
    where $w_j$ is the $j$th weight and $y_j$ is the $j$th value.

- What would be an "unweighted" average? This is where each $w_j = \frac{1}{n}$ since

    \begin{align}
    \sum_{j=1}^n w_j y_j &= \sum_{j=1}^n \frac{1}{n} y_j\\
    &= \frac{1}{n}y_1 + \frac{1}{n}y_2 + \cdots + \frac{1}{n}y_n\\
    &= \frac{1}{n}(y_1 +y_2 + \cdots + y_n)\\
    &= \frac{1}{n}\sum_{j=1}^n y_j\\
    &= \bar{y}
    \end{align}
    
- Properties of summations:
    - $\sum_{i=1}^n c a_i = c\sum_{i=1}^na_i$
    - $\sum_{i=1}^n a_i + \sum_{i=1}^nb_i = \sum_{i=1}^n(a_i + b_i)$
    
- **Exercise**: 51\% of Americans are female while 49\% of Americans are male. 79\% of teachers are female while 21\% of teachers are male. Female teachers make on average \$45,865, while male teachers make on average \$49,207. What is the average salary for all teachers?

    ```{block, eval = FALSE, echo = FALSE}
    We need to use the 79\%/21\% split, since we are interested in the population of teachers, not the total population. So we have
    
    0.79 * 45865 + 0.21 * 49207 = 46567
    ```

- **Exercise**: What is $\sum_{i=0}^4 i$?

    ```{block, eval = FALSE, echo = FALSE}
    0 + 1 + 2 + 3 + 4 = 10
    ```

- **Exercise**: Prove that it is not generally true that $\left(\sum_{i=1}^n y_i\right)^2 = \sum_{i=1}^n y_i^2$ (*hint*: provide a counterexample).

    ```{block, eval = FALSE, echo = FALSE}
    $(1 + 2)^2 = 3^2 = 9 \neq 5 = 1^2 + 2^2$
    ```
    
## Products

- We use capital-pi notation to represent product.

    $$
    \prod_{i=1}^n a_i = a_1 \times a_2 \times \cdots \times a_n
    $$

# Lines

- All lines are of the form 
    $$
    y = \beta_0 + \beta_1 x
    $$

- $\beta_1$ is the **slope**, the amount $y$ is larger by when $x$ is 1 unit larger.

- When $\beta_1$ is *negative*, the line slopes *down*.

    ```{r, echo = FALSE, fig.height=3, fig.width=5}
    a <- 0.95
    b <- -0.4
    par(mar=c(3,3,1,1), mgp=c(2,.5,0), tck=-.01)
    plot(c(0,2.2), c(0,a+.2), pch=20, cex=.5, main=TeX("$y = \\beta_0 + \\beta_1 x$ (with $\\beta_1 < 0$)"),
      bty="l", type="n", xlab="x", ylab="y", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
    axis(1, c(0,1,2))
    axis(2, c(a,a+b,a+2*b), c(TeX("$\\beta_0$"),TeX("$\\beta_0+\\beta_1$"),TeX("$\\beta_0+2\\beta_1$")))
    abline(a, b, lwd = 3, col = "blue")
    ```

- When $\beta_1$ is *positive*, the line slopes *up*.

    ```{r, echo = FALSE, fig.height=3, fig.width=5}
    a <- 0.15
    b <- 0.4
    par(mar=c(3,3,1,1), mgp=c(2,.5,0), tck=-.01)
    plot(c(0,2.2), c(0,a+2.2*b), pch=20, cex=.5, main=TeX("$y = \\beta_0 + \\beta_1 x$ (with $\\beta_1 > 0$)"),
      bty="l", type="n", xlab="x", ylab="y", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
    axis(1, c(0,1,2))
    axis(2, c(a,a+b,a+2*b), c(TeX("$\\beta_0$"),TeX("$\\beta_0+\\beta_1$"),TeX("$\\beta_0+2\\beta_1$")))
    abline(a, b, lwd = 3, col = "blue")
    ```

- When $\beta_1$ is *0*, the line is *horizontal*. In this case, $y$ is the same for every value of $x$ (in other words, $x$ does not affect $y$).

    ```{r, echo = FALSE, fig.height=3, fig.width=5}
    a <- 0.15
    b <- 0
    par(mar=c(3,3,1,1), mgp=c(2,.5,0), tck=-.01)
    plot(c(0, 2), c(a, a), pch=20, cex=.5, main=TeX("$y = \\beta_0 + \\beta_1 x$ (with $\\beta_1 = 0$)"),
      bty="l", type="n", xlab="x", ylab="y", xaxt="n", yaxt="n", xaxs="i", yaxs="i")
    axis(1, c(0,1,2))
    axis(2, c(a), c(TeX("$\\beta_0$")))
    abline(a, b, lwd = 3, col = "blue")
    ```
    
- **Example**: The progression of mile world records during the 20th century is well approximated by the line $y = 1007 - 0.393x$

    ```{r, message = FALSE}
    library(readr)
    library(ggplot2)
    mile <- read_csv("https://dcgerard.github.io/stat_415_615/data/mile.csv")
    qplot(x = year, y = seconds, data = mile) + 
      geom_abline(slope = -0.393, intercept = 1007, lty = 2, col = 2)
    ```
    
- The world record in 1950 was about 
    $$
    y = 1007 - 0.393 * 1950 = 240.6 \text{ seconds},
    $$
    which is actually between the two world record values of 241.4 seconds in 1946 and 239.4 seconds in 1954.
    
- Interpretation of $\beta_0$: Can we interpret 1007 seconds (16.8 minutes) as the approximate world record in ancient times? No! Our data are only from 1913 to 1999, so this is an obviously improper extrapolation. It is just the $y$-intercept, with no other interpretation.

- Interpretation of $\beta_1$: For two adjacent years, the world record is about 0.393 seconds less for the more recent year.

- Do *not* say "the world record decreases by about 0.393 seconds each year" as this creates an implicit causal connection.

- **Exercise**: Year versus maximum life expectancy (where maximum was taken over country) is well approximated by the line $y = -296 + 0.189 x$.
    
    ```{r, echo = FALSE, message = FALSE}
    library(gapminder)
    library(tidyverse)
    data("gapminder_unfiltered")
    gapminder_unfiltered %>%
      group_by(year) %>%
      filter(lifeExp == max(lifeExp)) %>%
      select(country, lifeExp, year) %>%
      arrange(year) ->
      sumdat
    ggplot(sumdat, aes(x = year, y = lifeExp)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE) +
      ylab("Maximum Life Expectancy")
    ```
    
    a. What is the expected life expectancy in 1990?
        ```{block, eval = FALSE, echo = FALSE}
        -296 + 0.189 * 1990 = 80.11
        ```
    b. Interpret -296
        ```{block, eval = FALSE, echo = FALSE}
        It is the $y$-intercept
        ```
    c. Interpret 0.189
        ```{block, eval = FALSE, echo = FALSE}
        The life expectancy is about 0.189 years higher each subsequent year.
        ```

# Additive/Multiplicative differences

- This semester, we will spend a lot of time talking about differences in one variable associated with differences in another variable.

- We need to know how to properly speak about "differences".

- Suppose car $A$ has a gas mileage of 30 mpg, and car $B$ has a gas mileage of 20 mpg. Then obviously car $A$ has 10 mpg better gas mileage. This is called an *additive* difference since we can write it as $A + -B = 10$.

- We can also describe this difference as *multiplicative*. Here are some common ways to describe multiplicative differences.

- $A$ has 50 percent more gas mileage than $B$. 
    - $A = B(1 + 0.5) = B + 0.5B = B + 50\% B$
    - Thus, $A$ is $B$ plus 50 percent of $B$'s value. This is why we say "50 percent more".
    
- $B$ has 66.7 percent the value of $A$.
    - This is since $B / A = 2/3 \approx 0.667 = 66.7\%$.
    
- $B$ has 33.3 percent less gas mileage than $A$.
    - $B = A(1 - 1/3) = A - (1/3)A \approx A - 0.333A = A - 33.3\%A$
    - Thus, $B$ is $A$ minus 33.3 percent of $A$'s value. This is why we say "33.3 percent less".
    
- **Exercise**: Suppose John makes \$40,000 a year and Alina makes \$50,000 a year. Provide three different ways to describe the multiplicative difference between John's and Alina's salaries.

    ```{block, eval = FALSE, echo = FALSE}
    - Alina makes 25\% more money than John.
    - John makes 20\% less money than Alina.
    - John's salary is 80\% that of Alina's.
    ```

- Sometimes we have additive differences for a varaible whose scale is percent.

- For example, candidate $A$ won 40\% of the vote, and candidate $B$ won 30\% of the vote.

- If we want to describe the *additive* difference between these two candidates, we cannot say "percent" because that would imply a mulitiplicative difference.

- To describe additive differences of a variable whose units are percent, we say **percentage point**. 

- Thus, candidate $A$'s vote share is 10 percentage points higher than candidate $B$'s.

- We can still describe multiplicative differences for these variables. E.g. Candidate $A$'s vote share is 33.3 percent higher than candidate $B$'s.

# Logarithms and Exponentials

- Often, we consider linear relationships on the log-scale. So we need to know something about logarithms and exponentials.

- Let's start with exponentials:
    $$
    \exp(x) = e^x = \underbrace{e \times \dots \times e}_{x\, \textrm{times}}
    $$.
    
    ```{r}
    ## Define e
    e <- exp(1)
    e
    
    ## show exp(3) == e * e * e
    exp(3) 
    e * e * e
    ```
    
- Recall that $e$ is [Euler's number](https://en.wikipedia.org/wiki/E_(mathematical_constant)), which is about `r exp(1)`.

- The last equality in the above equation for `exp()` only follows if $x$ is positive integer, but exponentiation can be extended to any real number.

    ```{r}
    exp(1.414)
    ```

- $\log(x)$ is the **natural** logarithm of $x$ (**not** base 10). This is the inverse of exponentiation
    $$
    \log(\exp(x)) = \exp(\log(x)) = x
    $$
    
- You can verify this in R
    ```{r}
    exp(log(31))
    log(exp(31))
    ```
    
- You can also interpret $\log(x)$ as the number of times you have to divide $x$ by $e$ to obtain 1. For example, since you would have to divide $e^4$ by $e$ 4 times to get 1 ($1 = \frac{e^4}{e \times e \times e \times e}$), we have that $\log(e^4) = 4$.

- A useful property of logs/exponents is how it can convert multiplication to summation and vice versa.
    - $\exp(a + b) = \exp(a)\exp(b)$.
    - $\log(ab) = \log(a) + \log(b)$.
    
- Why do we care about exponentials and logarithms? Because many relationships in the real world follow **exponential** or **power** laws.

## Exponential growth/decline

- $\log(y) = \beta_0 + \beta_1 x$ represents **exponential growth** if $\beta_1 > 0$ and **exponential decline** if $\beta_1 < 0$.

- Exponentiating both sides, we get
    $$
    y = e^{\beta_0}e^{\beta_1 x}
    $$
    
- Exponential growth:

    ```{r, echo = FALSE}
    xseq <- seq(0, 4, length.out = 100)
    df <- tibble(x = xseq, y = exp(xseq))
    ggplot(df, aes(x = x, y = y)) +
      geom_line() +
      theme(axis.text = element_blank(),
            axis.ticks = element_blank())
    ```
    
- Exponential decline:

    ```{r, echo = FALSE}
    xseq <- seq(0, 4, length.out = 100)
    df <- tibble(x = xseq, y = exp(-xseq))
    ggplot(df, aes(x = x, y = y)) +
      geom_line() +
      theme(axis.text = element_blank(),
            axis.ticks = element_blank())
    ```

- $e^{\beta_0}$ is the value of $y$ when $x = 0$

- $\beta_1$ determines the rate of growth or decline.

- A 1-unit difference in $x$ corresponds to a multiplicative factor of $e^{\beta_1}$ in $y$. (you multiply the old $y$ value by $e^{\beta_1}$ to figure out the new $y$ value when you have an $x$ value that is 1 larger).
    
- This follows from:
    \begin{align}
    y_{old} &= e^{\beta_0}e^{\beta_1 x}\\
    y_{new} &= e^{\beta_0}e^{\beta_1 (x + 1)}\\
            &= e^{\beta_0}e^{\beta_1 x + \beta_1}\\
            &= e^{\beta_0}e^{\beta_1 x} e^{\beta_1}\\
            &= y_{old}e^{\beta_1}
    \end{align}
    
- **Example**: The early growth of [COVID-19 in DC](https://dcgerard.github.io/stat_415_615/data.html#dc-covid-tests) looked exponential:
    ```{r, message = FALSE}
    dc <- read_csv("https://dcgerard.github.io/stat_415_615/data/dccovid.csv")
    dc <- select(dc, day, positives)
    dc <- filter(dc, day <= "2020-04-01", day >= "2020-03-11") 
    qplot(x = day, y = positives, data = dc)
    ```
    
    We determine if an exponential growth model is appropriate by seeing if day versus log-positives is approximately linear:
    
    ```{r}
    dc <- mutate(dc, 
                 lpos = log(positives),
                 daysin = as.numeric(day) - as.numeric(day[[1]]))
    qplot(x = daysin, y = lpos, data = dc)
    ```
    
    ```{r, echo = FALSE}
    coefvec <- coef(lm(lpos ~ daysin, data = dc))
    beta0 <- coefvec[[1]]
    beta1 <- coefvec[[2]]
    ```

    A curve that fits this growth well is $y = `r exp(beta0)`e^{`r beta1`x}$.
    
    ```{r, echo = FALSE}
    df <- data.frame(x = seq(min(dc$daysin), max(dc$daysin), length.out = 100))
    df$y <- exp(beta0) * exp(df$x * beta1)
    ggplot() +
      geom_point(data = dc, mapping = aes(x = daysin, y = positives)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```
    
    So that means that early on in the pandemic, each day positive tests were multiplicatively increasing by about $e^{\beta_1} = e^{`r beta1`} = `r exp(beta1)`$. Or, about a 24\% increase each day.
    
- **Exercise**: Consider the population growth of DC from 1800 to 1950, taken from [Wikipedia](https://en.wikipedia.org/wiki/Washington,_D.C.#Demographics).
    ```{r}
    dcpop <- tribble(~year,   ~pop,
                     1800L,   8144,
                     1810L,  15471,
                     1820L,  23336,
                     1830L,  30261,
                     1840L,  33745,
                     1850L,  51687,
                     1860L,  75080,
                     1870L, 131700,
                     1880L, 177624,
                     1890L, 230392,
                     1900L, 278718,
                     1910L, 331069,
                     1920L, 437571,
                     1930L, 486869,
                     1940L, 663091,
                     1950L, 802178)
    qplot(x = year, y = pop, data = dcpop)
    ```
    ```{r, echo = FALSE}
    coefvec <- coef(lm(log(pop) ~ year, data = dcpop))
    beta0 <- coefvec[[1]]
    beta1 <- coefvec[[2]]
    ```
    
    A researcher has determined that the following relationship approximates this growth well:
    
    $$
    \log(y) = `r beta0` + `r beta1` x
    $$
    
    ```{r, echo = FALSE}
    df <- data.frame(x = seq(min(dcpop$year), max(dcpop$year), length.out = 100))
    df$y <- exp(beta0) * exp(df$x * beta1)
    ggplot() +
      geom_point(data = dcpop, mapping = aes(x = year, y = pop)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```
    

    1. Interpret the `r beta1` value.
    
    ```{block, eval = FALSE, echo = FALSE}
    DC experienced about 3\% growth each year. This is since `exp(0.0289) = 1.03`.
    ```

    2. Interpret the `r beta0` value.

    ```{block, eval = FALSE, echo = FALSE}
    It is the $y$-intercept of the line $\log(y) = \beta_0 + \beta_1x$. It has no other interpretation because the range of values are from 1800 to 1950, and 0 is outside of this range.
    
    So it is **incorrect** to say that `exp(beta0)` is the population at year 0.
    ```
    
    3. What is the average growth every 10 years?
    
    ```{block, echo = FALSE, eval = FALSE}
    `exp(10 * 0.0299)` = 1.349, or about 35\% growth every 10 years.
    ```
    
## Why use $e$ and not some other base?

- Tradition.

- For small values of $\beta_1$ (say, $-0.1 \leq \beta_1 \leq 0.1$), we can interpret $\beta_1$ as the proportion change for a 1 unit difference in $x$.

- This is because for small $\beta_1$, we have $e^{\beta_1} \approx 1 + \beta_1$.

```{r, echo = FALSE}
tibble(beta1 = seq(0, 0.2, length = 11)) %>%
  mutate(`exp(beta1)` = exp(beta1),
         `Percent Change` = (`exp(beta1)` - 1) * 100) %>%
  knitr::kable()
```

- In many real world applications, $\beta_1$ is typically small.

- This relationship does not hold with other bases. E.g. $10^{0.02} = `r 10^0.02` \not\approx 1 + 0.02$.

- **Example**: From the DC population example above, we had $\beta_1 = `r beta1`$ and $e^{\beta_1} = `r exp(beta1)`$. So, a 3\% increase in population each year, and you can get that either from $\beta_1$ or $e^{\beta_1}$.

## Power-law growth/decline

- $\log(y) = \beta_0 + \beta_1 \log(x)$ represents **power-law growth** if $\beta_1 > 0$ and **power-law decline** if $\beta_1 < 0$.
    - Sub-linear growth if $0 < \beta_1 < 1$.
    - Super-linear growth if $\beta_1 > 1$.

- Exponentiating both sides, we get the relationship
    $$
    y = e^{\beta_0}x^{\beta_1}
    $$

- Interpret $\beta_0$: The value of $y$ when $x = 1$ is $e^{\beta_0}$.

- Interpret $\beta_1$: 
    - When you double $x$, you multiply $y$ by $2^{\beta_1}$.
    - When you multiply $x$ by 10, you multiply $y$ by $10^{\beta_1}$.
    - When you multiply $x$ by 1.1 (10\% increase), you multiply $y$ by $1.1^{\beta_1}$.
    - Choose a multiplier that makes sense for the range of your data. E.g., if it is never the case that one value is 10 times larger than another, don't use that as the interpretation.
    
- This follows from:

    \begin{align}
    y_{old} &= e^{\beta_0}x^{\beta_1}\\
    y_{new} &= e^{\beta_0}(2x)^{\beta_1}\\
            &= e^{\beta_0}2^{\beta_1}x^{\beta_1}\\
            &= 2^{\beta_1}e^{\beta_0}x^{\beta_1}\\
            &= 2^{\beta_1}y_{old}
    \end{align}
    
- **Example**: From the `{Sleuth3}` R package, we have a dataset on 7 islands measuring their land area and the number of species on each island. The goal is to estimate the relationship between these two variables, which has applications in conservation.

    ```{r}
    library(Sleuth3)
    data("case0801")
    qplot(x = Area, y = Species, data = case0801)
    ```
    
    ```{r, echo = FALSE}
    coefout <- coef(lm(log(Species) ~ log(Area), data = case0801))
    beta0 <- coefout[[1]]
    beta1 <- coefout[[2]]
    ```
    
    - This relationship is well-approximated by the curve $y = `r exp(beta0)`x^{`r beta1`}$.

    ```{r, echo = FALSE}
    df <- data.frame(x = seq(0, max(case0801$Area), length.out = 100))
    df$y <- exp(beta0) * df$x^beta1
    ggplot() +
      geom_point(data = case0801, aes(x = Area, y = Species)) +
      geom_line(data = df, aes(x = x, y = y))
    ```

    - So islands that are twice as large tend to have $2^{\beta_1} = 2^{`r beta1`} = `r 2^beta1`$ times as many species. Or, islands that are twice as large tend to have 19\% more species.
    
    - Notice that I didn't use the language "change" or "increase", because those would imply causal connections.
    
- **Exercise**: Prove that the interpretation "when you multiply $x$ by 10, you multiply $y$ by $10^{\beta_1}$" is correct.

    ```{block, eval = FALSE, echo = FALSE}
    Just redo the calculations for the interpretation of doubling, but with 10 instead of 2.
    ```

- **Exercise**: Consider the wine consumption and heart disease data from the `{Sleuth3}` package. The observational units are the counties and the variables are
    - `Wine`: consumption of wine (liters per person per year)
    - `Mortality`: heart disease mortality rate (deaths per 1,000)

    ```{r}
    data("ex0823")
    qplot(x = Wine, y = Mortality, data = ex0823)
    ```
 
    - Researchers have determined that this relationship is well-approximated by the following equation.
    
    ```{r, echo = FALSE}
    coefout <- coef(lm(log(Mortality) ~ log(Wine), data = ex0823))
    beta0 <- coefout[[1]]
    beta1 <- coefout[[2]]
    ```
    
    $$
    y = `r exp(beta0)`x^{`r beta1`}
    $$
    
    ```{r, echo = FALSE}
    df <- data.frame(x = seq(min(ex0823$Wine), max(ex0823$Wine), length.out = 100))
    df$y <- exp(beta0) * df$x ^ beta1
    ggplot() +
      geom_point(data = ex0823, mapping = aes(x = Wine, y = Mortality)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```

    1. Interpret `r beta1`.
    
    ```{block, eval = FALSE, echo = FALSE}
    Countries that drink twice as much wine tend to have mortality rates 22% lower. This is since 2^-0.3556 = 0.7815.
    ```
    
    2. Interpret `r beta0` (the value of $\beta_0$).
    
    ```{block, eval = FALSE, echo = FALSE}
    The y-intercept of the line of log(Mortality) on log(Wine). You can kind of interpret $exp(\beta_0) = 12.88$ as the average mortality rate of countries that don't drink, since the data are near there, but it is a stretch since no countries are at 0.
    ```
        
    3. If one country drinks 50\% more wine per person per year than another country, what is the expected difference in mortality rates?
    
    ```{block, echo = FALSE, eval = FALSE}
    `1.5^-0.3556 = 0.8657`, so a mortality rate that is 13\% lower.
    ```
        
## Logging just the predictor variable

- Sometimes, the relationship is of the form
    $$
    y = \beta_0 + \beta_1 \log(x)
    $$

- You intrepret $\beta_1$ with the following:
    - When you double $x$, you add $\beta_1\log(2)$ to $y$.
    - When you multiply $x$ by 10, you add $\beta_1\log(10)$ to $y$.
    - When you multiply $x$ by 1.1 (10\% increase), you add $\beta_1\log(1.1)$ to $y$.
    - Choose a multiplier that makes sense for the range of your data. E.g. if it is never the case that one value is 10 times larger than another, don't use that as the interpretation.
    
- This follows from
    \begin{align}
    y_{old} &= \beta_0 + \beta_1\log(x)\\
    y_{new} &=  \beta_0 + \beta_1\log(2x)\\
            &=  \beta_0 + \beta_1[\log(x) + \log(2)]\\
            &=  \beta_0 + \beta_1\log(x) + \beta_1\log(2)\\
            &= y_{old} + \beta_1\log(2).
    \end{align}
 
```{r, echo = FALSE}
data("mtcars")
mtcars <- mutate(mtcars, log_wt = log(wt))
lm_mt <- lm(mpg ~ log_wt, data = mtcars)
tout <- tidy(lm_mt)
beta0 <- tout$estimate[[1]]
beta1 <- tout$estimate[[2]]
```
 
 - Example: From the `mtcars` dataset, it was determined that the relationship between `mpg` and `wt` was well approximated by the curve $y = `r beta0` + `r beta1`\log(x)$.
    ```{r}
    df <- data.frame(x = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100))
    df$y <- beta0 + beta1 * log(df$x)
    ggplot() +
      geom_point(data = mtcars, mapping = aes(x = wt, y = mpg)) +
      geom_line(data = df, mapping = aes(x = x, y = y))
    ```

- So cars that are 50\% heavier tend to have $-17.0 \times \log(1.5) = -6.9$ worse miles per gallon.

## Summary of relationships on different scales

- If relationship is $y = \beta_0 + \beta_1x$, then add $c$ to $x$ means add $c\beta_1$ to $y$.

- If relationship is $y = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means add $\beta_1\log(c)$ to $y$.

- If relationship is $\log(y) = \beta_0 + \beta_1 x$, then add $c$ to $x$ means multiply $y$ by $\exp(c\beta_1)$.

- If relationship is $\log(y) = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means multiply $y$ by $c^{\beta_1}$.
