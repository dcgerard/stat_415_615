---
title: "Equation Review"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
urlcolor: "blue"
---

```{r setup, include=FALSE}
set.seed(1)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Summary

Here is a list of the equations I want you to have memorized on the top of your head.

# List

- Sample Mean: $\bar{Y} = \frac{1}{n}\sum_{i=1}^nY_i$.

- Sample variance: $s_y = \frac{1}{n-1}\sum_{i=1}^n(Y_i - \bar{Y})^2$.

- SLR Model: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$, $E[\epsilon_i] = 0$, $var(\epsilon_i) = \sigma^2$, $cor(\epsilon_i, \epsilon_j) = 0$ for $i\neq j$.

- Normal SLR Model: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$, $\epsilon \overset{iid}{\sim} N(0, \sigma^2)$.

- OLS Objective: $\sum_{i=1}^n\left[Y_i - (\beta_0 + \beta_1X_i)\right]^2$

- SLR OLS estimates: $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$, $\hat{\beta}_1 = cor(X, Y)\frac{sd(Y)}{sd(X)}$.

- Fitted values: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$

- Residuals: $e_i = Y_i - \hat{Y}_i$.

- Properties of fitted regression line: (i) mean of residuals is 0, (ii) mean of observed values equals mean of fitted values, (iii) residuals are uncorrelated with predictors, (iv) residuals are uncorrelated with *fitted* values, and (v) regression line always goes through mean $(\bar{X}, \bar{Y})$.

- MSE: $MSE = \frac{1}{n-p}\sum_{i=1}^n\left[Y_i - \hat{Y}_i\right]^2$, where $p=2$ in SLR.
    - The MSE is the estimate of $\sigma^2$.
    
- $t$-statistic: $t^* = \frac{\hat{\beta}_1}{s(\hat{\beta}_1)}$, which follows a $t_{n-p}$ distribution under the null that $\beta_1 = 0$.

    - If the null is $H_0: \beta_1 = c$ for some constant $c$, then the $t$-statistic is then $t^* = \frac{\hat{\beta}_1 - c}{s(\hat{\beta}_1)}$. This also follows a $t_{n-p}$ distribution under the null that $\beta_1 = c$.

- Get two-sided $p$-value manually via $2 * pt(q = -abs(t^*), df = n - p)$, where $p=2$ in SLR.

- Confidence interval = estimate $\pm$ multiplier $\times$ standard error
    - Typically, multiplier = $qt(1 - \alpha/2, df)$, tell me what $\alpha$ and $df = n-p$ should be. Note, $p=2$ in SLR.
    - In prediction interval, $s^2(pred) = s^2(\hat{Y}_i) + MSE$.
    
- $SSE = \sum_{i=1}^n(Y_i - \hat{Y}_i)^2$, with $df$ of $n - p$

- $SSR = \sum_{i=1}^n(\hat{Y}_i - \bar{Y}_i)^2$, with $df$ of $p - 1$ 

- $SSTO = \sum_{i=1}^n(Y_i - \bar{Y}_i)^2$, with $df$ of $n-1$

- $SSTO = SSE + SSR$

- $F^* = \frac{[SSE(R) - SSE(F)]/[df_R - df_F]]}{SSE(F)/df_F}$ which follows a $F(dr_R - df_F, df_F)$ distribution under the null of the reduced model.
    - Tell me what $df_R$ and $df_F$ should be.

- $R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}$

- Interpretations:
    - If relationship is $y = \beta_0 + \beta_1x$, then add $c$ to $x$ means add $c\beta_1$ to $y$.
        - "Individuals that are $c$ units higher in $x$ are $c\beta_1$ units higher in $y$ on average."
    - If relationship is $y = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means add $\beta_1\log(c)$ to $y$.
        - "Individuals that are $c$ times higher in $x$ are $\beta_1\log(c)$ units higher in $y$ on average."
    - If relationship is $\log(y) = \beta_0 + \beta_1 x$, then add $c$ to $x$ means multiply $y$ by $\exp(c\beta_1)$.
        - "Individuals that are $c$ units higher in $x$ are $\exp(c\beta_1)$ times higher in $y$ on average."
    - If relationship is $\log(y) = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means multiply $y$ by $c^{\beta_1}$.
        - "Individuals that are $c$ times higher in $x$ are $c^{\beta_1}$ times higher in $y$ on average."
    - Make sure you describe interpretations in terms of the original variables (not saying "x" and "y").
    - Make sure you do not use causal language, implying any sort of "change" to the x variable.
    
- Bonferroni corrected $p$-values: unadjusted $p$-value $\times$ number of tests.

- Matrix stuff
    - $x_{ij}$ is the $(i, j)$th element of the matrix $\mathbf{X}$.
    - Matrix transpose definition $\mathbf{X}^T$
    - Identity matrix, $\mathbf{I}_n$.
    - Matrix multiplication rules.
    - Properties of matrix inverse.
    - Matrix form of linear model: $\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$
    - Matrix form of coefficient estimates: $\hat{\mathbf{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$
    - Hat matrix, $\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$, and why it is called the hat matrix.

- Multiple linear regression model: 
    \begin{align}
    Y_i &= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \cdots + \beta_{p-1}X_{i,p-1} + \epsilon_i\\
    E[\epsilon_i] &= 0\\
    var(\epsilon_i) &= \sigma^2\\
    cov(\epsilon_i, \epsilon_j) &= 0 \text{ for all } i \neq j
    \end{align}
    
- Quadratic Regression.

- Indicator variables, how they show up in a design matrix.

- Interaction Effects.

- The *error sum of squares* given predictors $X_1, X_2, \ldots, X_{p-1}$.
    $$
    SSE(X_1,X_2,\ldots,X_{p-1}) = \sum_{i=1}^n\left[Y_i - (\hat{\beta}_0 + \hat{\beta}_1X_{i1} + \hat{\beta}_2X_{i2} + \cdots + \hat{\beta}_{p-1}X_{i,p-1})\right]^2
    $$

- The **extra sum of squares**
    \begin{align}
    SSR(X_1|X_2) &= SSE(X_2) - SSE(X_1, X_2) = SSR(X_1, X_2) - SSR(X_2)\\
    SSR(X_2|X_1) &= SSE(X_1) - SSE(X_1, X_2) = SSR(X_1, X_2) - SSR(X_1)\\
    SSR(X_1, X_2|X_3) &= SSE(X_3) - SSE(X_1, X_2, X_3) = SSR(X_1, X_2, X_3) - SSR(X_3)\\
    \text{ etc...}
    \end{align}
 
- Decomposing sum of squares (with corresponding degrees of freedom), e.g. 
    ```{r,echo = FALSE,out.width="50%"}
    knitr::include_graphics(path = "./decomp.png")
    ```
    
- Type I versus Type II sums of squares.

- How the $F$-test can be used for different hypothesis tests.

- Adjusted coefficient of multiple determination:
    \begin{align}
    R^2_a = 1 - \left(\frac{n-1}{n-p}\right)\frac{SSE}{SSTO}.
    \end{align}
    
- Coefficients of partial determination
    \begin{align}
    R^2_{Y1|23} &= \frac{SSR(X_1|X_2, X_3)}{SSE(X_2, X_3)}\\
    R^2_{Y2|13} &= \frac{SSR(X_2|X_1, X_3)}{SSE(X_1, X_3)}\\
    R^2_{Y3|12} &= \frac{SSR(X_3|X_1, X_2)}{SSE(X_1, X_2)}\\
    R^2_{Y4|123} &= \frac{SSR(X_4|X_1, X_2, X_3)}{SSE(X_1, X_2, X_3)}\\
    &\text{etc...}
    \end{align}
    
- $Z$-score
    $$
    Z_i = \frac{X_i - \bar{X}}{s_x}
    $$
    
- AIC: Akaike's Information Criterion
    $$
    AIC = n\log\left(\frac{SSE}{n}\right) + 2p
    $$
    
- BIC: Bayesian information Criterion 
    $$
    BIC = n\log\left(\frac{SSE}{n}\right) + \log(n)p
    $$
    
- Mallows $C_p$:
    $$
    C_p = p + (n-p)\frac{\hat{\sigma}^2 - \hat{\sigma}^2_{full}}{\hat{\sigma}^2_{full}},
    $$
    - $\hat{\sigma}^2$ is the MSE for the model under consideration. $\hat{\sigma}^2_{full}$ is the MSE for the model with every predictor in it.

- The leverage value: $h_{ii}$ is the $i$th diagonal element of the hat matrix.

- The studentized residual
    $$
    r_i = \frac{e_i}{\sqrt{MSE(1 - h_{ii})}}
    $$
    - $e_i$ is the residual for individual $i$, and $h_{ii}$ is the $i$th diagonal element of the hat matrix.
    
- Cook's Distance: 
    $$
    D_i = \frac{\sum_{j=1}^n\left(\hat{Y}_j - \hat{Y}_{j(i)}\right)^2}{pMSE}
    $$
    - $\hat{Y}_{j(i)}$: The fit of observation $j$ when observation $i$ is not in the data.
    
- What are good values of leverage, studentized residuals, and cook's distance?
