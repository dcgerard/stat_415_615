---
title: "Equation Review"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
urlcolor: "blue"
---

```{r setup, include=FALSE}
set.seed(1)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.align  = "center",
                      fig.height = 3)
ggplot2::theme_set(ggplot2::theme_bw() + ggplot2::theme(strip.background = ggplot2::element_rect(fill = "white")))
```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy()
```

# Summary

Here is a list of the equations I want you to have memorized on the top of your head.

# List

- Sample Mean: $\bar{Y} = \frac{1}{n}\sum_{i=1}^nY_i$.

- Sample variance: $s_y = \frac{1}{n-1}\sum_{i=1}^n(Y_i - \bar{Y})^2$.

- SLR Model: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$, $E[\epsilon_i] = 0$, $var(\epsilon_i) = \sigma^2$, $cor(\epsilon_i, \epsilon_j) = 0$ for $i\neq j$.

- Normal SLR Model: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$, $\epsilon \overset{iid}{\sim} N(0, \sigma^2)$.

- OLS Objective: $\sum_{i=1}^n\left[Y_i - (\beta_0 + \beta_1X_i)\right]^2$

- SLR OLS estimates: $\hat{\beta}_0 = \bar{Y} - \hat{\beta}_1\bar{X}$, $\hat{\beta}_1 = cor(X, Y)\frac{sd(Y)}{sd(X)}$.

- Fitted values: $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$

- Residuals: $e_i = Y_i - \hat{Y}_i$.

- Properties of fitted regression line: (i) mean of residuals is 0, (ii) mean of observed values equals mean of fitted values, (iii) residuals are uncorrelated with predictors, (iv) residuals are uncorrelated with *fitted* values, and (v) regression line always goes through mean $(\bar{X}, \bar{Y})$.

- MSE: $MSE = \frac{1}{n-p}\sum_{i=1}^n\left[Y_i - \hat{Y}_i\right]^2$, where $p=2$ in SLR.
    - The MSE is the estimate of $\sigma^2$.
    
- $t$-statistic: $t^* = \frac{\hat{\beta}_1}{s(\hat{\beta}_1)}$, which follows a $t_{n-p}$ distribution under the null that $\beta_1 = 0$.

- Get two-sided $p$-value manually via $2 * pt(q = -abs(t^*), df = n - p)$, where $p=2$ in SLR.

- Confidence interval = estimate $\pm$ multiplier $\times$ standard error
    - Typically, multiplier = $qt(1 - \alpha/2, df)$, tell me what $\alpha$ and $df = n-p$ should be. Note, $p=2$ in SLR.
    - In prediction interval, $s^2(pred) = s^2(\hat{Y}_i) + MSE$.
    
- $SSE = \sum_{i=1}^n(Y_i - \hat{Y}_i)^2$, with $df$ of $n - p$

- $SSR = \sum_{i=1}^n(\hat{Y}_i - \bar{Y}_i)^2$, with $df$ of $p - 1$ 

- $SSTO = \sum_{i=1}^n(Y_i - \bar{Y}_i)^2$, with $df$ of $n-1$

- $SSTO = SSE + SSR$

- $F^* = \frac{[SSE(R) - SSE(F)]/[df_R - df_F]]}{SSE(F)/df_F}$ which follows a $F(dr_R - df_F, df_F)$ distribution under the null of the reduced model.
    - Tell me what $df_R$ and $df_F$ should be.

- $R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}$

- Interpretations:
    - If relationship is $y = \beta_0 + \beta_1x$, then add $c$ to $x$ means add $c\beta_1$ to $y$.
    - If relationship is $y = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means add $\beta_1\log(c)$ to $y$.
    - If relationship is $\log(y) = \beta_0 + \beta_1 x$, then add $c$ to $x$ means multiply $y$ by $\exp(c\beta_1)$.
    - If relationship is $\log(y) = \beta_0 + \beta_1 \log(x)$, then multiply $x$ by $c$ means multiply $y$ by $c^{\beta_1}$.
    - Make sure you describe interpretations in terms of the original variables (not saying "x" and "y").
    - Make sure you do not use causal language, implying any sort of "change" to the x variable.
    
- Bonferroni corrected $p$-values: unadjusted $p$-value $\times$ number of tests.

- Matrix stuff
    - For the midterm, I just want you to be able to give me the matrix form of the SLR model.
    - $x_{ij}$ is the $(i, j)$th element of the matrix $\mathbf{X}$.
    - Matrix transpose definition $\mathbf{X}^T$
    - Identity matrix, $\mathbf{I}_n$.
    - Matrix multiplication rules.
    - Properties of matrix inverse.
    - Matrix form of linear model: $\mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$
    - Matrix form of coefficient estimates: $\hat{\mathbf{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$
    - Hat matrix, $\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$, and why it is called the hat matrix.
