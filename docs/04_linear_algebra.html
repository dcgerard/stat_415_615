<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2022-10-06" />

<title>Linear Algebra</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Linear Algebra</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2022-10-06</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#learning-objectives" id="toc-learning-objectives">Learning
Objectives</a></li>
<li><a href="#vectors-and-matrices"
id="toc-vectors-and-matrices">Vectors and Matrices</a>
<ul>
<li><a href="#basics" id="toc-basics">Basics</a></li>
<li><a href="#matrix-inverse" id="toc-matrix-inverse">Matrix
Inverse</a></li>
<li><a href="#matrix-rank" id="toc-matrix-rank">Matrix Rank</a></li>
<li><a href="#basic-results-for-matrices"
id="toc-basic-results-for-matrices">Basic Results for Matrices</a></li>
</ul></li>
<li><a href="#linear-algebra-and-simple-linear-regression"
id="toc-linear-algebra-and-simple-linear-regression">Linear Algebra and
Simple Linear Regression</a>
<ul>
<li><a href="#model-representation" id="toc-model-representation">Model
Representation</a></li>
<li><a href="#random-vectors" id="toc-random-vectors">Random
Vectors</a></li>
<li><a href="#full-expression-of-simple-linear-model"
id="toc-full-expression-of-simple-linear-model">Full Expression of
Simple Linear Model</a></li>
<li><a href="#some-standard-matrix-multiplications"
id="toc-some-standard-matrix-multiplications">Some standard matrix
multiplications</a></li>
<li><a href="#estimates-representation"
id="toc-estimates-representation">Estimates Representation</a></li>
<li><a href="#inference-of-regression-coefficients"
id="toc-inference-of-regression-coefficients">Inference of regression
coefficients</a></li>
</ul></li>
<li><a href="#linear-algebra-in-r" id="toc-linear-algebra-in-r">Linear
Algebra in R</a></li>
</ul>
</div>

<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Introduce/review basic linear algebra.</li>
<li>Chapter 5 of KNNL</li>
<li>Chapters 2 and 3 of <a
href="https://shainarace.github.io/LinearAlgebra/">Linear Algebra for
Data Science with examples in R</a></li>
</ul>
</div>
<div id="vectors-and-matrices" class="section level1">
<h1>Vectors and Matrices</h1>
<div id="basics" class="section level2">
<h2>Basics</h2>
<ul>
<li><p>A <strong>vector</strong> is a list of numbers.</p>
<p><span class="math display">\[
  \mathbf{x} =
  \left(
  \begin{array}{c}
  10\\
  3\\
  5
  \end{array}
  \right)
  \]</span></p></li>
<li><p>In the above vector, the first element is 10, the second is 3,
and the third is 5. Or, using subscripts to denote indices, we have
<span class="math display">\[
  x_1 = 10,~x_2 = 3, \text{ and } x_3 = 5.
  \]</span></p></li>
<li><p>Vectors are ordered, so <span class="math display">\[
  \left(
  \begin{array}{c}
  10\\
  3\\
  5
  \end{array}
  \right)
  \neq
  \left(
  \begin{array}{c}
  3\\
  10\\
  5
  \end{array}
  \right).
  \]</span></p></li>
<li><p>I will generally use bold, lower-case letters to represent
vectors.</p></li>
<li><p>A <strong>matrix</strong> is a rectangular array of numbers.</p>
<p><span class="math display">\[
  \mathbf{A} =
  \left(
  \begin{array}{cc}
  -1 &amp; 4\\
  11 &amp; -4\\
  2 &amp; 8
  \end{array}
  \right)
  \]</span></p></li>
<li><p>The <strong>dimension</strong> of a matrix is the number of rows
and columns of that matrix. So the dimension of the above matrix is
<span class="math inline">\(3\times 2\)</span> since it has three rows
and two columns.</p></li>
<li><p>We index elements by the rows first, then the columns second. For
example, in the above matrix, the <span class="math inline">\((1,
1)\)</span>th element is -1, the <span class="math inline">\((1,
2)\)</span>th element is 4, the <span class="math inline">\((2,
1)\)</span>th element is 11, the <span
class="math inline">\((2,2)\)</span>th element is -4, the <span
class="math inline">\((3, 1)\)</span>th element is 2, and the <span
class="math inline">\((3,2)\)</span>th element is 8. Or, using
subscripts as indices we have <span class="math display">\[
  a_{11} = -1,~a_{12} = 4,~a_{21}=11,~a_{22}=-4,~a_{31} = 2,\text{ and }
a_{32}=8.
  \]</span></p></li>
<li><p>I will generally use bold, upper-case letters to represent
matrices.</p></li>
<li><p>A vector is just a matrix with one column.</p></li>
<li><p>The <strong>transpose</strong> of a matrix/vector swaps the row
and column indices.</p>
<p><span class="math display">\[\begin{align}
  \mathbf{x}^T &amp;= (10, 3, 5)\\
  \mathbf{A}^T &amp;=
  \left(
  \begin{array}{ccc}
  -1 &amp; 11 &amp; 2\\
  4 &amp; -4 &amp; 8
  \end{array}
  \right)
  \end{align}\]</span></p>
<ul>
<li>First column of <span class="math inline">\(\mathbf{A}\)</span> is
the first row of <span class="math inline">\(\mathbf{A}^T\)</span>,
second column of <span class="math inline">\(\mathbf{A}\)</span> is the
second row of <span class="math inline">\(\mathbf{A}^T\)</span>.</li>
</ul></li>
<li><p>You can add two matrices together <em>as long as they have the
same dimensions</em>. You just sum up the values element-wise:</p>
<p><span class="math display">\[
  \left(
  \begin{array}{cc}
  1 &amp; 2 \\
  3 &amp; 4
  \end{array}
  \right)
  +
  \left(
  \begin{array}{cc}
  5 &amp; 6 \\
  7 &amp; 8
  \end{array}
  \right)
  =
  \left(
  \begin{array}{cc}
  6 &amp; 8 \\
  10 &amp; 12
  \end{array}
  \right)
  \]</span></p></li>
<li><p>You can multiply a matrix by a scalar, where you multiply each
element of that matrix by that scalar. <span class="math display">\[
  2\mathbf{A} = 2
  \left(
  \begin{array}{cc}
  -1 &amp; 4\\
  11 &amp; -4\\
  2 &amp; 8
  \end{array}
  \right)
  =
  \left(
  \begin{array}{cc}
  2 \times -1 &amp; 2 \times 4\\
  2 \times 11 &amp; 2 \times -4\\
  2 \times 2 &amp; 2 \times8
  \end{array}
  \right)
  =
  \left(
  \begin{array}{cc}
  -2 &amp; 8\\
  22 &amp; -8\\
  4 &amp; 16
  \end{array}
  \right)
  \]</span></p></li>
<li><p>You can multiply two matrices together as long as the left matrix
has the same number of columns as the number or rows of the right
matrix. The rules for matrix multiplication are kinda weird (but they
are justified if you ever take linear algebra). In effect, you multiply
the <em>row</em> of the left matrix with the <em>column</em> of the
right matrix, and add up the resulting multiplied numbers:</p>
<p><img src="04_figs/matmult.gif" /> </p></li>
<li><p>An awesome visualization from <a
href="https://github.com/shainarace">Shaina Race</a> does a better job
of illustrating this:</p>
<p><img src="04_figs/multlincombanim.gif" /> </p></li>
<li><p>The special case of a matrix multiplied by a vector is below
(also from Shaina Race):</p>
<p><img src="04_figs/animmatrixvectormult.gif" /> </p></li>
<li><p><strong>Exercise</strong>: Let <span
class="math display">\[\begin{align}
  \mathbf{X} &amp;=
  \left(
  \begin{array}{cc}
  3 &amp; -1\\
  1 &amp; -2
  \end{array}
  \right)\\
  \mathbf{Y} &amp;=
  \left(
  \begin{array}{cc}
  1 &amp; 2\\
  2 &amp; 1
  \end{array}
  \right)
  \end{align}\]</span> Calculate <span
class="math inline">\(\mathbf{X}^T\mathbf{Y}\)</span>.</p></li>
<li><p>Note that matrix multiplication is generally <em>not</em>
commutative. That is <span class="math display">\[
  \mathbf{A}\mathbf{B} \neq \mathbf{B}\mathbf{A}
  \]</span> Indeed, even if <span
class="math inline">\(\mathbf{A}\mathbf{B}\)</span> exists does not mean
that <span class="math inline">\(\mathbf{B}\mathbf{A}\)</span> exists.
E.g. Suppose <span class="math inline">\(\mathbf{A}\)</span> is <span
class="math inline">\(2 \times 3\)</span> and <span
class="math inline">\(\mathbf{B}\)</span> is <span
class="math inline">\(3 \times 4\)</span>.</p></li>
<li><p>The <strong>identity matrix</strong> is the diagonal matrix with
1’s in the diagonal. It is the matrix analogue to the number 1. For
example, the <span class="math inline">\(3 \times 3\)</span> diagonal
matrix is</p>
<p><span class="math display">\[
  \mathbf{I}_3 = \left(
  \begin{array}{ccc}
  1 &amp; 0 &amp; 0 \\
  0 &amp; 1 &amp; 0 \\
  0 &amp; 0 &amp; 1
  \end{array}
  \right)
  \]</span></p></li>
<li><p>For any matrix <span class="math inline">\(\mathbf{A}\)</span> of
dimension <span class="math inline">\(n \times p\)</span>, we have <span
class="math display">\[
  \mathbf{I}_n\mathbf{A} = \mathbf{A}\mathbf{I}_p = \mathbf{A}
  \]</span></p></li>
<li><p><strong>Exercise</strong>: Verify that <span
class="math display">\[
  \left(
  \begin{array}{cc}
  1 &amp; 0 \\
  0 &amp; 1
  \end{array}
  \right)
  \left(
  \begin{array}{cc}
  a &amp; b \\
  c &amp; d
  \end{array}
  \right)
  =
  \left(
  \begin{array}{cc}
  a &amp; b \\
  c &amp; d
  \end{array}
  \right)
  \left(
  \begin{array}{cc}
  1 &amp; 0 \\
  0 &amp; 1
  \end{array}
  \right)
  =
  \left(
  \begin{array}{cc}
  a &amp; b \\
  c &amp; d
  \end{array}
  \right)
  \]</span></p></li>
<li><p>The <span class="math inline">\(n\)</span>-vector of all <span
class="math inline">\(1\)</span>’s is usually denoted <span
class="math inline">\(\mathbf{1}_n\)</span> <span
class="math display">\[
  \mathbf{1}_n =
  \left(
  \begin{array}{c}
  1 \\
  1 \\
  \vdots\\
  1
  \end{array}
  \right)
  \]</span></p></li>
<li><p>The <span class="math inline">\(n\times p\)</span> matrix of all
<span class="math inline">\(1\)</span>’s is usually denoted <span
class="math inline">\(\mathbf{J}_{n\times p}\)</span>. <span
class="math display">\[
  \mathbf{J}_{n\times p} =
  \left(
  \begin{array}{ccc}
  1 &amp; \cdots &amp; 1\\
  \vdots &amp; \ddots &amp; \vdots\\
  1 &amp; \cdots &amp; 1
  \end{array}
  \right)
  \]</span></p></li>
<li><p>The <span class="math inline">\(n\)</span>-vector of all <span
class="math inline">\(0\)</span>’s is usually denoted <span
class="math inline">\(\mathbf{0}_n\)</span>. <span
class="math display">\[
  \mathbf{0}_n =
  \left(
  \begin{array}{c}
  0 \\
  0 \\
  \vdots\\
  0
  \end{array}
  \right)
  \]</span></p></li>
<li><p><strong>Exercise</strong>: What is <span
class="math inline">\(\mathbf{1}_n\mathbf{1}_n^T\)</span>? What is <span
class="math inline">\(\mathbf{1}_n^T\mathbf{1}_n\)</span>?</p></li>
</ul>
</div>
<div id="matrix-inverse" class="section level2">
<h2>Matrix Inverse</h2>
<ul>
<li><p>The inverse of a scalar is its reciprocal. E.g. the inverse of 5
is <span class="math inline">\(\frac{1}{5}\)</span>. A number multiplied
by its inverse is 1 <span class="math display">\[
  5 \times \frac{1}{5} = 1
  \]</span></p></li>
<li><p>There is a concept of an inverse in linear algebra as well. Wheen
you multiply a matrix by its inverse, you obtain the identity matrix
<span class="math inline">\(\mathbf{I}\)</span>.</p></li>
<li><p>Only square matrices (the number of rows equals the number of
columns) may have inverses.</p></li>
<li><p>Not all square matrices have an inverse.</p></li>
<li><p>When a square matrix has an inverse, it is unique.</p></li>
<li><p>If <span class="math inline">\(\mathbf{A}\)</span> is an <span
class="math inline">\(r \times r\)</span> square matrix, then we denote
the inverse of <span class="math inline">\(\mathbf{A}\)</span> as <span
class="math inline">\(\mathbf{A}^{-1}\)</span>. It is the unique <span
class="math inline">\(r \times r\)</span> matrix such that <span
class="math display">\[
  \mathbf{A}\mathbf{A}^{-1} = \mathbf{A}^{-1}\mathbf{A} = \mathbf{I}_r
  \]</span></p></li>
<li><p><strong>Example</strong>: The inverse of <span
class="math display">\[
  \mathbf{A} =
  \left(
  \begin{array}{cc}
  2 &amp; 4\\
  3 &amp; 1
  \end{array}
  \right)
  \]</span> is <span class="math display">\[
  \mathbf{A}^{-1} =
  \left(
  \begin{array}{cc}
  -0.1 &amp; 0.4\\
  0.3 &amp; -0.2
  \end{array}
  \right)
  \]</span></p></li>
<li><p>The formula for an inverse matrix is complicated, and not
something you need to know for regression (just use a
computer).</p></li>
<li><p>Inverse matrices allow us to solve systems of equations. Suppose
we have <span class="math display">\[
  \mathbf{A}\mathbf{Y} = \mathbf{C}
  \]</span> and we want to solve for <span
class="math inline">\(\mathbf{Y}\)</span>. Then we can
<em>pre</em>multiply (multiply on the left) both the left and right
equations by <span class="math inline">\(\mathbf{A}^{-1}\)</span> to
obtain <span class="math display">\[\begin{align}
  \mathbf{A}^{-1}\mathbf{A}\mathbf{Y} &amp;= \mathbf{A}^{-1}\mathbf{C}\\
  \Rightarrow \mathbf{I}\mathbf{Y} &amp;= \mathbf{A}^{-1}\mathbf{C}\\
  \Rightarrow \mathbf{Y} &amp;= \mathbf{A}^{-1}\mathbf{C}.
  \end{align}\]</span></p></li>
</ul>
</div>
<div id="matrix-rank" class="section level2">
<h2>Matrix Rank</h2>
<ul>
<li><p>The matrix inverse does not exist if there is redundant
information between the columns in the matrix.</p></li>
<li><p>Formally, vectors <span
class="math inline">\(\mathbf{a}_1,\mathbf{a}_2,\ldots,\mathbf{a}_p\)</span>
are <strong>linearly dependent</strong> if there exist scalars <span
class="math inline">\(k_1, k_2,\ldots, k_p\)</span> (not all zero) such
that <span class="math display">\[
  k_1\mathbf{a}_1 + k_2\mathbf{a}_2 + \cdots k_p \mathbf{a}_p =
\mathbf{0}
  \]</span></p></li>
<li><p>I.e., you can write one vector in terms of the other
vectors.</p></li>
<li><p><strong>Example</strong>: Let <span
class="math display">\[\begin{align}
  \mathbf{a}_1 &amp;=
  \left(
  \begin{array}{c}
  1\\
  2\\
  3
  \end{array}
  \right)\\
  \mathbf{a}_2 &amp;=
  \left(
  \begin{array}{c}
  -1\\
  3\\
  2
  \end{array}
  \right)\\
  \mathbf{a}_3 &amp;=
  \left(
  \begin{array}{c}
  -3\\
  -1\\
  -4
  \end{array}
  \right)
  \end{align}\]</span> Then <span class="math inline">\(2\mathbf{a}_1 -
\mathbf{a}_2 + \mathbf{a}_3 = \mathbf{0}_3\)</span>.</p></li>
<li><p>Vectors are <strong>linearly independent</strong> if there do not
exist such constants (so each vector provides some new
information).</p></li>
<li><p>The <strong>rank</strong> of the matrix is the maximum number of
linearly independent columns in the matrix. (we treat each column as a
vector).</p></li>
<li><p><strong>Example</strong>: The rank of the following matrix is 2:
<span class="math display">\[
  \mathbf{A} =
  \left(
  \begin{array}{ccc}
  1 &amp; -1 &amp; -3\\
  2 &amp; 3 &amp; -1\\
  3 &amp; 2 &amp; -4
  \end{array}
  \right)
  \]</span></p></li>
<li><p><em>The inverse of an <span class="math inline">\(r \times
r\)</span> square matrix <span class="math inline">\(\mathbf{A}\)</span>
exists if and only if the rank of <span
class="math inline">\(\mathbf{A}\)</span> is <span
class="math inline">\(r\)</span>.</em></p></li>
<li><p>The maximum possible rank of an <span class="math inline">\(n
\times p\)</span> matrix is <span class="math inline">\(\min(n,
p)\)</span>.</p></li>
<li><p>If <span class="math inline">\(\mathbf{C} =
\mathbf{A}\mathbf{B}\)</span>, then the rank of <span
class="math inline">\(\mathbf{C}\)</span> is less than or equal to <span
class="math inline">\(\min(\text{rank }\mathbf{A}, \text{rank
}\mathbf{B})\)</span>.</p></li>
<li><p>This will show up when we put the covariates in an <span
class="math inline">\(n \times p\)</span> (<span
class="math inline">\(n\)</span> individuals and <span
class="math inline">\(p\)</span> variables) matrix <span
class="math inline">\(\mathbf{X}\)</span> and need to calculate the
inverse of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span>,
which is only possible if <span class="math inline">\(n \geq
p\)</span>.</p></li>
</ul>
</div>
<div id="basic-results-for-matrices" class="section level2">
<h2>Basic Results for Matrices</h2>
<ul>
<li><span class="math inline">\(\mathbf{A} + \mathbf{B} = \mathbf{B} +
\mathbf{A}\)</span></li>
<li><span class="math inline">\((\mathbf{A} + \mathbf{B}) + \mathbf{C} =
\mathbf{A} + (\mathbf{B} + \mathbf{C})\)</span></li>
<li><span class="math inline">\((\mathbf{A}\mathbf{B})\mathbf{C} =
\mathbf{A}(\mathbf{B}\mathbf{C})\)</span></li>
<li><span class="math inline">\(\mathbf{C}(\mathbf{A} + \mathbf{B}) =
\mathbf{C}\mathbf{A} + \mathbf{C}\mathbf{B}\)</span></li>
<li><span class="math inline">\(k(\mathbf{A} + \mathbf{B}) = k\mathbf{A}
+ k\mathbf{B}\)</span></li>
<li><span class="math inline">\((\mathbf{A}^T)^T =
\mathbf{A}\)</span></li>
<li><span class="math inline">\((\mathbf{A} + \mathbf{B})^T =
\mathbf{A}^T + \mathbf{B}^T\)</span></li>
<li><span class="math inline">\((\mathbf{A}\mathbf{B})^T =
\mathbf{B}^T\mathbf{A}^T\)</span></li>
<li><span class="math inline">\((\mathbf{A}\mathbf{B}\mathbf{C})^T =
\mathbf{C}^T\mathbf{B}^T\mathbf{A}^T\)</span></li>
<li><span class="math inline">\((\mathbf{A}\mathbf{B})^{-1} =
\mathbf{B}^{-1}\mathbf{A}^{-1}\)</span></li>
<li><span class="math inline">\((\mathbf{A}\mathbf{B}\mathbf{C})^{-1} =
\mathbf{C}^{-1}\mathbf{B}^{-1}\mathbf{A}^{-1}\)</span></li>
<li><span class="math inline">\((\mathbf{A}^{-1})^{-1} =
\mathbf{A}\)</span></li>
<li><span class="math inline">\((\mathbf{A}^T)^{-1} =
(\mathbf{A}^{-1})^T\)</span></li>
</ul>
</div>
</div>
<div id="linear-algebra-and-simple-linear-regression"
class="section level1">
<h1>Linear Algebra and Simple Linear Regression</h1>
<ul>
<li>Why am I torturing you with this?
<ul>
<li>Linear algebra allows us to concisely represent linear regression
models/estimates/procedures in terms of matrices and vectors.</li>
<li>Without linear algebra, such representations in multiple linear
regression would be verbose, to say the least.</li>
</ul></li>
</ul>
<div id="model-representation" class="section level2">
<h2>Model Representation</h2>
<ul>
<li><p>We can represent the simple linear regression model by arranging
values in matrices.</p></li>
<li><p>Let <span class="math display">\[\begin{align}
  \mathbf{y} &amp;=
  \left(
  \begin{array}{c}
  Y_1\\
  Y_2\\
  \vdots\\
  Y_n
  \end{array}
  \right),\\
  \mathbf{X} &amp;=
  \left(
  \begin{array}{cc}
  1 &amp; X_1\\
  1 &amp; X_2\\
  \vdots &amp; \vdots\\
  1 &amp; X_n
  \end{array}
  \right),\\
  \mathbf{\beta} &amp;=
  \left(
  \begin{array}{c}
  \beta_0\\
  \beta_1
  \end{array}
  \right),\\
  \mathbf{\epsilon} &amp;=
  \left(
  \begin{array}{c}
  \epsilon_1\\
  \epsilon_2\\
  \vdots\\
  \epsilon_n
  \end{array}
  \right)
  \end{align}\]</span></p></li>
<li><p>Then we can represent the simple linear regression model in terms
of these matrices <span class="math display">\[
  \mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}
  \]</span> [demonstrate on white board]</p></li>
<li><p>Using expectation notation, we can say that <span
class="math display">\[
  E[\mathbf{y}] =
  \left(
  \begin{array}{c}
  E[Y_1]\\
  E[Y_2]\\
  \vdots\\
  E[Y_n]
  \end{array}
  \right)
  = \mathbf{X}\mathbf{\beta}
  \]</span></p></li>
<li><p>The column of <span class="math inline">\(1\)</span>’s in <span
class="math inline">\(\mathbf{X}\)</span> can be seen of defining a
variable <span class="math inline">\(X_0 = 1\)</span> and then having
<span class="math display">\[
  Y_i = \beta_0 X_0 + \beta_1 X_i + \epsilon_i
  \]</span></p></li>
</ul>
</div>
<div id="random-vectors" class="section level2">
<h2>Random Vectors</h2>
<ul>
<li><p>A <strong>random vector</strong> is a vector whose elements are
random variables. So <span
class="math inline">\(\mathbf{\epsilon}\)</span> is a random
vector.</p></li>
<li><p>The expected value of a random vector is a vector whose elements
are the expected values of the elements of the random vector. So, for
example <span class="math display">\[
E[\mathbf{\epsilon}] =
E\left[
\left(
\begin{array}{c}
\epsilon_1\\
\epsilon_2\\
\vdots\\
\epsilon_n
\end{array}
\right)
\right]
=
\left(
\begin{array}{c}
E[\epsilon_1]\\
E[\epsilon_2]\\
\vdots\\
E[\epsilon_n]
\end{array}
\right)
\]</span></p></li>
<li><p>We know, from the assumptions of the linear model, that <span
class="math inline">\(E[\epsilon_i] = 0\)</span> for all <span
class="math inline">\(i\)</span>, and so <span
class="math inline">\(E[\mathbf{\epsilon}] =
\mathbf{0}_n\)</span>.</p></li>
<li><p>We can concisely express the variances and covariances between
all elements of a random vector in terms of the
<strong>variance-covariance matrix</strong> (often just called the
<strong>covariance matrix</strong>). <span class="math display">\[
  cov(\mathbf{\epsilon}) =
  \left(
  \begin{array}{cccc}
  var(\epsilon_1) &amp; cov(\epsilon_1, \epsilon_2) &amp; \cdots &amp;
cov(\epsilon_1, \epsilon_n)\\
  cov(\epsilon_2, \epsilon_1) &amp; var(\epsilon_2) &amp; \cdots &amp;
cov(\epsilon_2, \epsilon_n)\\
  \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
  cov(\epsilon_n, \epsilon_1) &amp; cov(\epsilon_n, \epsilon_2) &amp;
\cdots &amp; var(\epsilon_n)
  \end{array}
  \right)
  \]</span></p></li>
<li><p>Since we know, from the assumptions of the linear model, that
<span class="math inline">\(var(\epsilon_i) = \sigma^2\)</span> for all
<span class="math inline">\(i\)</span> (equal variance assumption) and
<span class="math inline">\(cov(\epsilon_i, \epsilon_j) = 0\)</span> for
all <span class="math inline">\(i \neq j\)</span> (independence
assumption), we have that <span class="math display">\[
  cov(\mathbf{\epsilon}_i) = \sigma^2\mathbf{I}_n
  \]</span></p></li>
<li><p>We frequently encounter a random vector <span
class="math inline">\(\mathbf{w}\)</span> that is obtained by
premultiplying a random vector <span
class="math inline">\(\mathbf{y}\)</span> by a matrix <span
class="math inline">\(\mathbf{A}\)</span>. <span class="math display">\[
  \mathbf{w} = \mathbf{A}\mathbf{y}
  \]</span></p></li>
<li><p>Then <span class="math display">\[
  cov(\mathbf{w}) = \mathbf{A}cov(\mathbf{y})\mathbf{A}^T
  \]</span></p></li>
<li><p>Let’s calculate together the mean and covariance of the random
vector <span class="math inline">\(\mathbf{y}\)</span> [on the white
board].</p></li>
</ul>
</div>
<div id="full-expression-of-simple-linear-model" class="section level2">
<h2>Full Expression of Simple Linear Model</h2>
<ul>
<li><p>The simple linear model can be fully expressed with assumptions
via <span class="math display">\[\begin{align}
  \mathbf{y} &amp;= \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\\
  E[\mathbf{\epsilon}] &amp;= \mathbf{0}_n\\
  cov(\mathbf{\epsilon}) &amp;= \sigma^2\mathbf{I}_n
  \end{align}\]</span></p></li>
<li><p>For the normal linear model (important only for prediction
intervals), you further assume that <span
class="math inline">\(\mathbf{\epsilon}\)</span> is distributed
according to the <a
href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">multivariate
normal distribution</a>.</p></li>
</ul>
</div>
<div id="some-standard-matrix-multiplications" class="section level2">
<h2>Some standard matrix multiplications</h2>
<p><span class="math display">\[
\mathbf{Y}^T\mathbf{Y} = \sum_{i=1}^nY_i^2
\]</span></p>
<p><span class="math display">\[
\mathbf{X}^T\mathbf{X} =
\left(
\begin{array}{cc}
n &amp; \sum_{i=1}^nX_i\\
\sum_{i=1}^nX_i &amp; \sum_{i=1}^nX_i^2
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{X}^T\mathbf{Y} =
\left(
\begin{array}{c}
\sum_{i=1}^n Y_i\\
\sum_{i=1}^n X_iY_i\\
\end{array}
\right)
\]</span></p>
</div>
<div id="estimates-representation" class="section level2">
<h2>Estimates Representation</h2>
<ul>
<li><p>You can show that the estimated regression coefficients are <span
class="math display">\[
  \hat{\mathbf{\beta}} =
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
  \]</span></p></li>
<li><p>The fitted values are then <span class="math display">\[
  \hat{\mathbf{Y}} = \mathbf{X}\hat{\mathbf{\beta}} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
  \]</span></p></li>
<li><p>The matrix <span class="math display">\[
  \mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T
  \]</span> transforms <span class="math inline">\(\mathbf{Y}\)</span>
to <span class="math inline">\(\hat{\mathbf{Y}}\)</span> through
premultiplication, and so is often called the <strong>hat
matrix</strong>.</p></li>
<li><p>The hat matrix plays an important part in diagnostics (Chapter
10).</p></li>
<li><p><strong>Exercise</strong>: Prove that the hat matrix is
<strong>idempotent</strong>. That is <span class="math display">\[
  \mathbf{H}\mathbf{H} = \mathbf{H}
  \]</span></p></li>
<li><p>The residuals are <span class="math display">\[
  \mathbf{e} = \mathbf{Y} - \hat{\mathbf{Y}} = \mathbf{Y} -
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} =
(\mathbf{I}_n -
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T)\mathbf{Y} =
(\mathbf{I}_n - \mathbf{H})\mathbf{Y}.
  \]</span></p></li>
<li><p><strong>Exercise</strong>: What is the covariance matrix of the
residuals? Are the residuals uncorrelated?</p></li>
</ul>
</div>
<div id="inference-of-regression-coefficients" class="section level2">
<h2>Inference of regression coefficients</h2>
<ul>
<li><p>To obtain the standard errors of <span
class="math inline">\(\hat{\beta}_0\)</span> and <span
class="math inline">\(\hat{\beta}_1\)</span>, we can just obtain the
covariance matrix of <span
class="math inline">\(\hat{\mathbf{\beta}}\)</span>. <span
class="math display">\[\begin{align}
  cov(\hat{\mathbf{\beta}}) &amp;=
cov((\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y})\\
  &amp;=
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Tcov(\mathbf{Y})\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\
  &amp;=
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\sigma^2\mathbf{I}_n\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\
  &amp;=
\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\
  &amp;= \sigma^2(\mathbf{X}^T\mathbf{X})^{-1}.
  \end{align}\]</span></p></li>
<li><p>We estimate this variance covariance matrix with <span
class="math display">\[
  s^2(\hat{\mathbf{\beta}}) = MSE(\mathbf{X}^T\mathbf{X})^{-1}
  \]</span></p></li>
<li><p>Similarly concise equations for standard errors exist for mean
responses (<span class="math inline">\(s^2(\hat{Y}_h)\)</span>) and
predictions (<span class="math inline">\(s^2\{pred\}\)</span>).</p></li>
</ul>
</div>
</div>
<div id="linear-algebra-in-r" class="section level1">
<h1>Linear Algebra in R</h1>
<ul>
<li><p>You create matrices with the <code>matrix()</code> function.</p>
<pre class="r"><code>A &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 3, ncol = 2)
A</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    4
## [2,]    2    5
## [3,]    3    6</code></pre></li>
<li><p>Notice that the first column is populated first, then the second
column, etc…</p></li>
<li><p>You can add/subtract/multiply/divide using a scalar with basic
arithmetic operations.</p>
<pre class="r"><code>2 + A</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    3    6
## [2,]    4    7
## [3,]    5    8</code></pre>
<pre class="r"><code>2 - A</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1   -2
## [2,]    0   -3
## [3,]   -1   -4</code></pre>
<pre class="r"><code>A - 2</code></pre>
<pre><code>##      [,1] [,2]
## [1,]   -1    2
## [2,]    0    3
## [3,]    1    4</code></pre>
<pre class="r"><code>2 * A</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    2    8
## [2,]    4   10
## [3,]    6   12</code></pre>
<pre class="r"><code>A / 2</code></pre>
<pre><code>##      [,1] [,2]
## [1,]  0.5  2.0
## [2,]  1.0  2.5
## [3,]  1.5  3.0</code></pre>
<pre class="r"><code>2 / A</code></pre>
<pre><code>##        [,1]   [,2]
## [1,] 2.0000 0.5000
## [2,] 1.0000 0.4000
## [3,] 0.6667 0.3333</code></pre></li>
<li><p>Matrices can be element-wise added/sabtracted/multiplied/divided
using <code>+</code>/<code>-</code>/<code>*</code>/<code>/</code> as
long as these matrices are the same dimension.</p>
<pre class="r"><code>B &lt;- matrix(c(7, 8, 9, 10, 11, 12), nrow = 3, ncol = 2)
B</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    7   10
## [2,]    8   11
## [3,]    9   12</code></pre>
<pre class="r"><code>A + B</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    8   14
## [2,]   10   16
## [3,]   12   18</code></pre>
<pre class="r"><code>A - B</code></pre>
<pre><code>##      [,1] [,2]
## [1,]   -6   -6
## [2,]   -6   -6
## [3,]   -6   -6</code></pre>
<pre class="r"><code>A * B</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    7   40
## [2,]   16   55
## [3,]   27   72</code></pre>
<pre class="r"><code>A / B</code></pre>
<pre><code>##        [,1]   [,2]
## [1,] 0.1429 0.4000
## [2,] 0.2500 0.4545
## [3,] 0.3333 0.5000</code></pre></li>
<li><p>Matrix multiplication is performed using <code>%*%</code>. The
number of columns of the left matrix must equal the number of rows of
the right matrix.</p>
<pre class="r"><code>C &lt;- matrix(c(-1, 2, 1, 3), nrow = 2, ncol = 2)
C</code></pre>
<pre><code>##      [,1] [,2]
## [1,]   -1    1
## [2,]    2    3</code></pre>
<pre class="r"><code>A %*% C</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    7   13
## [2,]    8   17
## [3,]    9   21</code></pre></li>
<li><p>The transpose of a matrix is calculated using
<code>t()</code>.</p>
<pre class="r"><code>t(A)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6</code></pre></li>
<li><p>The inverse of a matrix is calculated using <code>solve()</code>
(remember, has to be a square matrix).</p>
<pre class="r"><code>solve(C)</code></pre>
<pre><code>##      [,1] [,2]
## [1,] -0.6  0.2
## [2,]  0.4  0.2</code></pre></li>
<li><p>You can create the identity matrix using <code>diag()</code></p>
<pre class="r"><code>diag(x = 3)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    0    1    0
## [3,]    0    0    1</code></pre></li>
<li><p>You can create a matrix of <code>1</code>’s by just giving
<code>matrix()</code> the number 1.</p>
<pre class="r"><code>matrix(1, nrow = 2, ncol = 3)</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    1    1
## [2,]    1    1    1</code></pre></li>
<li><p><strong>Exercise</strong>: Suppose we have a response vector
<span class="math inline">\(\mathbf{y}\)</span> and a design matrix
<span class="math inline">\(\mathbf{X}\)</span> such that <span
class="math display">\[
  \mathbf{y} =
  \left(
  \begin{array}{c}
  7.8\\
  9.0\\
  10.2\\
  11.0\\
  11.7
  \end{array}
  \right)\\
  \mathbf{X} =
  \left(
  \begin{array}{cc}
  1 &amp; 8\\
  1 &amp; 4\\
  1 &amp; 0\\
  1 &amp; -4\\
  1 &amp; -8
  \end{array}
  \right)
  \]</span></p>
<p>Use matrix methods to derive the estimated regression coefficients of
a regression of <span class="math inline">\(\mathbf{y}\)</span> on <span
class="math inline">\(\mathbf{X}\)</span>. Verify these estimates using
<code>lm()</code>.</p></li>
</ul>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
