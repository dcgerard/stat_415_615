<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2021-11-08" />

<title>MLR III: Special Predictors</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">MLR III: Special Predictors</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2021-11-08</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Incorporating/interpreting quadratic terms.</li>
<li>Incorporating/interpreting categorical variables (through indicators).</li>
<li>Incorporating/interpreting interaction effects.</li>
<li>Chapter 8 of KNNL.</li>
</ul>
</div>
<div id="quadratic-terms" class="section level1">
<h1>Quadratic Terms</h1>
<ul>
<li><p>Consider the Muscle Mass data which explores the association between age and muscle mass. You can read about it <a href="https://dcgerard.github.io/stat_415_615/data.html#Muscle_Mass">here</a>.</p>
<pre class="r"><code>library(tidyverse)
library(broom)
muscle &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/muscle.csv&quot;)
glimpse(muscle)</code></pre>
<pre><code>## Rows: 60
## Columns: 2
## $ mass &lt;dbl&gt; 106, 106, 97, 113, 96, 119, 92, 112, 92, 102, 107, 107, 102, 115,…
## $ age  &lt;dbl&gt; 43, 41, 47, 46, 45, 41, 47, 41, 48, 48, 42, 47, 43, 44, 42, 55, 5…</code></pre></li>
<li><p>These data look like a quadratic fit could help</p>
<pre class="r"><code>qplot(x = age, y = mass, data = muscle) +
  geom_smooth(se = FALSE)</code></pre>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-2-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>lm_musc &lt;- lm(mass ~ age, data = muscle)
a_musc &lt;- augment(lm_musc)
qplot(x = .fitted, y = .resid, data = a_musc) +
  geom_hline(yintercept = 0, lty = 2, col = 2)</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-2-2.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>A quadratic regression model with one predictor variable is <span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + \beta_{2}X_{i}^2 + \epsilon_i
  \]</span> with the usual assumptions on the errors.</p></li>
<li><p>We fit this in R by first creating a new variable, say <code>age2</code>, with contains <code>age</code> squared.</p>
<pre class="r"><code>muscle &lt;- mutate(muscle, age2 = age^2)
glimpse(muscle)</code></pre>
<pre><code>## Rows: 60
## Columns: 3
## $ mass &lt;dbl&gt; 106, 106, 97, 113, 96, 119, 92, 112, 92, 102, 107, 107, 102, 115,…
## $ age  &lt;dbl&gt; 43, 41, 47, 46, 45, 41, 47, 41, 48, 48, 42, 47, 43, 44, 42, 55, 5…
## $ age2 &lt;dbl&gt; 1849, 1681, 2209, 2116, 2025, 1681, 2209, 1681, 2304, 2304, 1764,…</code></pre></li>
<li><p>We then fit a multiple linear regression model using <code>age</code> and <code>age2</code> as predictors.</p>
<pre class="r"><code>lm_musc2 &lt;- lm(mass ~ age + age2, data = muscle)
tidy(lm_musc2)</code></pre>
<pre><code>## # A tibble: 3 × 5
##   term        estimate std.error statistic       p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;
## 1 (Intercept) 207.      29.2          7.09 0.00000000221
## 2 age          -2.96     1.00        -2.96 0.00453      
## 3 age2          0.0148   0.00836      1.78 0.0811</code></pre></li>
<li><p>The estimated regression surface is <span class="math display">\[
  \hat{Y} = 207.360 - 2.964X + 0.015X^2
  \]</span></p></li>
<li><p>The <span class="math inline">\(p\)</span>-value corresponding to <code>age2</code> is a test for the quadratic term (linear regression as the null versus quadratic regression as the alternative). The <span class="math inline">\(p\)</span>-value in this case (0.08) says that we only have week evidence of a quadratic relationship.</p></li>
<li><p><strong>Exercise</strong>: Write out the null and alternative models associated with the <span class="math inline">\(p\)</span>-value of 0.08109.</p></li>
<li><p><strong>Exercise</strong>: What are the null and alternative models associated with the <span class="math inline">\(p\)</span>-value of 0.004535?</p></li>
<li><p>It’s possible to fit higher order polynomials. E.g. a cubic polynomial <span class="math display">\[
  Y_i = \beta_0 + \beta_1 X_i + \beta_{2}X_{i}^2 + \beta_3 X_{i}^3 + \epsilon_i
  \]</span> We would do this via</p>
<pre class="r"><code>muscle &lt;- mutate(muscle, age3 = age^3)
lm_m3 &lt;- lm(mass ~ age + age2 + age3, data = muscle)
tidy(lm_m3)</code></pre>
<pre><code>## # A tibble: 4 × 5
##   term          estimate  std.error statistic p.value
##   &lt;chr&gt;            &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept) 140.       188.          0.748    0.458
## 2 age           0.565      9.82        0.0575   0.954
## 3 age2         -0.0456     0.168      -0.272    0.786
## 4 age3          0.000337   0.000933    0.361    0.719</code></pre>
<p>However, it is rarely a good idea to fit terms higher than quadratic. This is because</p>
<ol style="list-style-type: decimal">
<li>They tend to be sensative to overfitting and</li>
<li>They are hard to interpret.</li>
</ol>
<p>So at that point, you should just fit a cubic spline to these data, since it will be equally uninterpretable.</p></li>
<li><p>If you include a quadratic term you should <strong>always</strong> include the linear term as well.</p></li>
<li><p>That is, you should <strong>never</strong> fit the model <span class="math display">\[
  Y_i = \beta_0 + \beta_{2} X_i^2 + \epsilon_i
  \]</span> even if the <span class="math inline">\(p\)</span>-value is very high for the <span class="math inline">\(\beta_1\)</span> coefficient..</p></li>
<li><p>Why? This follows the same logic as always including the intercept term in the model. Lower order terms are thought to provide more basic information on the relationship, so you should include them.</p></li>
<li><p>More generally, if you do end up using a cubic term, you should always include both linear and quadratic terms in the model, etc…</p></li>
<li><p>When there are multiple predictors in the model, it is usual to denote quadratic coefficients with repeat indices. E.g. <span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \beta_{11}X_{i1}^2 + \beta_{2}X_{i2} + \beta_{22}X_{i2}^2 + \beta_{12}X_{i1}X_{i2} + \epsilon_i
  \]</span></p></li>
<li><p><strong>Exercise</strong>: Write out a model that contains two predictors, <span class="math inline">\(X_{i1}\)</span> and <span class="math inline">\(X_{i2}\)</span>, where only <span class="math inline">\(X_{i2}\)</span> is quadratic (and so the model is linear in <span class="math inline">\(X_{i1}\)</span>). Use the repeated indexing that we just introduced.</p></li>
</ul>
<div id="practical-considerations-for-using-quadratic-terms" class="section level2">
<h2>Practical considerations for using quadratic terms</h2>
<p>From the <em>Statistical Sleuth</em>: - Quadratic terms should not routinely be included.</p>
<ul>
<li>Use in four situations:
<ol style="list-style-type: decimal">
<li>When the analyst has good reason to suspect that the response is nonlinear in some explanatory variable (through knowledge of the process or by graphical examination).</li>
<li>When the question of interest calls for finding the values that maximize or minimize the mean response.</li>
<li>When careful modeling of the regression is called for by the questions of interest (and presumably this is only the case if there are just a few explanatory variables).</li>
<li>When inclusion is used to produce a rich model for assessing the fit of an inferential model.</li>
</ol></li>
</ul>
</div>
</div>
<div id="splines" class="section level1">
<h1>Splines</h1>
<ul>
<li><p>Splines are piece-wise cubic functions.</p></li>
<li><p>Many curves are well-approximated <strong>locally</strong> by cubic functions, even if they are not well approximated <strong>globally</strong> by cubic functions.</p></li>
<li><p>Think about them as kind of non-parametric curves. But because they are piecewise cubic, we can use them as components in <code>lm()</code>.</p></li>
<li><p>You can include a non-parametric curve as a covariate in R via splines.</p></li>
<li><p>Fit a spline in R using the <code>bs()</code> function.</p>
<pre class="r"><code>library(splines)
lm_mspline &lt;- lm(mass ~ bs(age, df = 5), data = muscle)</code></pre></li>
<li><p>The <code>df</code> argument tells you how much degrees of freedom you want to give up to fit the spline. Higher means more flexible, but also possibly more unstable (if you have too small a sample size).</p></li>
<li><p>Let’s make predictions and plot the predictions:</p>
<pre class="r"><code>newdf &lt;- data.frame(age = seq(min(muscle$age), max(muscle$age), length.out = 100))
newdf$mass &lt;- predict(object = lm_mspline, newdata = newdf)                    
ggplot() +
  geom_line(data = newdf, mapping = aes(x = age, y = mass)) +
  geom_point(data = muscle, mapping = aes(x = age, y = mass))</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-10-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>Do <strong>not</strong> interpret the coefficient estimates and <span class="math inline">\(p\)</span>-values here</p>
<pre class="r"><code>tidy(lm_mspline)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term             estimate std.error statistic  p.value
##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)        110.        3.84    28.7   2.44e-34
## 2 bs(age, df = 5)1    -8.49      8.70    -0.976 3.33e- 1
## 3 bs(age, df = 5)2   -12.0       6.94    -1.72  9.05e- 2
## 4 bs(age, df = 5)3   -44.3       8.22    -5.39  1.60e- 6
## 5 bs(age, df = 5)4   -31.2       7.24    -4.30  7.17e- 5
## 6 bs(age, df = 5)5   -45.6       5.09    -8.95  2.98e-12</code></pre></li>
<li><p>But you can interpret the overall <span class="math inline">\(F\)</span>-test as testing for whether a non-parameteric association exists.</p>
<pre class="r"><code>glance(lm_mspline) %&gt;%
  select(p.value)</code></pre>
<pre><code>## # A tibble: 1 × 1
##    p.value
##      &lt;dbl&gt;
## 1 2.38e-16</code></pre></li>
<li><p>Splines aren’t good for interpretation, but they are good for flexibly controlling for a variable that you can’t seem to get to behave via other transformations.</p></li>
</ul>
</div>
<div id="categorical-variables" class="section level1">
<h1>Categorical Variables</h1>
<div id="two-classes" class="section level2">
<h2>Two classes</h2>
<ul>
<li><p>An innovation in the insurance industry was introduced, and a researcher wanted to study what factors affect how quickly different insurance firms adopted this new innovation. Variables include</p>
<ul>
<li><code>months</code>: How long, in months, it took the firm to adopt the new innovation.</li>
<li><code>size</code>: The amount of total assets of the insurance firm, in millions of dollars.</li>
<li><code>type</code>: The type of firm. Either a mutual company (<code>"mutual"</code>) or a stock company (<code>"stock"</code>).</li>
</ul>
<p>You can load these data into R via:</p>
<pre class="r"><code>firm &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/firm.csv&quot;)
glimpse(firm)</code></pre>
<pre><code>## Rows: 20
## Columns: 3
## $ months &lt;dbl&gt; 17, 26, 21, 30, 22, 0, 12, 19, 4, 16, 28, 15, 11, 38, 31, 21, 2…
## $ size   &lt;dbl&gt; 151, 92, 175, 31, 104, 277, 210, 120, 290, 238, 164, 272, 295, …
## $ type   &lt;chr&gt; &quot;mutual&quot;, &quot;mutual&quot;, &quot;mutual&quot;, &quot;mutual&quot;, &quot;mutual&quot;, &quot;mutual&quot;, &quot;mu…</code></pre></li>
<li><p>Recall that we deal with categorical variables by creating indicators <span class="math display">\[
  X_{i2} =
  \begin{cases}
  1 &amp; \text{ if stock company}\\
  0 &amp; \text{ otherwise}
  \end{cases}
  \]</span></p></li>
<li><p>If a categorical variable has <span class="math inline">\(c\)</span> classes, then we need to use <span class="math inline">\(c-1\)</span> indicator variables to represent this categorical variable.</p></li>
<li><p>Here, <span class="math inline">\(c = 2\)</span> (for <code>"stock"</code> and <code>"mutual"</code>), so we only need <span class="math inline">\(c-1=1\)</span> indicator variable.</p></li>
<li><p>Let <span class="math inline">\(Y_i\)</span> be the number of months elapsed for company <span class="math inline">\(i\)</span>, and <span class="math inline">\(X_{i1}\)</span> be the size of the firm in millions of dollars. Then we will fit the following model.</p>
<p><span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i
  \]</span></p></li>
<li><p>The interpretation of <span class="math inline">\(\beta_1\)</span> is as usual. Firms that are the same type, but are 10 million dollars larger, tended to take <span class="math inline">\(10 \times \beta_1\)</span> months less to innovate.</p></li>
<li><p>To interpret <span class="math inline">\(\beta_2\)</span>, consider that the model for mutual companies is <span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \epsilon_i
  \]</span> while the model for stock companies is <span class="math display">\[
  Y_i = \beta_0 + \beta_2 + \beta_1X_{i1} + \epsilon_i
  \]</span> where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the same in both instances.</p></li>
<li><p>This means that <span class="math inline">\(\beta_2\)</span> is the expected difference in months between mutual and stock companies that are about the same size.</p></li>
<li><p>Let’s visualize this model</p>
<p><img src="figs/cat_interp.png" width="264" style="display: block; margin: auto;" /></p></li>
<li><p>In R, you fit this model by first converting the categorical variable into a <strong>factor</strong> with <code>parse_factor()</code> from the <code>{readr}</code> package. You specify the order of the levels by the <code>levels</code> argument, where the first level is the <strong>reference level</strong> (the one that does not have an indicator for it).</p>
<pre class="r"><code>firm &lt;- mutate(firm, type = parse_factor(type, levels = c(&quot;mutual&quot;, &quot;stock&quot;)))</code></pre>
<p>Then you can use the new factor variable in <code>lm()</code>. It will automatically create <span class="math inline">\(c-1\)</span> indicator variables to fit.</p>
<pre class="r"><code>lm_firm &lt;- lm(months ~ size + type, data = firm)
tidy(lm_firm, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 3 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   33.9     1.81        18.7  9.15e-13   30.0     37.7   
## 2 size          -0.102   0.00889    -11.4  2.07e- 9   -0.121   -0.0830
## 3 typestock      8.06    1.46         5.52 3.74e- 5    4.98    11.1</code></pre>
<p>We conclude that stock companies tended to take 8 months longer than mutual companies to adopt the new innovation (95% CI of 5 to 11 months longer), adjusting for company size. Companies of the same type that had $10 million more assets tended to take 1 fewer month to adopt the new innovation (95% CI of 0.8 to 1.2 fewer months).</p></li>
<li><p>If you wanted to change which level is the reference level of a factor variable, you could use <code>fct_relevel()</code> from the <code>{forcats}</code> package (a part of the tidyverse).</p>
<pre class="r"><code>firm &lt;- mutate(firm, type = fct_relevel(type, &quot;stock&quot;, &quot;mutual&quot;))
lm_firm &lt;- lm(months ~ size + type, data = firm)
tidy(lm_firm, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 3 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   41.9     2.01        20.9  1.50e-13   37.7     46.2   
## 2 size          -0.102   0.00889    -11.4  2.07e- 9   -0.121   -0.0830
## 3 typemutual    -8.06    1.46        -5.52 3.74e- 5  -11.1     -4.98</code></pre></li>
<li><p>Why not fit two separate regressions (one for each firm)?</p>
<ol style="list-style-type: decimal">
<li>Enforces interpretability of <span class="math inline">\(\beta_2\)</span> parameter since otherwise <span class="math inline">\(\beta_1\)</span> would differ between firms.</li>
<li>Use all of the data to estimate <span class="math inline">\(\beta_1\)</span>.</li>
<li>Use all of the data to estimate <span class="math inline">\(\sigma^2\)</span>.</li>
</ol></li>
</ul>
</div>
<div id="more-than-two-classes" class="section level2">
<h2>More than two classes</h2>
<ul>
<li><p>If a categorical variable has <span class="math inline">\(c\)</span> classes, then you use <span class="math inline">\(c-1\)</span> indicator variables to represent this variable.</p></li>
<li><p>The <code>mpg</code> dataset’s <code>drv</code> variable has classes <code>"f"</code>, <code>"4"</code>, and <code>"r"</code>.</p>
<pre class="r"><code>data(&quot;mpg&quot;)
unique(mpg$drv)</code></pre>
<pre><code>## [1] &quot;f&quot; &quot;4&quot; &quot;r&quot;</code></pre></li>
<li><p>We can represent this categorical variable with two indicator variables:</p>
<p><span class="math display">\[\begin{align}
  X_{i1} &amp;= 
  \begin{cases}
  1 &amp; \text{ if front-wheel drive}\\
  0 &amp; \text{ otherwise}
  \end{cases}\\
  X_{i2} &amp;= 
  \begin{cases}
  1 &amp; \text{ if 4-wheel drive}\\
  0 &amp; \text{ otherwise}
  \end{cases}
  \end{align}\]</span></p></li>
<li><p>Suppose we want to explore the association between log-<code>cty</code> with <code>displ</code> and <code>drv</code>. Let <span class="math inline">\(Y\)</span> be the city miles per gallon, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be the indicator variables for front- and 4-wheel drive cars, and let <span class="math inline">\(X_3\)</span> be the car’s engine displacement (in liters). Then our model is <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i
  \]</span></p></li>
<li><p>The model for rear-wheel drive cars is <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_3X_{i3} + \epsilon_i
  \]</span> The model for front-wheel drive cars is <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_1 + \beta_3X_{i3} + \epsilon_i
  \]</span> The model for 4-wheel drive cars is <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_2 + \beta_3X_{i3} + \epsilon_i
  \]</span></p></li>
<li><p>Interpretation:</p>
<ul>
<li><p><span class="math inline">\(\beta_1\)</span> is the average difference in city mpg between rear-wheel and front drive cars of about the same engine size.</p></li>
<li><p><span class="math inline">\(\beta_2\)</span> is the average difference in city mpg between rear-wheel and 4-wheel drive cars of about the same engine size.</p></li>
<li><p><span class="math inline">\(\beta_2 - \beta_1\)</span> is the average difference in city mpg between 4-wheel and front wheel drive cars of about the same engine size.</p></li>
<li><p><span class="math inline">\(\beta_3\)</span> is the average difference in city mpg between cars that are of the same type, but have 1 liter different engine displacement.</p></li>
</ul></li>
<li><p>We can visualize this model below:</p>
<p><img src="figs/cat_interp3.png" width="825" style="display: block; margin: auto;" /></p></li>
<li><p>We fit this model in R by first selecting the order of the levels of <code>drv</code> to include rear-wheel drive cars as the reference class. We do this using <code>parse_factor()</code> from the <code>{forcats}</code> package:</p>
<pre class="r"><code>mpg &lt;- mutate(mpg, 
              l_cty = log(cty),
              drv = parse_factor(drv, levels = c(&quot;r&quot;, &quot;f&quot;, &quot;4&quot;)))</code></pre>
<p>Then we run <code>lm()</code>.</p>
<pre class="r"><code>lm_mpg &lt;- lm(l_cty ~ drv + displ, data = mpg)
tidy(lm_mpg, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 4 × 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   3.37     0.0534     63.2   2.55e-147   3.27      3.48  
## 2 drvf         -0.0263   0.0370     -0.712 4.77e-  1  -0.0992    0.0466
## 3 drv4         -0.158    0.0304     -5.20  4.48e-  7  -0.218    -0.0981
## 4 displ        -0.143    0.00905   -15.8   2.01e- 38  -0.160    -0.125</code></pre></li>
<li><p>The estimated regression surface is <span class="math display">\[
  \log(\text{city mpg}) = 3.37 - 0.026\times \text{front} - 0.158 \times \text{4-wheel} - 0.142 \times \text{displacement}
  \]</span></p></li>
<li><p>We obtained confidence intervals for each of these coefficients, but how do we obtain a confidence interval for <span class="math inline">\(\beta_2 - \beta_1\)</span> (the difference in average mpg between front- and 4-wheel drive cars of the same size)? There are some third-party packages that do this. But an easy way is to just change which class is the reference</p>
<pre class="r"><code>mpg &lt;- mutate(mpg, drv = fct_relevel(drv, &quot;f&quot;, &quot;r&quot;, &quot;4&quot;))
lm_mpg_2 &lt;- lm(l_cty ~ drv + displ, data = mpg)
tidy(lm_mpg_2, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 4 × 7
##   term        estimate std.error statistic   p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   3.34     0.0263    127.    3.11e-215   3.29      3.40  
## 2 drvr          0.0263   0.0370      0.712 4.77e-  1  -0.0466    0.0992
## 3 drv4         -0.132    0.0220     -6.00  7.66e-  9  -0.175    -0.0884
## 4 displ        -0.143    0.00905   -15.8   2.01e- 38  -0.160    -0.125</code></pre>
<p>So this confidence interval is -0.175 to -0.089.</p></li>
</ul>
</div>
</div>
<div id="interaction-effects" class="section level1">
<h1>Interaction Effects</h1>
<ul>
<li><p>An <strong>interaction</strong> between two variables means that the slope with respect to one variable changes with the value of the second variable.</p></li>
<li><p>We represent this is by multiplying the two variables together. <span class="math display">\[
  Y_{i} = \beta_0 + \beta_1X_{i1} + \beta_{2}X_{i2} + \beta_{12}X_{i1}X_{i2} + \epsilon_i
  \]</span></p></li>
<li><p>The slope with respect to <span class="math inline">\(X_1\)</span> when <span class="math inline">\(X_2\)</span> is fixed is <span class="math inline">\(\beta_1 + \beta_{12}X_2\)</span></p></li>
<li><p>The slope with respect to <span class="math inline">\(X_2\)</span> when <span class="math inline">\(X_1\)</span> is fixed is <span class="math inline">\(\beta_2 + \beta_{12}X_1\)</span></p></li>
<li><p>NOTE: <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> <strong>no longer</strong> represent the expected difference in <span class="math inline">\(Y\)</span> given a unit difference of <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>, respectively.</p>
<ul>
<li><p><span class="math inline">\(\beta_1\)</span> is the expected difference in <span class="math inline">\(Y\)</span> given a unit difference in <span class="math inline">\(X_1\)</span> if <span class="math inline">\(X_2 = 0\)</span>. This interpretation only makes sense if 0 is in the range of <span class="math inline">\(X_2\)</span>.</p></li>
<li><p><span class="math inline">\(\beta_2\)</span> is the expected difference in <span class="math inline">\(Y\)</span> given a unit difference in <span class="math inline">\(X_2\)</span> if <span class="math inline">\(X_1 = 0\)</span>. This interpretation only makes sense if 0 is in the range of <span class="math inline">\(X_1\)</span>.</p></li>
<li><p>Generally, it is not useful to provide these interpretations of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>.</p></li>
</ul></li>
<li><p>The association between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(Y\)</span> depends on the level of <span class="math inline">\(X_2\)</span>. Likewise, the association between <span class="math inline">\(X_2\)</span> and <span class="math inline">\(Y\)</span> depends on the level of <span class="math inline">\(X_1\)</span>.</p></li>
<li><p>When <span class="math inline">\(\beta_{12}\)</span> is positive, this is called an interaction of the <strong>reinforcement</strong> or <strong>synergistic</strong> type. The slope is more positive for more positive levels of <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>.</p></li>
<li><p>When <span class="math inline">\(\beta_{12}\)</span> is negative, this is called an interaction of the <strong>interference</strong> or <strong>antagonistic</strong> type. The slope is less positive for more positive levels of <span class="math inline">\(X_1\)</span> or <span class="math inline">\(X_2\)</span>.</p></li>
<li><p>Synergistic:</p>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-25-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>Antagonistic:</p>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-26-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>If you include an interaction term, make sure that you include all lower-order terms. That is, <strong>Never</strong> fit a model like <span class="math display">\[
  Y_i = \beta_0 + \beta_{12}X_{i1}X_{i2}
  \]</span></p>
<ul>
<li>Why? The above model says that the effect of <span class="math inline">\(X_1\)</span> depends on the level of <span class="math inline">\(X_2\)</span>, but there is no effect of <span class="math inline">\(X_1\)</span>. This is a logical inconsistency.</li>
</ul></li>
<li><p>For the <code>mtcars</code> dataset, suppose we are fitting a model for mpg (<span class="math inline">\(Y\)</span>) on the rear-axle ration (<span class="math inline">\(X_1\)</span>) and weight (<span class="math inline">\(X_2\)</span>). The model with interactions is: <span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_{12}X_{i1}X_{i2} + \epsilon_i
  \]</span></p></li>
<li><p>We don’t need to make transformations ahead of time. In <code>lm()</code>, we just multiply the variables together. It will automatically include all lower-order terms.</p>
<pre class="r"><code>lm_mt &lt;- lm(mpg ~ drat * wt, data = mtcars)
tidy(lm_mt)</code></pre>
<pre><code>## # A tibble: 4 × 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)     5.55     12.6      0.439  0.664 
## 2 drat            8.49      3.32     2.56   0.0162
## 3 wt              3.88      3.80     1.02   0.315 
## 4 drat:wt        -2.54      1.09    -2.33   0.0274</code></pre></li>
<li><p>We actually have some evidence here of needing an interaction.</p></li>
<li><p>We would still include <code>wt</code> in the model even though it has a large <span class="math inline">\(p\)</span>-value.</p></li>
<li><p>You can provide an interpretation to a client when there are interactions via <strong>conditional effects plots</strong> that plot the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_i\)</span> at different levels of the other <span class="math inline">\(X\)</span>’s.</p>
<pre class="r"><code>## drat versus mpg at different levels of wt
wt_seq &lt;- quantile(mtcars$mpg, c(0.025, 0.25, 0.5, 0.75, 0.975))
dr_seq &lt;- seq(min(mtcars$drat), max(mtcars$drat), length.out = 100)
newdf &lt;- expand.grid(wt = wt_seq, drat = dr_seq)
newdf$mpg &lt;- predict(object = lm_mt, newdata = newdf)
newdf &lt;- mutate(newdf, wt = as_factor(round(wt, digits = 2)))
qplot(x = drat, y = mpg, color = wt, data = newdf, geom = &quot;line&quot;)</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-28-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## wt versus mpg at different levels of drat
wt_seq &lt;- seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)
dr_seq &lt;- quantile(mtcars$drat, c(0.025, 0.25, 0.5, 0.75, 0.975))
newdf &lt;- expand.grid(wt = wt_seq, drat = dr_seq)
newdf$mpg &lt;- predict(object = lm_mt, newdata = newdf)
newdf &lt;- mutate(newdf, drat = as_factor(round(drat, digits = 2)))
qplot(x = wt, y = mpg, color = drat, data = newdf, geom = &quot;line&quot;)</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-28-2.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="interactions-and-categorical-variables." class="section level2">
<h2>Interactions and categorical variables.</h2>
<ul>
<li><p>An interaction between a categorical and quantitative variable means that each level of the categorical variable has its own line (different intercepts and slopes).</p></li>
<li><p>Let’s study again the association between <code>cty</code> (<span class="math inline">\(Y\)</span>) with <code>displ</code> (<span class="math inline">\(X_3\)</span>) and <code>drv</code> from the <code>mpg</code> dataset.</p>
<pre class="r"><code>data(&quot;mpg&quot;)
mpg_sub &lt;- select(mpg, drv, displ, cty)
glimpse(mpg_sub)</code></pre>
<pre><code>## Rows: 234
## Columns: 3
## $ drv   &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;,…
## $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.8, 2.8,…
## $ cty   &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 15, 15, …</code></pre></li>
<li><p>We fit the following model (recall that <span class="math inline">\(X_1\)</span> is the indicator for front-wheel drive and <span class="math inline">\(X_2\)</span> is the indicator for 4-wheel drive cars). <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_3X_{i3} + \beta_{13}X_{i1}X_{i3} + \beta_{23}X_{i2}X_{i3} + \epsilon_i
  \]</span></p></li>
<li><p>If you include an interaction between one indicator for a categorical and a quantitative, then you should include interactions for all of the indicators with that quantitative. R will do this automatically as long as you use factors.</p></li>
<li><p>To interpret this model, you should think about what the response surface is for each class in the categorical variable.</p>
<ul>
<li>The model for rear-wheel drive cars is <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_3X_{i3} + \epsilon_i
  \]</span></li>
<li>The model for front-wheel drive cars is <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_1 + (\beta_3 + \beta_{13})X_{i3} + \epsilon_i
  \]</span></li>
<li>The model for 4-wheel drive cars is <span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_2 + (\beta_3 + \beta_{23})X_{i3} + \epsilon_i
  \]</span></li>
</ul></li>
<li><p><span class="math inline">\(\beta_{13}\)</span> is the expected difference in the slope between rear-wheel and front-wheel drive cars.</p></li>
<li><p><span class="math inline">\(\beta_{23}\)</span> is the expected difference in the slope between rear-wheel and 4-wheel drive cars.</p></li>
<li><p>But it is better to just interpret interactions here by making conditional effects plots. This is the default output of <code>geom_smooth()</code> when you color code by a categorical variable:</p>
<pre class="r"><code>qplot(x = displ, y = cty, color = drv, data = mpg) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  scale_y_log10()</code></pre>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-30-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>To fit this in R, just make sure you have the reference level as you like via <code>parse_factor()</code> or <code>fct_relevel()</code>. Then use <code>lm()</code> the same way with interactions as before (using multiplication).</p>
<pre class="r"><code>mpg_sub &lt;- mutate(mpg_sub, 
                  drv = parse_factor(drv, levels = c(&quot;r&quot;, &quot;f&quot;, &quot;4&quot;)),
                  l_cty = log(cty))
lm_mpg_int &lt;- lm(l_cty ~ drv * displ, data = mpg_sub)
tidy(lm_mpg_int)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   3.00      0.168      17.8  4.19e-45
## 2 drvf          0.417     0.174       2.39 1.77e- 2
## 3 drv4          0.210     0.174       1.21 2.29e- 1
## 4 displ        -0.0700    0.0321     -2.18 3.01e- 2
## 5 drvf:displ   -0.0990    0.0366     -2.70 7.36e- 3
## 6 drv4:displ   -0.0707    0.0339     -2.08 3.82e- 2</code></pre></li>
<li><p><strong>Exercise</strong>: What is the estimated regression surface for rear-wheel drive cars? Front-wheel drive cars? 4-wheel drive cars?</p></li>
<li><p>We obtain the same regression surface as if we fit three different models to the three different classes.</p>
<pre class="r"><code>mpg_sub_r &lt;- filter(mpg_sub, drv == &quot;r&quot;)
tidy(lm(l_cty ~ displ, data = mpg_sub_r))</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   3.00      0.204      14.7  3.63e-13
## 2 displ        -0.0700    0.0390     -1.80 8.56e- 2</code></pre></li>
<li><p>The benefit of fitting a single model is then</p>
<ol style="list-style-type: decimal">
<li>We can directly test for the “different lines” model using <span class="math inline">\(F\)</span>-tests</li>
<li>We use all of the data to estimate the residual variance.</li>
</ol></li>
<li><p>The <span class="math inline">\(p\)</span>-values for the interaction terms in the above output are <strong>not</strong> useful. This is because they only test for the association for an interaction between <code>displ</code> and a single indicator variable. But we want to test whether there is an interaction between <code>displ</code> and <code>drv</code>.</p></li>
<li><p>Let’s test for an interaction between <code>displ</code> and <code>drv</code>. We do so by fitting models both with and with out interactions. We then use <code>anova()</code> to run an <span class="math inline">\(F\)</span>-test.</p>
<pre class="r"><code>lm_mpg_no_int &lt;- lm(l_cty ~ drv + displ, data = mpg_sub)
lm_mpg_int &lt;- lm(l_cty ~ drv * displ, data = mpg_sub)
anova(lm_mpg_no_int, lm_mpg_int)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: l_cty ~ drv + displ
## Model 2: l_cty ~ drv * displ
##   Res.Df  RSS Df Sum of Sq   F Pr(&gt;F)
## 1    230 3.75                        
## 2    228 3.64  2     0.118 3.7  0.026</code></pre>
<p>So we have moderate evidence of an interaction effect (<span class="math inline">\(p = 0.026\)</span>).</p></li>
</ul>
</div>
<div id="usage-guidelines" class="section level2">
<h2>Usage guidelines</h2>
<p>From the <em>Statistical Sleuth</em>:</p>
<ul>
<li><p>Interaction terms should not be routinely included in regression models.</p></li>
<li><p>Use in three scenarios:</p>
<ol style="list-style-type: decimal">
<li>When a question of interest pertains to interaction.</li>
<li>When good reason exists to suspect interactions (e.g. <em>a priori</em> knowledge or residual plots strongly suggest such).</li>
<li>When interactions are proposed as a more general model for the purpose of examining the goodness of fit of a model without interaction.</li>
</ol></li>
</ul>
</div>
</div>
<div id="comparing-a-saturated-second-order-model-for-lack-of-fit" class="section level1">
<h1>Comparing a Saturated Second-Order Model for Lack-of-fit</h1>
<ul>
<li><p>If you only have a few predictors, it is often a good idea to fit a saturated second-order model and compare the fully linear model to it as a type of lack-of-fit test.</p>
<ul>
<li><em>Saturated second-order model</em>: A model with all possible quadratic and interaction terms, along with lower-order terms.</li>
</ul></li>
<li><p>For example, with two predictors: <span class="math display">\[\begin{align}
  H_0: Y_i &amp;= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \epsilon_i\\
  H_A: Y_i &amp;= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_{11}X_{i1}^2 + \beta_{22}X_{i2}^2  + \beta_{12}X_{i1}X_{i2} + \epsilon_i
  \end{align}\]</span></p></li>
<li><p>Rejecting the null here indicates that we have evidence of a lack-of-fit and should include some quadratic or interaction terms.</p></li>
<li><p>Let’s look at a case example studying the effects of temperature (<span class="math inline">\(X_1\)</span>) in Celsius and charge rate (<span class="math inline">\(X_2\)</span>) in amperes on life expectancy of a batter (<span class="math inline">\(Y\)</span>) in the number of cycles before death. Data can be read into R via</p>
<pre class="r"><code>power &lt;- tibble::tribble(
  ~cycles, ~charge, ~temp,
      150,     0.6,    10,
       86,       1,    10,
       49,     1.4,    10,
      288,     0.6,    20,
      157,       1,    20,
      131,       1,    20,
      184,       1,    20,
      109,     1.4,    20,
      279,     0.6,    30,
      235,       1,    30,
      224,     1.4,    30
  )</code></pre>
<p>We fit the model <span class="math display">\[
  Y_i = \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \beta_{11}X_{i1}^2 + \beta_{22}X_{i2}^2  + \beta_{12}X_{i1}X_{i2} + \epsilon_i
  \]</span> and run the hypothesis tests for <span class="math display">\[
  H_0: \beta_{11} = \beta_{22} = \beta_{12} = 0\\
  H_A: \text{ at least one of } \beta_{11}, \beta_{22}, \beta_{12} \text{ nonzero}
  \]</span></p></li>
<li><p>We can run this test because the reduced model is a subset of the full model.</p></li>
<li><p>Let’s fit the full and reduced models</p>
<pre class="r"><code>power &lt;- mutate(power, charge2 = charge ^ 2, temp2 = temp ^ 2)
lm_full &lt;- lm(cycles ~ charge * temp + charge2 + temp2, data = power)
lm_reduced &lt;- lm(cycles ~ charge + temp, data = power)</code></pre></li>
<li><p>We then run <code>anova()</code> to run the corresponding <span class="math inline">\(F\)</span>-test</p>
<pre class="r"><code>anova(lm_reduced, lm_full)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: cycles ~ charge + temp
## Model 2: cycles ~ charge * temp + charge2 + temp2
##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)
## 1      8 7700                         
## 2      5 5240  3      2460 0.78   0.55</code></pre></li>
<li><p>We do not have evidence of a lack-of-fit (<span class="math inline">\(p=0.55\)</span>)</p></li>
<li><p><strong>Exercise</strong>: What is <span class="math inline">\(SSE(F)\)</span>? <span class="math inline">\(SSE(R)\)</span>? <span class="math inline">\(dr_F\)</span>? <span class="math inline">\(df_R\)</span>?</p></li>
<li><p>If we would have failed to reject, the best course of action for interpretation would be to come up with conditional effects plots.</p>
<pre class="r"><code>## Conditional effects plot for charge
charge_seq &lt;- unique(power$charge)
temp_seq &lt;- seq(min(power$temp), max(power$temp), length.out = 200)
expand.grid(charge = charge_seq, temp = temp_seq) %&gt;%
  mutate(charge2 = charge^2,
         temp2 = temp^2) -&gt;
  newdf
newdf$cycles &lt;- predict(object = lm_full, newdata = newdf)
newdf &lt;- mutate(newdf, charge = as_factor(charge))
qplot(x = temp, y = cycles, color = charge, data = newdf, geom = &quot;line&quot;)</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-39-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>## Conditional effects plot for temperature
temp_seq &lt;- unique(power$temp)
charge_seq &lt;- seq(min(power$charge), max(power$charge), length.out = 200)
expand.grid(charge = charge_seq, temp = temp_seq) %&gt;%
  mutate(charge2 = charge^2,
         temp2 = temp^2) -&gt;
  newdf
newdf$cycles &lt;- predict(object = lm_full, newdata = newdf)
newdf &lt;- mutate(newdf, temp = as_factor(temp))
qplot(x = charge, y = cycles, color = temp, data = newdf, geom = &quot;line&quot;)</code></pre>
<p><img src="05_mlr_iii_files/figure-html/unnamed-chunk-39-2.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
<div id="exercise" class="section level1">
<h1>Exercise</h1>
<ul>
<li><p>Researchers were studying the relationship between line speed and the amount of scrap for two production lines in a soap production company. The variables include</p>
<ul>
<li><code>scrap</code>: Amount of scrap (coded). This is the response variable.</li>
<li><code>speed</code>: Line production speed (coded).</li>
<li><code>line</code>: Production line. Either <code>"line1"</code> or <code>"line2"</code>.</li>
</ul>
<p>You can load these data into R via:</p>
<pre class="r"><code>library(tidyverse)
library(broom)
soap &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/soap.csv&quot;)
glimpse(soap)</code></pre>
<pre><code>## Rows: 27
## Columns: 3
## $ scrap &lt;dbl&gt; 218, 248, 360, 351, 470, 394, 332, 321, 410, 260, 241, 331, 275,…
## $ speed &lt;dbl&gt; 100, 125, 220, 205, 300, 255, 225, 175, 270, 170, 155, 190, 140,…
## $ line  &lt;chr&gt; &quot;line1&quot;, &quot;line1&quot;, &quot;line1&quot;, &quot;line1&quot;, &quot;line1&quot;, &quot;line1&quot;, &quot;line1&quot;, &quot;…</code></pre></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Run an exploratory data analysis. Does one line seem to produce less scrap than the other, controlling for speed? Does the association between scrap and speed differ between the different lines?</p></li>
<li><p>Define appropriate variables and write out a model that includes an interaction between the production line and the speed of production.</p></li>
<li><p>Fit the interaction model from part 2 in R. Does the model seem appropriate?</p></li>
<li><p>Do the two production lines have the same slope for the relationship between scrap and speed?</p></li>
<li><p>Formally answer the question on whether one line produces less scrap than the other.</p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
