<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2022-09-21" />

<title>MLR: Practical Considerations</title>

<script src="site_libs/header-attrs-2.16/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "Óâô";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">MLR: Practical Considerations</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2022-09-21</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Choice of scaling predictors for interpretation.</li>
<li>Multicollinearity</li>
<li>Effect sizes, practical significance, and statistical
significance.</li>
<li>Chapter 12 from ROS</li>
<li>Section 7.6 from KNNL</li>
</ul>
<!-- # Steps of an analysis -->
<!-- - For **any analysis** where you are applying a model, the steps you should take are: -->
<!--     1. Exploratory data analysis -->
<!--     2. Choose the form of the model. -->
<!--     3. Fit the model to the data. -->
<!--     4. Assess how well the model describes the data. -->
<!--     5. Use the model to address the question of interest. -->
<!-- - You should be performing each of these steps when you apply a model to data. -->
</div>
<div id="scales" class="section level1">
<h1>Scales</h1>
<div id="interpretable-scales" class="section level2">
<h2>Interpretable Scales</h2>
<ul>
<li><p>Recall <a
href="https://dcgerard.github.io/stat_415_615/data.html#Earnings_Data">earnings
data</a> exploring the relationship between height and earnings.</p>
<pre class="r"><code>library(tidyverse)
library(broom)
earnings &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/earnings.csv&quot;)
earnings &lt;- mutate(earnings,
                   height_mm = height * 25.4,
                   height_in = height,
                   height_mi = height / 63360)

tidy(lm(earn ~ height_mm, data = earnings))</code></pre>
<pre><code>## # A tibble: 2 √ó 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept) -85027.    8861.       -9.60 2.64e-21
## 2 height_mm       62.8      5.23     12.0  5.42e-32</code></pre>
<pre class="r"><code>tidy(lm(earn ~ height_in, data = earnings))</code></pre>
<pre><code>## # A tibble: 2 √ó 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  -85027.     8861.     -9.60 2.64e-21
## 2 height_in      1595.      133.     12.0  5.42e-32</code></pre>
<pre class="r"><code>tidy(lm(earn ~ height_mi, data = earnings))</code></pre>
<pre><code>## # A tibble: 2 √ó 5
##   term          estimate std.error statistic  p.value
##   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    -85027.     8861.     -9.60 2.64e-21
## 2 height_mi   101056650.  8419609.     12.0  5.42e-32</code></pre></li>
<li><p>The estimated regression line depends on the units for <span
class="math inline">\(X\)</span> and <span
class="math inline">\(Y\)</span>.</p>
<ul>
<li>earnings = -85,000 + 36 * height(millimeters)</li>
<li>earnings = -85,000 + 1600 * height(inches)</li>
<li>earnings = -85,000 + 101,001,000 * height(miles)</li>
</ul></li>
<li><p>Does this mean that height in miles is a stronger predictor of
earnings (since its coefficient estimate is larger?)</p>
<ul>
<li>NO! ü§¶</li>
<li>The scale of height was chosen arbitrarily. These three equations
all reflect the same underlying information.</li>
</ul></li>
<li><p>By itself, <strong>the size of the regression coefficients tells
you nothing about the importance of a predictor</strong> because this
coefficient can change if you change the units of the
predictor.</p></li>
<li><p><strong>Exercise</strong>: When exploring the effect of the
percent of of the population with bachelor‚Äôs degrees and percent
unemployment on percent below the poverty line in the <a
href="https://dcgerard.github.io/stat_415_615/data.html#County_Demographic_Information">County
Demographic Information</a> data, researchers obtained the following
output:</p>
<pre class="r"><code>cdi &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/cdi.csv&quot;)
cdi &lt;- select(cdi, poverty, bachelors, unemployment)
lmout &lt;- lm(poverty ~ bachelors + unemployment, data = cdi)
tidy(lmout)</code></pre>
<pre><code>## # A tibble: 3 √ó 5
##   term         estimate std.error statistic  p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)     7.83     1.15        6.80 3.56e-11
## 2 bachelors      -0.148    0.0303     -4.88 1.47e- 6
## 3 unemployment    0.608    0.0992      6.13 1.97e- 9</code></pre>
<p>Which has a bigger effect on <code>poverty</code>:
<code>bachelors</code> or <code>unemployment</code> and why?</p></li>
<li><p>You should choose a scale which is in the most interpretable
units.</p></li>
<li><p>A 1 millimeter difference in height is not a meaningful
difference, in terms of the distribution of human heights.</p></li>
<li><p>A 1 mile difference in height is unrealistic.</p></li>
<li><p>A 1 inch difference in height is more reasonable.</p>
<pre class="r"><code>qplot(x = height_in, data = earnings, bins = 20)</code></pre>
<p><img src="05_mlr_ii_files/figure-html/unnamed-chunk-4-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>You can use the standard deviation of the variables to help guide
you. If the standard deviation is between 0.5 and 10, then you have a
good scale. But this is a rough rule of thumb.</p>
<pre class="r"><code>sd(earnings$height_mm)</code></pre>
<pre><code>## [1] 97.33</code></pre>
<pre class="r"><code>sd(earnings$height_in)</code></pre>
<pre><code>## [1] 3.832</code></pre>
<pre class="r"><code>sd(earnings$height_mi)</code></pre>
<pre><code>## [1] 6.048e-05</code></pre></li>
<li><p>What if you are logging variables?</p>
<ul>
<li>Recall, we interpret log-relationships in terms of
<em>multiplicative</em> changes. So being twice as tall is just as good
in inches (84 inches versus 42 inches) as it is in miles (0.00133 miles
versus 0.00066 miles).</li>
<li>So choosing the units does not really matter.</li>
<li>But choosing the <em>size</em> of the multiplicative difference is
important (50% larger versus twice as large versus ten times as large,
etc).</li>
<li>E.g., there are very few humans who are twice as large as other
humans. So a better multiplicative difference would be 1.05 (5%
taller).</li>
</ul>
<pre class="r"><code>earnings &lt;- mutate(earnings, log_height = log(height))
tidy(lm(earn ~ log_height, data = earnings))</code></pre>
<pre><code>## # A tibble: 2 √ó 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept) -425211.    37370.     -11.4 5.03e-29
## 2 log_height   106362.     8904.      11.9 1.03e-31</code></pre>
<p>Individuals that are 5% taller make <span
class="math inline">\(106362 * \log(1.05) = \$5189\)</span> more on
average.</p>
<ul>
<li>You can choose a good multiplier by seeing if the standard deviation
of the log of that base is between 0.5 and 10 (but this is a rough rule
of thumb).</li>
</ul>
<pre class="r"><code>sd(log(earnings$height, base = 10)) # bad</code></pre>
<pre><code>## [1] 0.02484</code></pre>
<pre class="r"><code>sd(log(earnings$height, base = 2)) # bad</code></pre>
<pre><code>## [1] 0.08253</code></pre>
<pre class="r"><code>sd(log(earnings$height, base = 1.05)) # good</code></pre>
<pre><code>## [1] 1.173</code></pre>
<pre class="r"><code>sd(log(earnings$height, base = 1.01)) # good</code></pre>
<pre><code>## [1] 5.749</code></pre></li>
<li><p><strong>Exercise</strong>: A study was exploring the association
between a country‚Äôs wine consumption (liters per person per year) and
mortality rate (deaths per 1000 individuals). Researchers found that a
power-law relationship was a fine approximation. Based on the below plot
alone, choose a good multiplier to interpret the coefficient of a
regression of log-mortality on log-wine. Verify your result using our
rule of thumb above.</p>
<pre class="r"><code>library(Sleuth3)
data(&quot;ex0823&quot;)
wine &lt;- ex0823
qplot(x = Wine, y = Mortality, data = wine)</code></pre>
<p><img src="05_mlr_ii_files/figure-html/unnamed-chunk-8-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p><strong>Exercise</strong>: World record mile time progressions
from 1913 to 1999. Data are from Figure A.1 of ROS. Variables
include</p>
<ul>
<li><code>year</code>: The date (in years) of the new world record.</li>
<li><code>seconds</code>: The new world record (in seconds).</li>
</ul>
<pre class="r"><code>mile &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/mile.csv&quot;)
glimpse(mile)</code></pre>
<pre><code>## Rows: 32
## Columns: 2
## $ year    &lt;dbl&gt; 1913, 1916, 1924, 1932, 1934, 1934, 1938, 1943, 1943, 1943, 19‚Ä¶
## $ seconds &lt;dbl&gt; 254.4, 252.6, 250.4, 249.2, 247.6, 246.8, 246.4, 246.2, 246.2,‚Ä¶</code></pre>
<p>The default scale of the <span class="math inline">\(X\)</span>
variable is in years. Is this the best scale? If not, transform the
<span class="math inline">\(X\)</span>-variable to a more appropriate
scale, fit a regression of world record on year, then provide an
interpretation for the slope.</p></li>
</ul>
</div>
<div id="z-score-scales" class="section level2">
<h2>Z-score scales</h2>
<ul>
<li><p>The idea behind using standard deviations is that roughly
(i.e.¬†generally only exactly if normal) 68% of observations will be
within 1 standard deviation of the mean. So this difference represents
something meaningful.</p></li>
<li><p>Above, I suggested to use standard deviations to <em>guide</em>
your scaling. Alternatively, we could scale explicitly <em>by</em>
standard deviations.</p></li>
<li><p>The <span class="math inline">\(Z\)</span>-score for a variable
is its value, minus its mean, divided by its standard deviation. <span
class="math display">\[
  Z_i = \frac{X_i - \bar{X}}{s_x}
  \]</span></p></li>
<li><p><span class="math inline">\(Z_i\)</span> is now in units of
<em>standard deviations of <span class="math inline">\(X_i\)</span> from
its mean</em>.</p>
<ul>
<li><span class="math inline">\(Z_i = 1\)</span> means individual <span
class="math inline">\(i\)</span> has an <span
class="math inline">\(X_i\)</span> value that is one standard deviation
above the mean.</li>
<li><span class="math inline">\(Z_i = -1\)</span> means individual <span
class="math inline">\(i\)</span> has an <span
class="math inline">\(X_i\)</span> value that is one standard deviation
below the mean.</li>
<li><span class="math inline">\(Z_i = 2\)</span> means individual <span
class="math inline">\(i\)</span> has an <span
class="math inline">\(X_i\)</span> value that is two standard deviations
above the mean.</li>
<li>etc</li>
</ul></li>
<li><p><strong>Example</strong>: For the heights and earnings dataset,
let‚Äôs create <span class="math inline">\(Z\)</span>-scores.</p>
<pre class="r"><code>earnings &lt;- mutate(earnings, 
                   height_z = (height - mean(height)) / sd(height),
                   earn_z = (earn - mean(earn)) / sd(earn))
glimpse(select(earnings, earn_z, height_z))</code></pre>
<pre><code>## Rows: 1,816
## Columns: 2
## $ earn_z   &lt;dbl&gt; 1.28053, 1.72435, 0.39290, 0.17099, 1.28053, 1.81312, 1.32492‚Ä¶
## $ height_z &lt;dbl&gt; 1.9393, -0.1484, -0.6704, -0.4094, -0.9314, 0.3735, -0.9314, ‚Ä¶</code></pre>
<p>Centering and scaling does not change the underlying information.</p>
<pre class="r"><code>qplot(x = height, y = earn, data = earnings) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="05_mlr_ii_files/figure-html/unnamed-chunk-17-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = height_z, y = earn_z, data = earnings) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="05_mlr_ii_files/figure-html/unnamed-chunk-17-2.png" width="384" style="display: block; margin: auto;" />
But this will make interpretation a little easier.</p>
<pre class="r"><code>lm_earn_z &lt;- lm(earn_z ~ height_z, data = earnings)
tidy(lm_earn_z, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 2 √ó 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) 4.70e-16    0.0226  2.08e-14 1.00e+ 0  -0.0443    0.0443
## 2 height_z    2.71e- 1    0.0226  1.20e+ 1 5.42e-32   0.227     0.316</code></pre>
<p>So a 1 standard deviation difference in height corresponds to 0.27
standard deviations higher earnings.</p></li>
<li><p>If you do this to all variables in your model, you can use the
following multiple regression model: <span class="math display">\[
  Y_i = \beta_0^* + \beta_1^*Z_{i1} + \beta_2^* Z_{i2} + \cdots +
\beta_{p-1}^*Z_{i, p-1} + \epsilon_i
  \]</span></p></li>
<li><p>If we run a linear regression under this model, then the
interpretation of coefficient <span
class="math inline">\(\beta_i^*\)</span> is:</p>
<blockquote>
<p>Individuals that are 1 standard deviation larger in <span
class="math inline">\(X_i\)</span> have <span
class="math inline">\(\beta_1^*\)</span> higher <span
class="math inline">\(Y_i\)</span> on average, adjusting for all other
predictors in our model.</p>
</blockquote></li>
<li><p>Note: If <span class="math inline">\(X_i^* = \frac{X_i -
a}{b}\)</span>, then <span class="math inline">\(\beta_i^* =
b\beta_i\)</span> and <span class="math inline">\(\hat{\beta}_i^* =
b\hat{\beta}_i\)</span>.</p></li>
<li><p><strong>Exercise</strong>: Recall the portrait studio example
where A portrait studio chain contains data on 21 cities (from Section
6.9 of KNNL). Variables include</p>
<ul>
<li><p><code>young</code>: The number of persons aged 16 or younger in
the community (thousands of persons).</p></li>
<li><p><code>disp</code>: The per capita disposable personal income in
the community (thousands of dollars).</p></li>
<li><p><code>sales</code>: Portrait studio sales in the community
(thousands of dollars).</p></li>
<li><p>You can load these data into R with:</p>
<pre class="r"><code>portrait &lt;- tribble(~young, ~disp, ~sales,
                    68.5,   16.7,  174.4,
                    45.2,   16.8,  164.4,
                    91.3,   18.2,  244.2,
                    47.8,   16.3,  154.6,
                    46.9,   17.3,  181.6,
                    66.1,   18.2,  207.5,
                    49.5,   15.9,  152.8,
                    52.0,   17.2,  163.2,
                    48.9,   16.6,  145.4,
                    38.4,   16.0,  137.2,
                    87.9,   18.3,  241.9,
                    72.8,   17.1,  191.1,
                    88.4,   17.4,  232.0,
                    42.9,   15.8,  145.3,
                    52.5,   17.8,  161.1,
                    85.7,   18.4,  209.7,
                    41.3,   16.5,  146.4,
                    51.7,   16.3,  144.0,
                    89.6,   18.1,  232.6,
                    82.7,   19.1,  224.1,
                    52.3,   16.0,  166.5)</code></pre></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Fit a linear model of sales (<span
class="math inline">\(Y\)</span>) on young (<span
class="math inline">\(X_1\)</span>) and disp (<span
class="math inline">\(X_2\)</span>). What are the coefficient
estimates?</p></li>
<li><p>What would happen to the coefficient estimates of
<code>young</code> and <code>disp</code> if you converted the units of
<code>disp</code> to tens of thousands of dollars?</p></li>
<li><p>Convert <code>disp</code> to units of tens of thousands of
dollars and rerun the regression. Did what you think would happen to the
coefficient happen?</p></li>
<li><p>What about if convert <code>disp</code> to <span
class="math inline">\(z\)</span>-scores? What do you think would happen
to the <code>young</code> coefficient estimate? Verify your
statement.</p></li>
<li><p>Run a linear model by first converting <code>young</code> and
<code>disp</code> to <span class="math inline">\(z\)</span>-scores.
Interpret the coefficients.</p></li>
</ol></li>
<li><p><span class="math inline">\(Z\)</span>-scores make the
coefficients more comparable. However, the issues of multicollinearity
(next section) still might make it hard to compare the importance of
various predictors. This is because, e.g.¬†in our portrait example, there
might not be any cities that are 1 standard deviation higher in
disposable income but have the same number of young folks.</p></li>
</ul>
</div>
</div>
<div id="multicollinearity" class="section level1">
<h1>Multicollinearity</h1>
<ul>
<li><p><strong>Multicollinearity</strong>: Correlation between
predictors.</p></li>
<li><p>In observational studies, multicollinearity is the rule, not the
exception.</p>
<ul>
<li>In observational studies, you will always have some correlation
between predictors.</li>
<li>But too much correlation can be a really bad thing (how much is too
much is calculated in Chapter 10).</li>
</ul></li>
<li><p>The consequences of multicollinearity:</p>
<ol style="list-style-type: decimal">
<li>Estimates of coefficients change based on what other predictors are
in the model.</li>
<li>Extra sums of squares change based on what other predictors are in
the model.</li>
<li>Standard errors are inflated.</li>
<li>Fits and predictions and MSE are less affected.</li>
<li>The significance of some predictors can be masked.</li>
<li>Interpretation becomes more difficult.</li>
</ol></li>
<li><p>We will demonstrate all of these issues through the body fat
example with variables</p>
<ul>
<li><span class="math inline">\(X_1\)</span> <code>triceps</code>:
Triceps skinfold thickness.</li>
<li><span class="math inline">\(X_2\)</span> <code>thigh</code>: Thigh
circumference.</li>
<li><span class="math inline">\(X_3\)</span> <code>midarm</code>: Midarm
circumference</li>
<li><span class="math inline">\(Y\)</span> <code>fat</code>: Body
fat.</li>
</ul>
<pre class="r"><code>body &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/body.csv&quot;)
glimpse(body)</code></pre>
<pre><code>## Rows: 20
## Columns: 4
## $ triceps &lt;dbl&gt; 19.5, 24.7, 30.7, 29.8, 19.1, 25.6, 31.4, 27.9, 22.1, 25.5, 31‚Ä¶
## $ thigh   &lt;dbl&gt; 43.1, 49.8, 51.9, 54.3, 42.2, 53.9, 58.5, 52.1, 49.9, 53.5, 56‚Ä¶
## $ midarm  &lt;dbl&gt; 29.1, 28.2, 37.0, 31.1, 30.9, 23.7, 27.6, 30.6, 23.2, 24.8, 30‚Ä¶
## $ fat     &lt;dbl&gt; 11.9, 22.8, 18.7, 20.1, 12.9, 21.7, 27.1, 25.4, 21.3, 19.3, 25‚Ä¶</code></pre></li>
<li><p>These data exhibit multicollinearity</p>
<pre class="r"><code>library(GGally)
ggpairs(data = body)</code></pre>
<p><img src="05_mlr_ii_files/figure-html/unnamed-chunk-30-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>Notice that <span class="math inline">\(X_1\)</span> = triceps
and <span class="math inline">\(X_2\)</span> = thigh are highly
correlated with each other, but these are less correlated with <span
class="math inline">\(X_3\)</span> = midarm.</p></li>
<li><p>Let‚Äôs fit a bunch of models</p>
<pre class="r"><code>lm_tr &lt;- lm(fat ~ triceps, data = body)
lm_th &lt;- lm(fat ~ thigh, data = body)
lm_tr_th &lt;- lm(fat ~ triceps + thigh, data = body)
lm_tr_th_m &lt;- lm(fat ~ triceps + thigh + midarm, data = body)</code></pre></li>
</ul>
<div id="coefficient-estimates-change" class="section level2">
<h2>Coefficient Estimates Change</h2>
<pre class="r"><code>select(tidy(lm_tr), term, estimate)</code></pre>
<pre><code>## # A tibble: 2 √ó 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)   -1.50 
## 2 triceps        0.857</code></pre>
<pre class="r"><code>select(tidy(lm_th), term, estimate)</code></pre>
<pre><code>## # A tibble: 2 √ó 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)  -23.6  
## 2 thigh          0.857</code></pre>
<pre class="r"><code>select(tidy(lm_tr_th), term, estimate)</code></pre>
<pre><code>## # A tibble: 3 √ó 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)  -19.2  
## 2 triceps        0.222
## 3 thigh          0.659</code></pre>
<pre class="r"><code>select(tidy(lm_tr_th_m), term, estimate)</code></pre>
<pre><code>## # A tibble: 4 √ó 2
##   term        estimate
##   &lt;chr&gt;          &lt;dbl&gt;
## 1 (Intercept)   117.  
## 2 triceps         4.33
## 3 thigh          -2.86
## 4 midarm         -2.19</code></pre>
<ul>
<li>Coefficient estimates change based on what predictors are in the
model.</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center">Variables in Model</th>
<th align="center"><span
class="math inline">\(\hat{\beta}_1\)</span></th>
<th align="center"><span
class="math inline">\(\hat{\beta}_2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(X_1\)</span></td>
<td align="center">0.86</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(X_2\)</span></td>
<td align="center">-</td>
<td align="center">0.86</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(X_1, X_2\)</span></td>
<td align="center">0.22</td>
<td align="center">0.66</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(X_1, X_2,
X_3\)</span></td>
<td align="center">4.33</td>
<td align="center">-2.86</td>
</tr>
</tbody>
</table>
<ul>
<li><p>üò± Look how much they change!</p></li>
<li><p><strong>Exercise</strong>: What is the model for each row in the
above table?</p></li>
<li><p>We saw that <span class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span> were highly correlated, so it makes
sense that those coefficients change when they are included together in
the model.</p></li>
<li><p>But <span class="math inline">\(X_3\)</span> was not highly
correlated with either <span class="math inline">\(X_1\)</span> nor
<span class="math inline">\(X_2\)</span>, so why did the coefficients
change so much?</p></li>
<li><p>It turns out that, although <span
class="math inline">\(X_3\)</span> is not highly correlated with <span
class="math inline">\(X_1\)</span> and is not highly correlated with
<span class="math inline">\(X_2\)</span>, it is highly correlated with
the combination of <span class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span>.</p></li>
<li><p>We can measure the strength of the association between <span
class="math inline">\(X_3\)</span> and the combination of <span
class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span> by the multiple <span
class="math inline">\(R^2\)</span> value of regression of <span
class="math inline">\(X_3\)</span> on <span
class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_2\)</span></p>
<pre class="r"><code>glance(lm(midarm ~ triceps + thigh, data = body))$r.squared</code></pre>
<pre><code>## [1] 0.9904</code></pre></li>
<li><p>So, you cannot detect multicollinearity by just looking at
pairwise correlations between predictors.</p>
<ul>
<li>Chapter 10 provides more sophisticated measures of
multicollinearity.</li>
</ul></li>
<li><p>Key Takeaway: a regression coefficient does not reflect any
inherent effect of a particular predictor on the response, but only a
marginal or partial effect given whatever other predictors are in the
model.</p></li>
</ul>
</div>
<div id="extra-sums-of-squares-change" class="section level2">
<h2>Extra Sums of Squares Change</h2>
<pre class="r"><code>library(car)
Anova(lm_tr)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: fat
##           Sum Sq Df F value Pr(&gt;F)
## triceps      352  1    44.3  3e-06
## Residuals    143 18</code></pre>
<pre class="r"><code>Anova(lm_tr_th)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: fat
##           Sum Sq Df F value Pr(&gt;F)
## triceps      3.5  1    0.54  0.474
## thigh       33.2  1    5.13  0.037
## Residuals  110.0 17</code></pre>
<pre class="r"><code>Anova(lm_tr_th_m)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: fat
##           Sum Sq Df F value Pr(&gt;F)
## triceps     12.7  1    2.07   0.17
## thigh        7.5  1    1.22   0.28
## midarm      11.5  1    1.88   0.19
## Residuals   98.4 16</code></pre>
<ul>
<li><p>Extra sums of squares will differ based on what other predictors
are in the model</p>
<ul>
<li><span class="math inline">\(SSR(X_1) = 352\)</span></li>
<li><span class="math inline">\(SSR(X_1|X_2) = 3.5\)</span></li>
<li><span class="math inline">\(SSR(X_1|X_2, X_3) = 12.7\)</span></li>
</ul></li>
<li><p>Recall, the extra sum of squares is how much the error sum of
squares is reduced when we include a predictor in a model with other
predictors already in the model.</p></li>
<li><p>The extra sum of squares can increase or decrease depending on
what other variables are in the model.</p></li>
<li><p>Key Takeaway: there is no unique sum of squares that can be
ascribed to any one predictor as reflecting its effect in reducing the
total variation in <span class="math inline">\(Y\)</span> ‚Äî this depends
on what other predictors are in the model.</p></li>
</ul>
</div>
<div id="standard-errors-are-larger" class="section level2">
<h2>Standard Errors are Larger</h2>
<pre class="r"><code>select(tidy(lm_tr), term, std.error)</code></pre>
<pre><code>## # A tibble: 2 √ó 2
##   term        std.error
##   &lt;chr&gt;           &lt;dbl&gt;
## 1 (Intercept)     3.32 
## 2 triceps         0.129</code></pre>
<pre class="r"><code>select(tidy(lm_th), term, std.error)</code></pre>
<pre><code>## # A tibble: 2 √ó 2
##   term        std.error
##   &lt;chr&gt;           &lt;dbl&gt;
## 1 (Intercept)     5.66 
## 2 thigh           0.110</code></pre>
<pre class="r"><code>select(tidy(lm_tr_th), term, std.error)</code></pre>
<pre><code>## # A tibble: 3 √ó 2
##   term        std.error
##   &lt;chr&gt;           &lt;dbl&gt;
## 1 (Intercept)     8.36 
## 2 triceps         0.303
## 3 thigh           0.291</code></pre>
<pre class="r"><code>select(tidy(lm_tr_th_m), term, std.error)</code></pre>
<pre><code>## # A tibble: 4 √ó 2
##   term        std.error
##   &lt;chr&gt;           &lt;dbl&gt;
## 1 (Intercept)     99.8 
## 2 triceps          3.02
## 3 thigh            2.58
## 4 midarm           1.60</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">Variables in Model</th>
<th align="center"><span
class="math inline">\(s\{\hat{\beta}_1\}\)</span></th>
<th align="center"><span
class="math inline">\(s\{\hat{\beta}_2\}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(X_1\)</span></td>
<td align="center">0.13</td>
<td align="center">-</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(X_2\)</span></td>
<td align="center">-</td>
<td align="center">0.11</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(X_1, X_2\)</span></td>
<td align="center">0.30</td>
<td align="center">0.29</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(X_1, X_2,
X_3\)</span></td>
<td align="center">3.02</td>
<td align="center">2.58</td>
</tr>
</tbody>
</table>
<ul>
<li><p>Standard error increases as you include correlated predictors in
the model.</p></li>
<li><p>Why? Consider the case of perfectly correlated variables where
<span class="math inline">\(X_{i1} = X_{i2}\)</span> for all <span
class="math inline">\(i\)</span>. Then for an number <span
class="math inline">\(a\)</span> <span
class="math display">\[\begin{align}
  \hat{Y}_i &amp;= \hat{\beta}_0 + \hat{\beta}_1X_{i1} +
\hat{\beta}_2X_{i2}\\
  &amp;= \hat{\beta}_0 + (\hat{\beta}_1-a)X_{i1} + (\hat{\beta}_2 +
a)X_{i2}
  \end{align}\]</span></p>
<ul>
<li><p>So it is impossible to tell <span
class="math inline">\(\hat{\beta}_1\)</span> and <span
class="math inline">\(\hat{\beta}_2\)</span> apart.</p></li>
<li><p>We don‚Äôt know what effect to attribute to <span
class="math inline">\(X_1\)</span> and what to attribute to <span
class="math inline">\(X_2\)</span>.</p></li>
</ul></li>
</ul>
</div>
<div id="fitted-values-predictions-and-mse-are-relatively-stable"
class="section level2">
<h2>Fitted Values, Predictions, and MSE are Relatively Stable</h2>
<ul>
<li><p>The major issue with multicollinearity is that we can‚Äôt tell what
effect comes from which predictor.</p></li>
<li><p>But the effect of multicollinearity on fitted values,
predictions, and MSE is much more modest.</p></li>
<li><p>Confidence intervals for mean have about the same width
(predictor levels chosen to have about the same mean estimate in each
model)</p>
<pre class="r"><code>df1 &lt;- data.frame(triceps = 25)
df2 &lt;- data.frame(triceps = 25, thigh = 50)
df3 &lt;- data.frame(triceps = 25, thigh = 50, midarm = 28.8)
predict(lm_tr, newdata = df1, interval = &quot;confidence&quot;)</code></pre>
<pre><code>##     fit   lwr   upr
## 1 19.93 18.61 21.26</code></pre>
<pre class="r"><code>predict(lm_tr_th, newdata = df2, interval = &quot;confidence&quot;)</code></pre>
<pre><code>##     fit   lwr   upr
## 1 19.36 18.04 20.67</code></pre>
<pre class="r"><code>predict(lm_tr_th_m, newdata = df3, interval = &quot;confidence&quot;)</code></pre>
<pre><code>##     fit   lwr upr
## 1 19.64 18.27  21</code></pre></li>
<li><p>Prediction intervals have about the same width</p>
<pre class="r"><code>predict(lm_tr, newdata = df1, interval = &quot;prediction&quot;)</code></pre>
<pre><code>##     fit   lwr upr
## 1 19.93 13.86  26</code></pre>
<pre class="r"><code>predict(lm_tr_th, newdata = df2, interval = &quot;prediction&quot;)</code></pre>
<pre><code>##     fit   lwr   upr
## 1 19.36 13.83 24.88</code></pre>
<pre class="r"><code>predict(lm_tr_th_m, newdata = df3, interval = &quot;prediction&quot;)</code></pre>
<pre><code>##     fit   lwr   upr
## 1 19.64 14.21 25.07</code></pre></li>
<li><p>Residual standard deviation (square root of MSE) is about the
same in all models</p>
<pre class="r"><code>glance(lm_tr)$sigma</code></pre>
<pre><code>## [1] 2.82</code></pre>
<pre class="r"><code>glance(lm_tr_th)$sigma</code></pre>
<pre><code>## [1] 2.543</code></pre>
<pre class="r"><code>glance(lm_tr_th_m)$sigma</code></pre>
<pre><code>## [1] 2.48</code></pre></li>
<li><p>Why? Consider the case of perfectly correlated variables where
<span class="math inline">\(X_{i1} = X_{i2}\)</span> for all <span
class="math inline">\(i\)</span>. Then for an number <span
class="math inline">\(a\)</span> <span
class="math display">\[\begin{align}
  \hat{Y}_i &amp;= \hat{\beta}_0 + \hat{\beta}_1X_{i1} +
\hat{\beta}_2X_{i2}\\
  &amp;= \hat{\beta}_0 + (\hat{\beta}_1-a)X_{i1} + (\hat{\beta}_2 +
a)X_{i2}
  \end{align}\]</span></p>
<ul>
<li><p>The estimated mean value does not change no matter what value of
<span class="math inline">\(a\)</span> is provided (so predictions would
also be robust to change).</p></li>
<li><p>This means the residuals would also not change no matter what
value of <span class="math inline">\(a\)</span> is provided (so the MSE
would be more robust to change).</p></li>
</ul></li>
<li><p>This is not to say that you should just include every predictor
in the model. This is called ‚Äúoverfitting‚Äù and can cause bad issues
(machine learning will discuss this).</p>
<ul>
<li>The point is just that predictions are more <em>robust</em> to
issues of multicollinearity.</li>
</ul></li>
</ul>
</div>
<div id="masking-significance" class="section level2">
<h2>Masking Significance</h2>
<ul>
<li><p>The <span class="math inline">\(p\)</span>-values for all three
variables are all large, indicating a lack of evidence to include any of
them.</p>
<pre class="r"><code>tidy(lm_tr_th_m)</code></pre>
<pre><code>## # A tibble: 4 √ó 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)   117.       99.8       1.17   0.258
## 2 triceps         4.33      3.02      1.44   0.170
## 3 thigh          -2.86      2.58     -1.11   0.285
## 4 midarm         -2.19      1.60     -1.37   0.190</code></pre></li>
<li><p>But the overall <span class="math inline">\(F\)</span>-test
indicates strong evidence that we should include at least one of
them:</p>
<pre class="r"><code>glance(lm_tr_th_m)$p.value</code></pre>
<pre><code>##     value 
## 7.343e-06</code></pre></li>
<li><p>The <span class="math inline">\(t\)</span>-test for <span
class="math inline">\(H_0: \beta_1 = 0\)</span> compares the two
models:</p>
<ul>
<li><span class="math inline">\(H_0: Y_i = \beta_0 + \beta_2X_{i2} +
\beta_3X_{i3} + \epsilon_i\)</span></li>
<li><span class="math inline">\(H_A: Y_i = \beta_0 + \beta_1X_{i1} +
\beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i\)</span></li>
</ul>
<p>and if <span class="math inline">\(X_2\)</span> is highly correlated
with <span class="math inline">\(X_1\)</span>, and <span
class="math inline">\(X_2\)</span> is already in the model, then why do
we also need to have <span class="math inline">\(X_1\)</span> in the
model?</p></li>
<li><p>The overall <span class="math inline">\(F\)</span>-test compares
the two models:</p>
<ul>
<li><span class="math inline">\(H_0: Y_i = \beta_0 +
\epsilon_i\)</span></li>
<li><span class="math inline">\(H_A: Y_i = \beta_0 + \beta_1X_{i1} +
\beta_2X_{i2} + \beta_3X_{i3} + \epsilon_i\)</span></li>
</ul>
<p>So this test says that we should have at least one of these variables
in the model.</p></li>
<li><p>Key point: Just because the <span
class="math inline">\(p\)</span>-values from the <span
class="math inline">\(t\)</span>-tests are large does <em>not</em> mean
that none of the predictors are associated with <span
class="math inline">\(Y\)</span>.</p></li>
</ul>
</div>
<div id="the-difficulty-of-interpretation" class="section level2">
<h2>The Difficulty of Interpretation</h2>
<ul>
<li><p>Sometimes, no data points exist that are one unit larger <span
class="math inline">\(X_1\)</span> but the same <span
class="math inline">\(X_2\)</span>, because of multicollinearity. Thus,
this interpretation is suspect.</p></li>
<li><p><strong>Example</strong>: Researchers were interested in studying
the association between energy expenditure (<span
class="math inline">\(Y\)</span>) on body mass (<span
class="math inline">\(X_1\)</span>) and echolocation type. Echolocation
type is a categorical variable with levels ‚Äúnon-echolocating bats‚Äù,
‚Äúnon-echolocating birds‚Äù, and ‚Äúecholocating bats‚Äù. The idea is that
echolocation consumes a lot of energy, and so researchers were
interested in if evolution found a way to correct for this, so that the
sum of energy expenditure was not higher in echolocating bats than what
would be expected based on their mass. You can load these data into R
with</p>
<pre class="r"><code>library(Sleuth3)
data(&quot;case1002&quot;)
echo &lt;- case1002
glimpse(echo)</code></pre>
<pre><code>## Rows: 20
## Columns: 3
## $ Mass   &lt;dbl&gt; 779.0, 628.0, 258.0, 315.0, 24.3, 35.0, 72.8, 120.0, 213.0, 275‚Ä¶
## $ Type   &lt;fct&gt; non-echolocating bats, non-echolocating bats, non-echolocating ‚Ä¶
## $ Energy &lt;dbl&gt; 43.70, 34.80, 23.30, 22.40, 2.46, 3.93, 9.15, 13.80, 14.60, 22.‚Ä¶</code></pre></li>
<li><p>Since echolocation type is a categorical variable, we define two
new indicator variables</p>
<p><span class="math display">\[\begin{align}
  X_{i2} &amp;= \begin{cases}
  1 &amp; \text{ if individual $i$ is a non-echolocating bird}\\
  0 &amp; \text{ otherwise}
  \end{cases}\\
  X_{i3} &amp;= \begin{cases}
  1 &amp; \text{ if individual $i$ is an echolocating bat}\\
  0 &amp; \text{ otherwise}
  \end{cases}
  \end{align}\]</span></p></li>
<li><p>It turns out that the best model is one that includes a
log-transformation on both energy and mass.</p>
<pre class="r"><code>echo &lt;- mutate(echo, l_energy = log(Energy), l_mass = log(Mass))
echo &lt;- mutate(echo, Type = fct_relevel(Type, &quot;non-echolocating bats&quot;, &quot;non-echolocating birds&quot;, &quot;echolocating bats&quot;))</code></pre>
<p>The last line above changes the order of the factor levels so that
non-echolocating bats are the reference level.</p></li>
<li><p>We then fit the model</p>
<p><span class="math display">\[
  \log(Y_i) = \beta_0 + \beta_1\log(X_{i1}) + \beta_2X_{i2} +
\beta_3X_{i3} + \epsilon_i
  \]</span></p>
<pre class="r"><code>lm_echo &lt;- lm(l_energy ~ l_mass + Type, data = echo)
tidy(lm_echo, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 4 √ó 7
##   term                       estimate std.error stati‚Ä¶¬π  p.value conf.‚Ä¶¬≤ conf.‚Ä¶¬≥
##   &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)                 -1.58      0.287   -5.49  4.96e- 5  -2.19   -0.967
## 2 l_mass                       0.815     0.0445  18.3   3.76e-12   0.721   0.909
## 3 Typenon-echolocating birds   0.102     0.114    0.896 3.84e- 1  -0.140   0.344
## 4 Typeecholocating bats        0.0787    0.203    0.388 7.03e- 1  -0.351   0.508
## # ‚Ä¶ with abbreviated variable names ¬π‚Äãstatistic, ¬≤‚Äãconf.low, ¬≥‚Äãconf.high</code></pre></li>
<li><p>So the fit is</p>
<p><span class="math display">\[
  \hat{Y} = -1.58 + 0.82 X_1 + 0.11X_2 + 0.08X_3
  \]</span></p></li>
<li><p>How do we interpret the 0.08 coefficient? Typically, we would say
that ‚Äúecholocating bats expend about 8.3% more energy (<span
class="math inline">\(e^{0.08}\)</span> = 1.083) than non-echolocating
bats of about the same mass.‚Äù</p></li>
<li><p><strong>However</strong>, there are no echolocating bats and
non-echolocating bats of the same mass.</p>
<pre class="r"><code>echo %&gt;%
  filter(Type != &quot;non-echolocating birds&quot;) %&gt;%
  qplot(x = Type, y = l_mass, data = ., ylab = &quot;log(Mass)&quot;)</code></pre>
<p><img src="05_mlr_ii_files/figure-html/unnamed-chunk-45-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>This is an example of multicollinearity.</p>
<p>So this interpretation is suspect.</p></li>
<li><p>The best interpretation is to just say that we are adjusting for
body size. E.g. ‚Äúecholocating bats expend about 8.3% more energy (<span
class="math inline">\(e^{0.08}\)</span> = 1.083) than non-echolocating
bats, adjusting for body size‚Äù.</p></li>
<li><p>What this means is that group differences are considered only
after the best explanation for body size is taken into account.</p></li>
</ul>
</div>
<div id="remedies-for-multicollinearity" class="section level2">
<h2>Remedies for Multicollinearity</h2>
<ol style="list-style-type: decimal">
<li><p>If it is not extreme (Section 10.5 of KNNL), don‚Äôt do anything.
Just be cognizant of it.</p></li>
<li><p>Drop some of the correlated predictor variables.</p></li>
<li><p>Develop ‚Äúcomposite‚Äù predictors that combine the correlated
predictors.</p>
<ul>
<li>Do this manually.</li>
<li>Use <a
href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal
component analysis</a> (PCA).</li>
</ul></li>
<li><p>Apply <a
href="https://en.wikipedia.org/wiki/Ridge_regression">ridge
regression</a> (Chapter 10).</p></li>
</ol>
</div>
<div
id="ok-that-sucked-but-what-about-when-there-is-no-multicollinearity"
class="section level2">
<h2>OK, that sucked, but what about when there is no
multicollinearity?</h2>
<ul>
<li><p>Researchers were studying the effect of work crew size (<span
class="math inline">\(X_1\)</span>) and level of bonus pay (<span
class="math inline">\(X_2\)</span>) on crew productivity (<span
class="math inline">\(Y\)</span>).</p>
<pre class="r"><code>crew &lt;- tibble::tribble(
  ~size, ~bonus, ~productivity,
      4,      2,            42,
      4,      2,            39,
      4,      3,            48,
      4,      3,            51,
      6,      2,            49,
      6,      2,            53,
      6,      3,            61,
      6,      3,            60
  )</code></pre></li>
<li><p><code>size</code> and <code>bonus</code> are both uncorrelated,
by design.</p>
<pre class="r"><code>cor(crew$size, crew$bonus)</code></pre>
<pre><code>## [1] 0</code></pre></li>
<li><p>Let‚Äôs fit these models:</p>
<pre class="r"><code>lm_s &lt;- lm(productivity ~ size, data = crew)
lm_b &lt;- lm(productivity ~ bonus, data = crew)
lm_bs &lt;- lm(productivity ~ bonus + size, data = crew)</code></pre></li>
<li><p>Notice that the estimated coefficients are unchanged when we add
more predictors.</p>
<pre class="r"><code>tidy(lm_s)</code></pre>
<pre><code>## # A tibble: 2 √ó 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)    23.5      10.1       2.32  0.0591
## 2 size            5.37      1.98      2.71  0.0351</code></pre>
<pre class="r"><code>tidy(lm_b)</code></pre>
<pre><code>## # A tibble: 2 √ó 5
##   term        estimate std.error statistic p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
## 1 (Intercept)    27.3      11.6       2.35  0.0572
## 2 bonus           9.25      4.55      2.03  0.0885</code></pre>
<pre class="r"><code>tidy(lm_bs)</code></pre>
<pre><code>## # A tibble: 3 √ó 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    0.375     4.74     0.0791 0.940   
## 2 bonus          9.25      1.33     6.97   0.000937
## 3 size           5.37      0.664    8.10   0.000466</code></pre></li>
<li><p>The interpretation of the coefficients still changes based on
what variables are in the model (controlling for the other predictor),
but the effect size is estimated to be the same regardless.</p></li>
<li><p>Did you also notice that the standard errors did not get
inflated?</p></li>
<li><p>The extra sums of squares are the same. That is <span
class="math display">\[
  SSR(X_1) = SSR(X_1|X_2)\\
  SSR(X_2) = SSR(X_2|X_1)
  \]</span></p>
<p>so the relative reduction in the variation in <span
class="math inline">\(Y\)</span> by including <span
class="math inline">\(X_1\)</span> is the same no matter if <span
class="math inline">\(X_2\)</span> is in the model.</p>
<pre class="r"><code>Anova(lm_b)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: productivity
##           Sum Sq Df F value Pr(&gt;F)
## bonus        171  1    4.13  0.088
## Residuals    249  6</code></pre>
<pre class="r"><code>Anova(lm_s)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: productivity
##           Sum Sq Df F value Pr(&gt;F)
## size         231  1    7.35  0.035
## Residuals    189  6</code></pre>
<pre class="r"><code>Anova(lm_bs)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: productivity
##           Sum Sq Df F value  Pr(&gt;F)
## bonus      171.1  1    48.5 0.00094
## size       231.1  1    65.6 0.00047
## Residuals   17.6  5</code></pre></li>
<li><p>Thus, it is relatively straightforward to determine if a variable
should be included in a model, because this question may be answered
without looking at other variables.</p></li>
<li><p>Conclusion: When designing an experiment, make sure your
predictors are all uncorrelated.</p></li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
