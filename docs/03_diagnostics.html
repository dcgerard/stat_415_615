<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2021-08-05" />

<title>Diagnostics and Remedial Measures</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Diagnostics and Remedial Measures</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2021-08-05</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Chapter 3 of KNNL (skip Sections 3.4–3.6 and the Box-Cox transformation).</li>
<li>Interpreting residuals plots.</li>
<li>Diagnosing violations in the assumptions of the simple linear model.</li>
<li>Suggesting solutions to remedy the violations.</li>
</ul>
</div>
<div id="recall-assumptions-in-decreasing-order-of-importance" class="section level1">
<h1>Recall: Assumptions in Decreasing Order of Importance</h1>
<ol style="list-style-type: decimal">
<li><p><strong>Linearity</strong> - Does the relationship look like a straight line?</p></li>
<li><p><strong>Independence</strong> - knowledge of the value of one observation does not give you any information on the value of another.</p></li>
<li><p><strong>Equal Variance</strong> - The spread is the same for every value of <span class="math inline">\(x\)</span></p></li>
<li><p><strong>Normality</strong> - The distribution isn’t too skewed and there aren’t any too extreme points. (only an issue if you have outliers and a small number of observations, or if you are doing prediction intervals).</p></li>
</ol>
</div>
<div id="problems-when-violated" class="section level1">
<h1>Problems when Violated</h1>
<ol style="list-style-type: decimal">
<li><p><strong>Linearity</strong> - Linear regression line does not pick up actual relationship</p></li>
<li><p><strong>Independence</strong> - Linear regression line is unbiased, but standard errors are off.</p></li>
<li><p><strong>Equal Variance</strong> - Linear regression line is unbiased, but standard errors are off.</p></li>
<li><p><strong>Normality</strong> - Unstable results if outliers are present and sample size is small.</p></li>
</ol>
<div id="assessment-tools-scatterplots-and-residual-plots" class="section level2">
<h2>Assessment Tools: Scatterplots and Residual Plots</h2>
<ul>
<li><p>Make a scatterplot of the explanatory variable (<span class="math inline">\(x\)</span>-axis) vs the response (<span class="math inline">\(y\)</span>-axis) to check for non-linearity, equal variance, and normality violations.</p></li>
<li><p>Residuals (<span class="math inline">\(y\)</span>-axis) vs fitted values (<span class="math inline">\(x\)</span>-axis) is often more clear because the signal is removed.</p></li>
<li><p>You should get used to residual plots, because they are necessary when we start doing multiple linear regression</p>
<ul>
<li><span class="math inline">\(x\)</span> versus <span class="math inline">\(y\)</span> does not work well as a diagnostic device when you have many <span class="math inline">\(x\)</span>’s.</li>
</ul></li>
</ul>
</div>
</div>
<div id="dataset-1-gold-standard" class="section level1">
<h1>Dataset 1: Gold Standard</h1>
<div id="dataset-1-scatterplot" class="section level2">
<h2>Dataset 1: Scatterplot</h2>
<pre class="r"><code>library(tidyverse)
library(broom)
x &lt;- runif(100, -3, 3)
y &lt;- x + rnorm(100)
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x = x, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-1-residual-plot" class="section level2">
<h2>Dataset 1: Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-1-summary" class="section level2">
<h2>Dataset 1: Summary</h2>
<ul>
<li><p>Means are straight lines</p></li>
<li><p>Residuals seem to be centered at 0 for all <span class="math inline">\(x\)</span></p></li>
<li><p>Variance looks equal for all <span class="math inline">\(x\)</span></p></li>
<li><p>Everything looks perfect</p></li>
<li><p><strong>Exercise</strong>: Describe any issues with this plot. <img src="03_diagnostics_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
</div>
<div id="dataset-2-curved-monotone-relationship-equal-variances" class="section level1">
<h1>Dataset 2: Curved Monotone Relationship, Equal Variances</h1>
<div id="dataset-2-scatterplot" class="section level2">
<h2>Dataset 2: Scatterplot</h2>
<pre class="r"><code>x &lt;- runif(100, 0, 6)
x &lt;- x - min(x) + 0.5
y &lt;- log(x) * 20 + rnorm(100, sd = 4)
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x = x, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-2-residual-plot" class="section level2">
<h2>Dataset 2: Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-2-summary" class="section level2">
<h2>Dataset 2: Summary</h2>
<ul>
<li><p>Curved (but always increasing <em>or</em> always decreasing) relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p></li>
<li><p>Variance looks equal for all <span class="math inline">\(x\)</span></p></li>
<li><p>Residual plot has a parabolic shape.</p></li>
<li><p>These indicate a <span class="math inline">\(\log\)</span> transformation of <span class="math inline">\(x\)</span> could help.</p></li>
<li><p>Why not <span class="math inline">\(\log(y)\)</span>? Because taking transforming <span class="math inline">\(y\)</span> can change the variance, and we already have constant variance, so we do not want to mess with that.</p>
<ul>
<li>Recall, random variation occurs in the <span class="math inline">\(y\)</span> direction, not the <span class="math inline">\(x\)</span> direction.</li>
</ul></li>
<li><p><strong>Exercise</strong>: Consider the following data:</p>
<pre class="r"><code>dftoy &lt;- tribble(~x, ~y,
                 1, 1, 
                 1, 2, 
                 2, 1, 
                 2, 5,
                 3, 1,
                 3, 20)</code></pre>
<p>Plot <span class="math inline">\(x\)</span> versus <span class="math inline">\(y\)</span>. Then log-transform <span class="math inline">\(x\)</span> and plot <span class="math inline">\(\log(x)\)</span> versus <span class="math inline">\(y\)</span>. Then try log-transforming <span class="math inline">\(y\)</span> and plotting <span class="math inline">\(x\)</span> versus <span class="math inline">\(\log(y)\)</span>. Discuss how variation changes (vertically or horizontally) when we log <span class="math inline">\(x\)</span> versus logging <span class="math inline">\(y\)</span>.</p></li>
</ul>
</div>
<div id="dataset-2-transformed-x-scatterplot" class="section level2">
<h2>Dataset 2: Transformed <span class="math inline">\(x\)</span> Scatterplot</h2>
<pre class="r"><code>df &lt;- mutate(df, x_log = log(x))
qplot(x = x_log, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-2-transformed-x-residual-plot" class="section level2">
<h2>Dataset 2: Transformed <span class="math inline">\(x\)</span> Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x_log, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="dataset-3-curved-non-monotone-relationship-equal-variances" class="section level1">
<h1>Dataset 3: Curved Non-monotone Relationship, Equal Variances</h1>
<div id="dataset-3-scatterplot" class="section level2">
<h2>Dataset 3: Scatterplot</h2>
<pre class="r"><code>x &lt;- runif(100, -3, 3)
y &lt;- -x^2 + rnorm(100)
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x = x, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-3-residual-plot" class="section level2">
<h2>Dataset 3: Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-3-summary" class="section level2">
<h2>Dataset 3: Summary</h2>
<ul>
<li><p>Curved relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></p></li>
<li><p>Sometimes the relationship is increasing, sometimes it is decreasing.</p></li>
<li><p>Variance looks equal for all <span class="math inline">\(x\)</span></p></li>
<li><p>Residual plot has a parabolic form.</p></li>
</ul>
</div>
<div id="dataset-3-solution" class="section level2">
<h2>Dataset 3: Solution</h2>
<ul>
<li><p>Fit model:</p>
<p><span class="math display">\[
  E[Y_i] = \beta_0 + \beta_1 X_i + \beta_2 X_i^2
  \]</span></p></li>
<li><p>A more complicated solution (which we will not cover) would be to fit model <span class="math display">\[
  Y_i = 
  \begin{cases}
  \beta_0 + \beta_1 X_i &amp; \text{ if } X_i &lt; C\\
  \beta_0^* + \beta_1^* X_i &amp; \text{ if } X_i &gt; C\\
  \end{cases}
  \]</span></p></li>
</ul>
</div>
<div id="dataset-3-fitting-ey_i-beta_0-beta_1-x_i-beta_2-x_i2" class="section level2">
<h2>Dataset 3: Fitting <span class="math inline">\(E[Y_i] = \beta_0 + \beta_1 X_i + \beta_2 X_i^2\)</span></h2>
<pre class="r"><code>df &lt;- mutate(df, x2 = x^2) ## create x^2 first
quad_lm &lt;- lm(y ~ x + x2, data = df) ## lm of x^2 + x
aout &lt;- augment(quad_lm)
qplot(x = x, y = y, data = aout) +
  geom_line(mapping = aes(x = x, y = .fitted), col = &quot;blue&quot;, lwd = 1)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-3-solution-1-residuals" class="section level2">
<h2>Dataset 3: Solution 1 Residuals</h2>
<pre class="r"><code>qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-3-peak-estimation" class="section level2">
<h2>Dataset 3: Peak estimation</h2>
<pre class="r"><code>ttemp &lt;- tidy(quad_lm)
beta0 &lt;- ttemp$estimate[[1]]
beta1 &lt;- ttemp$estimate[[2]]
beta2 &lt;- ttemp$estimate[[3]]</code></pre>
<ul>
<li><p>Recall that <span class="math inline">\(y = \beta_0 + \beta_1x + \beta_2x^2\)</span> is the equation of a parabola.</p></li>
<li><p>The estimated parabola is <span class="math inline">\(y = 0.2756 + -0.0248 x + -1.0802 x^2\)</span></p></li>
<li><p>We can get these coefficient estimates by running</p>
<pre class="r"><code>tidy(quad_lm)</code></pre>
<pre><code>## # A tibble: 3 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)   0.276     0.168      1.64  1.04e- 1
## 2 x            -0.0248    0.0572    -0.434 6.65e- 1
## 3 x2           -1.08      0.0361   -30.0   8.09e-51</code></pre></li>
<li><p>The negative coefficient for <span class="math inline">\(x^2\)</span> indicates that the parabola has a maximum, which we can recall occurs at <span class="math display">\[
  x = -\frac{\beta_1}{2\beta_2} = -\frac{-0.0248}{2 \times -1.0802} = -0.0115
  \]</span> So we could say that the trend increases to about 0, then decreases after 0.</p></li>
</ul>
</div>
</div>
<div id="dataset-4-curved-monotone-relationship-variance-increases-with-y" class="section level1">
<h1>Dataset 4: Curved Monotone Relationship, Variance Increases with <span class="math inline">\(Y\)</span></h1>
<div id="dataset-4-scatterplot" class="section level2">
<h2>Dataset 4: Scatterplot</h2>
<pre class="r"><code>x &lt;- runif(100, 0, 2)
y &lt;- exp(x + rnorm(100, sd = 1/2))
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x = x, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-4-residual-plot" class="section level2">
<h2>Dataset 4: Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-4-summary" class="section level2">
<h2>Dataset 4: Summary</h2>
<ul>
<li><p>Curved relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></p></li>
<li><p>Variance looks like it increases as <span class="math inline">\(y\)</span> increases</p></li>
<li><p>Residual plot has a parabolic form.</p></li>
<li><p>Residual plot variance looks larger to the right and smaller to the left.</p></li>
</ul>
</div>
<div id="dataset-4-solution" class="section level2">
<h2>Dataset 4: Solution</h2>
<ul>
<li><p>Take a log-transformation of <span class="math inline">\(y\)</span>.</p>
<pre class="r"><code>df &lt;- mutate(df, y_log = log(y))
qplot(x = x, y = y_log, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
<div id="dataset-4-solution-1" class="section level2">
<h2>Dataset 4: Solution</h2>
<pre class="r"><code>lmout &lt;- lm(y_log ~ x, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<ul>
<li><strong>Exercise</strong>: What if you see something like this? Do you think logging <span class="math inline">\(y\)</span> will help? <img src="03_diagnostics_files/figure-html/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></li>
</ul>
</div>
</div>
<div id="dataset-5-linear-relationship-equal-variances-skewed-distribution" class="section level1">
<h1>Dataset 5: Linear Relationship, Equal Variances, Skewed Distribution</h1>
<div id="dataset-5-scatterplot" class="section level2">
<h2>Dataset 5: Scatterplot</h2>
<pre class="r"><code>x &lt;- runif(200)
y &lt;- 15 * x + rexp(200, 0.2)
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x, y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-30-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-5-residual-plot" class="section level2">
<h2>Dataset 5: Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-5-summary" class="section level2">
<h2>Dataset 5: Summary</h2>
<ul>
<li><p>Straight line relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p></li>
<li><p>Variances about equal for all <span class="math inline">\(x\)</span></p></li>
<li><p>Skew for all <span class="math inline">\(x\)</span></p></li>
<li><p>Residual plots show skew.</p></li>
</ul>
</div>
<div id="dataset-5-solution" class="section level2">
<h2>Dataset 5: Solution</h2>
<ul>
<li><p>Do nothing, but report skew (usually OK to do)</p></li>
<li><p>Be fancy, fit quantile regression:</p></li>
</ul>
<p><span class="math display">\[
Median(Y_i) = \beta_0 + \beta_1 X_i
\]</span></p>
<ul>
<li>Be fancy, run a bootstrap (maybe we’ll talk about this later).</li>
</ul>
</div>
</div>
<div id="dataset-6-linear-relationship-unequal-variances" class="section level1">
<h1>Dataset 6: Linear Relationship, Unequal Variances</h1>
<div id="dataset-6-scatterplot" class="section level2">
<h2>Dataset 6: Scatterplot</h2>
<pre class="r"><code>x &lt;- runif(100)
y &lt;- x + rnorm(100, sd = (x + 0.3)^2 / 2)
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x = x, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-33-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-6-residual-plot" class="section level2">
<h2>Dataset 6: Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = lmout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-34-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="dataset-6-summary" class="section level2">
<h2>Dataset 6: Summary</h2>
<ul>
<li><p>Linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p></li>
<li><p>Variance is different for different values of <span class="math inline">\(x\)</span>. This is called <strong>heteroscedasticity</strong>.</p></li>
<li><p>Residual plots really good at showing this.</p></li>
</ul>
</div>
<div id="dataset-6-solution" class="section level2">
<h2>Dataset 6: Solution</h2>
<ul>
<li><p>You can try logging both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, sometimes that works. But that won’t work here. Be careful about negative values.</p>
<pre class="r"><code>df &lt;- mutate(df, y_log = log(y + 0.5),
             x_log = log(x))</code></pre>
<pre><code>## Warning in log(y + 0.5): NaNs produced</code></pre>
<pre class="r"><code>qplot(x = x_log, y = y_log, data = df)</code></pre>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_point).</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-35-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>The book will suggest weighted least squares (with weights inverse to the variance).</p></li>
<li><p>But the modern solution is to use <strong>sandwich</strong> estimates of the standard errors.</p>
<pre class="r"><code>library(lmtest)
library(sandwich)
cout &lt;- coeftest(x = lmout, vcov. = vcovHC(x = lmout))
tidy(cout, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 2 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   0.0124    0.0454     0.273 7.86e- 1  -0.0778     0.103
## 2 x             1.03      0.150      6.85  6.55e-10   0.731      1.33</code></pre></li>
<li><p><code>vcovHC()</code> stands for “Heteroscedastic-consistent variance/covariance”.</p></li>
<li><p>Compare to old standard errors</p>
<pre class="r"><code>tidy(lmout, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 2 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   0.0124    0.0703     0.176 8.61e- 1   -0.127     0.152
## 2 x             1.03      0.130      7.91  3.85e-12    0.772     1.29</code></pre></li>
</ul>
</div>
<div id="intuition-of-sandwich-estimator-of-variance" class="section level2">
<h2>Intuition of Sandwich Estimator of Variance</h2>
<ul>
<li><p>Simplified Model: <span class="math inline">\(Y_i = \beta_1 x_i\)</span> (so zero intercept)</p></li>
<li><p>Using Calculus: <span class="math inline">\(\hat{\beta}_1 = \frac{\sum_{i = 1}^n x_i y_i}{\sum_{i = 1}^n x_i^2}\)</span></p></li>
</ul>
<p>So <span class="math display">\[\begin{align*}
Var(\hat{\beta}_1) &amp;= Var\left(\frac{\sum_{i = 1}^n x_i y_i}{\sum_{i = 1}^n x_i^2}\right)\\
&amp;=\frac{\sum_{i = 1}^n x_i^2 Var(y_i|x_i)}{\left(\sum_{i = 1}^n x_i^2\right)^2}
\end{align*}\]</span></p>
<ul>
<li><p>Usual Method: Estimate <span class="math inline">\(Var(y_i|x_i) = \sigma^2\)</span> with the MSE.</p>
<ul>
<li>Assumes variance estimate is same for all <span class="math inline">\(i\)</span></li>
</ul></li>
<li><p>Sandwich Method: Estimate <span class="math inline">\(Var(y_i|x_i)\)</span> with <span class="math inline">\((y_i - \hat{\beta}_1x_i)^2\)</span></p>
<ul>
<li>Allows variance estimate to differ at each <span class="math inline">\(i\)</span></li>
</ul></li>
</ul>
</div>
<div id="notes-on-sandwich" class="section level2">
<h2>Notes on Sandwich</h2>
<ul>
<li><p>They result in accurate standard errors of the coefficient estimates as long as</p>
<ol style="list-style-type: decimal">
<li><p>The linearity assumption is satisfied.</p></li>
<li><p>Independence is satisfied</p></li>
<li><p>You have a large enough sample size.</p></li>
</ol></li>
<li><p>You cannot use them for prediction intervals</p></li>
<li><p>We will talk more about the sandwich estimator later.</p></li>
</ul>
</div>
</div>
<div id="dataset-7-outlying-observations" class="section level1">
<h1>Dataset 7: Outlying observations</h1>
<pre class="r"><code>x &lt;- runif(100, -3, 3)
y &lt;- x + rnorm(100)
x[[100]] &lt;- 2.6
y[[100]] &lt;- 10
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x = x, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-39-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="dataset-7-residual-plot" class="section level2">
<h2>Dataset 7: Residual Plot</h2>
<pre class="r"><code>lmout &lt;- lm(y ~ x, data = df)
aout &lt;- augment(lmout)
qplot(x = .fitted, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-40-1.png" width="672" style="display: block; margin: auto;" /></p>
<ul>
<li>So we see one observation very high in residual plot compared to the other points around that area.</li>
</ul>
</div>
<div id="dataset-7-solution" class="section level2">
<h2>Dataset 7: Solution</h2>
<ul>
<li><p>It’s generally a bad idea to discard outlying observations unless you know that this is a processing error or a calculation mistake.</p></li>
<li><p>Generally the procedure to dealing with outliers is:</p>
<ol style="list-style-type: decimal">
<li>Detect influential points.</li>
<li>Verify that the outliers are not caused by numerical errors.</li>
<li>Fit the model both with and without the outliers. If the results do not change, report both results.</li>
<li>If the results change, try fitting a robust approach.</li>
</ol></li>
<li><p>Some robust approaches are described in Section 11.3 of KNNL. One possible approach is quantile regression, which assumes the model <span class="math display">\[
  Median(Y_i) = \beta_0 + \beta_1X_i
  \]</span></p>
<pre class="r"><code>library(quantreg)
rqout &lt;- rq(y ~ x, data = df)
t_rq &lt;- tidy(rqout, conf.int = TRUE)
t_rq</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate conf.low conf.high   tau
##   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;
## 1 (Intercept)   -0.290   -0.525   -0.0317   0.5
## 2 x              1.02     0.878    1.17     0.5</code></pre></li>
<li><p>We have a large sample size, so the results do not change much.</p>
<pre class="r"><code>tidy(lmout, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 2 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)   -0.136    0.115      -1.18 2.39e- 1   -0.365    0.0922
## 2 x              1.08     0.0657     16.4  8.48e-30    0.945    1.21</code></pre></li>
<li><p>Chapter 10 contains more details on evaluating the influence of outliers.</p></li>
<li><p><strong>Exercise</strong>: Consider the following data:</p>
<pre class="r"><code>dftoy &lt;- tribble(~x, ~y,
                 2, 16,
                 4, 23,
                 5, 27, 
                 6, 29,
                 7, 32,
                 8, 35,
                 9, 37,
                 12, 54)</code></pre>
<p>Make a scatterplot of <code>y</code> on <code>x</code>, then make a residual versus fits plot for a regression of <code>y</code> on <code>x</code>. Does the residual plot suggest that outliers are the issue, or is it decieving you into thinking something else is the issue?</p></li>
</ul>
</div>
</div>
<div id="dataset-8-sequence-plots-of-residuals-to-check-independence" class="section level1">
<h1>Dataset 8: Sequence plots of residuals to check independence</h1>
<ul>
<li><p>Sometimes, the residuals can exhibit <strong>autocorrelation</strong> where residuals close to each other in time (or a sequence) are more similar than those further away.</p>
<pre class="r"><code>x &lt;- sort(runif(100, -3, 3))
epsilon &lt;- rep(NA_real_, length.out = length(x))
epsilon[[1]] &lt;- 0
rho &lt;- 0.9
for (i in 2:length(epsilon)) {
  epsilon[[i]] &lt;- 0.85 * epsilon[[i-1]] + rnorm(1)
}
y &lt;- x + epsilon
df &lt;- data.frame(x = x, y = y)</code></pre>
<pre class="r"><code>qplot(x = x, y = y, data = df) + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-47-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>lmout &lt;- lm(y ~ x, data = df)
aout &lt;- augment(lmout)
aout &lt;- mutate(aout, index = row_number())
qplot(x = index, y = .resid, data = aout) + geom_hline(yintercept = 0)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-48-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>This is a specific issue with the <strong>independence</strong> assumption.</p></li>
<li><p>The OLS fit is still unbiased, but the standard errors will be too small (because we have less information than we think).</p></li>
</ul>
<div id="dataset-8-solution" class="section level2">
<h2>Dataset 8: Solution</h2>
<ul>
<li><p>This is discussed in detail in Chapter 12 of KNNL.</p></li>
<li><p>You should first check if including omitted variables helps remove autocorrelation.</p>
<ul>
<li>E.g. plotting annual sales versus average price over time, if you are missing population size, then adjacent years probably have more similar errors.</li>
</ul></li>
<li><p>You can try to estimate autocorrelation directly (Section 12.4 of KNNL).</p></li>
<li><p>You can also adjust the OLS standard errors using a similar approach to sandwich estimation. This is called the “Newey-West HAC” (Heteroskedasticity- and autocorrelation-consistent) estimate of the standard error.</p>
<pre class="r"><code>library(lmtest)
library(sandwich)
cout &lt;- coeftest(x = lmout, vcov. = vcovHAC(x = lmout))
tidy(cout, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 2 × 7
##   term        estimate std.error statistic     p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)    0.236     0.332     0.709 0.480         -0.424     0.895
## 2 x              0.991     0.188     5.27  0.000000825    0.617     1.36</code></pre>
<ul>
<li>Notice we used <code>vcovHAC()</code> instead of <code>vcovHC()</code>.</li>
</ul></li>
<li><p>Compare to original standard errors</p>
<pre class="r"><code>tidy(lmout, conf.int = TRUE)</code></pre>
<pre><code>## # A tibble: 2 × 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)    0.236    0.156       1.51 1.33e- 1  -0.0731     0.544
## 2 x              0.991    0.0900     11.0  8.18e-19   0.812      1.17</code></pre></li>
<li><p>You should have large sample sizes for accurate inference (<span class="math inline">\(n\geq 100\)</span>) for estimating the autocorrelation or adjusting the standard errors.</p></li>
</ul>
</div>
</div>
<div id="quantile-quantile-plots-of-residuals-to-check-normality" class="section level1">
<h1>Quantile-Quantile plots of residuals to check normality</h1>
<ul>
<li><p>To check the normality of the residuals, plot the sample quantiles of the residuals against the theoretical quantiles of the normal distribution.</p></li>
<li><p><strong>Quantile</strong>: The <span class="math inline">\(q\)</span>th quantile of a variable is the value at with <span class="math inline">\(q\)</span> proportion of the observations fall.</p></li>
<li><p>Sample 0.25-quantile</p>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-51-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Theoretical 0.25-quantile</p>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-52-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>If the residuals were approximately normal, then we would expect the 0.25 quantile of the data to be about the 0.25 quantile of the normal, the 0.7 quantile of the data to be about the 0.7 quantile of the normal, etc.</p></li>
<li><p>We can graphically compare the quantiles by making a scatterplot of the sample quantiles to the theoretical quantiles. This is called a <strong>QQ-plot</strong> (for quantile-quantile).</p></li>
<li><p>We usually place the theoretical quantiles on the <span class="math inline">\(x\)</span>-axis and the sample quantiles on the <span class="math inline">\(y\)</span>-axis.</p></li>
<li><p>Constructing a QQ-plot</p>
<p><img src="03_figs/qqplot.gif" /> </p></li>
<li><p><strong>Exercise</strong>: The simplest definition of the <span class="math inline">\(p\)</span>th sample quantile is the minimum value <span class="math inline">\(q\)</span> in the sample such that <em>at least</em> <span class="math inline">\(p\)</span> proportion of the observations are at or below <span class="math inline">\(q\)</span>. In mathematical notation <span class="math display">\[
  Q(p) = \min\left(q \in \{X_1,X_2,\ldots,X_n\}: p\leq \frac{1}{n}\#\{X_i:X_i &lt; q\}\right)
  \]</span> In R, this corresponds to the <code>type = 1</code> argument from the quantile function. Try running this code and try understanding this definition:</p>
<pre class="r"><code>quantile(x = c(4, 9, 13, 22, 37), probs = c(0.19999, 0.2, 0.20001, 0.39999, 0.4, 0.40001), type = 1)</code></pre>
<p>Which quantiles do you think we would use when constructing a QQ-plot?</p></li>
<li><p>QQ-plots Can show us deviations from normality</p>
<p><img src="03_figs/bad_qqplot.gif" /> </p></li>
<li><p>QQ-plot indicating right skew. <img src="03_diagnostics_files/figure-html/unnamed-chunk-55-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>QQ-plot indicating left skew. <img src="03_diagnostics_files/figure-html/unnamed-chunk-56-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>QQ-plot indicating heavy tails. <img src="03_diagnostics_files/figure-html/unnamed-chunk-57-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>QQ-plot indicating light tails. <img src="03_diagnostics_files/figure-html/unnamed-chunk-58-1.png" width="672" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="qq-plots-in-r" class="section level2">
<h2>QQ-plots in R</h2>
<ul>
<li><p>We can evaluate the normality assumption from the bread and peace example.</p>
<pre class="r"><code>hibbs &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/hibbs.csv&quot;)
lmhibbs &lt;- lm(vote ~ growth, data = hibbs)</code></pre></li>
<li><p>Use <code>augment()</code> from the <code>{broom}</code> package to obtain the residuals. Then use the <code>geom = "qq"</code> argument in the <code>qplot()</code> function. Make sure you specify <code>sample = .resid</code>, <em>not</em> <code>x = .resid</code>.</p>
<pre class="r"><code>aout &lt;- augment(lmhibbs)
qplot(sample = .resid, data = aout, geom = &quot;qq&quot;) +
  geom_qq_line()</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-60-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>There is a small deviation, but it’s not too bad.</p></li>
</ul>
</div>
<div id="final-notes-about-normality" class="section level2">
<h2>Final notes about normality</h2>
<ul>
<li><p>Only worry about normality if you see extreme outliers or other extreme deviations from normality (or if you want prediction intervals).</p></li>
<li><p>Check for normality <strong>last</strong>. Violations of linearity could make it seem like normality is violated.</p></li>
</ul>
</div>
</div>
<div id="residual-plots-against-omitted-predictors" class="section level1">
<h1>Residual plots against omitted predictors</h1>
<ul>
<li><p>If you have predictors that you did not use in the model, it is always a good idea to make plots of the residuals against those omitted predictors.</p></li>
<li><p>Consider the Palmer Penguins Data.</p>
<pre class="r"><code>library(palmerpenguins)
data(&quot;penguins&quot;)
glimpse(penguins)</code></pre>
<pre><code>## Rows: 344
## Columns: 8
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…
## $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …
## $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …
## $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…
## $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …
## $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…
## $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…</code></pre>
<pre class="r"><code>lmpen &lt;- lm(body_mass_g ~ bill_length_mm, data = penguins)
aout &lt;- augment(lmpen)
penguins %&gt;%
  mutate(.rownames = as.character(row_number())) %&gt;%
  left_join(aout) %&gt;%
  qplot(x = species, y  = .resid, data = ., geom = &quot;boxplot&quot;)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-61-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Whenever you see a systematic pattern between the residuals and an omitted predictor (as above), this indicates that you should include that predictor in your model.</p></li>
</ul>
</div>
<div id="f-test-for-lack-of-fit" class="section level1">
<h1><span class="math inline">\(F\)</span>-test for lack of fit</h1>
<ul>
<li><p>In many cases, you have repeat observations at the same value of the predictors. If you design a study, it is a good idea to include repeats at the same predictor levels.</p></li>
<li><p>If so, then it is possible to run a <strong>lack-of-fit</strong> test:</p>
<ul>
<li><span class="math inline">\(H_0: E[Y_i] = \beta_0 + \beta_1 X_i\)</span></li>
<li><span class="math inline">\(H_A: E[Y_i] \neq \beta_0 + \beta_1 X_i\)</span></li>
</ul></li>
<li><p>Rejecting this test indicates that the linear model is not a good fit.</p></li>
<li><p>Failing to reject this test tells us that at least we do not have evidence that the linear model is a bad fit.</p></li>
<li><p>We will consider this example in the context of a experiment run on 11 branches of a bank. Each branch offered a set gift size (<span class="math inline">\(X\)</span>) to open a new account. The branches then measured the number of new accounts opened (<span class="math inline">\(Y\)</span>).</p>
<pre class="r"><code>bank &lt;- tibble::tribble(~gift, ~accounts,
                        125, 160,
                        100, 112,
                        200, 124,
                         75,  28,
                        150, 152,
                        175, 156,
                         75,  42,
                        175, 124,
                        125, 150,
                        200, 104,
                        100, 136
                        )</code></pre>
<p>There are 6 values of gift size, with all but one value having two units.</p></li>
<li><p>The basic idea of the lack-of-fit test is to compare the residuals under the regression model (the reduced model) to the residuals under a “saturated” model where each level of <span class="math inline">\(X\)</span> is allowed to have its own mean.</p></li>
<li><p>Reduced Model: SSE(R) <img src="03_diagnostics_files/figure-html/unnamed-chunk-63-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Full Model: SSE(F) <img src="03_diagnostics_files/figure-html/unnamed-chunk-64-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>There are 11 observations.</p></li>
<li><p>In the reduced model, there are two parameter (<span class="math inline">\(y\)</span>-intercept and slope), so <span class="math inline">\(df_R = 11 - 2 = 9\)</span>.</p></li>
<li><p>In the full model, there are six parameters (one for each mean to estimate), so <span class="math inline">\(df_F = 11 - 6 = 5\)</span>.</p></li>
<li><p>Use the <span class="math inline">\(F\)</span>-statistic <span class="math display">\[
  F^* = \frac{[SSE(R) - SSE(F)] / (df_R - df_F)}{SSE(F) / df_F}
  \]</span></p></li>
<li><p>Under the reduced model, we have <span class="math display">\[
  F^* \sim F(df_R - df_F, df_F)
  \]</span></p>
<p>So we can compare it to this distribution to obtain a <span class="math inline">\(p\)</span>-value.</p></li>
</ul>
<div id="lack-of-fit-test-in-r" class="section level2">
<h2>Lack-of-fit Test in R</h2>
<ul>
<li><p>First, fit both the reduced and full models.</p></li>
<li><p>The reduced model we’ve seen before.</p>
<pre class="r"><code>lm_r &lt;- lm(accounts ~ gift, data = bank)</code></pre></li>
<li><p>The full model is fit automatically if you convert <code>gift</code> to a factor variable. We’ll talk about the specifics of how this works when we get to indicator variables in multiple linear regression.</p>
<pre class="r"><code>df_full &lt;- mutate(bank, gift_factor = as.factor(gift))
lm_f &lt;- lm(accounts ~ gift_factor, data = df_full)</code></pre></li>
<li><p>Then use the <code>anova()</code> function to compare these two models.</p>
<pre class="r"><code>anova(lm_r, lm_f)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Model 1: accounts ~ gift
## Model 2: accounts ~ gift_factor
##   Res.Df   RSS Df Sum of Sq    F Pr(&gt;F)
## 1      9 14742                         
## 2      5  1148  4     13594 14.8 0.0056</code></pre></li>
<li><p>We can verify this <span class="math inline">\(p\)</span>-value by calculating the <span class="math inline">\(F\)</span>-statistic manually.</p>
<pre class="r"><code>resid_r &lt;- augment(lm_r)$.resid
resid_f &lt;- augment(lm_f)$.resid
sse_r &lt;- sum(resid_r^2)
sse_f &lt;- sum(resid_f^2)
df_r &lt;- nrow(bank) - 2
df_f &lt;- nrow(bank) - 6
f_star &lt;- ((sse_r - sse_f) / (df_r - df_f)) / (sse_f / df_f)
pf(q = f_star, df1 = df_r - df_f, df2 = df_f, lower.tail = FALSE)</code></pre>
<pre><code>## [1] 0.005594</code></pre></li>
</ul>
</div>
<div id="math-notation" class="section level2">
<h2>Math notation</h2>
<ul>
<li><p>We will change notation. Let <span class="math inline">\(Y_{ij}\)</span> be the <span class="math inline">\(j\)</span>th individual in group <span class="math inline">\(i\)</span>. Let <span class="math inline">\(X_i\)</span> be the level of group <span class="math inline">\(i\)</span>.</p></li>
<li><p>You might recognize the above “saturated” model as a one-way ANOVA model <span class="math display">\[
  Y_{ij} \sim N(\mu_i, \sigma^2)
  \]</span></p></li>
<li><p>We compare the ANOVA model to the “reduced” linear regression model. <span class="math display">\[
  Y_{ij} \sim N(\beta_0 + \beta_1X_i, \sigma^2)
  \]</span> where <span class="math inline">\(X_i\)</span> is the level of group <span class="math inline">\(i\)</span>.</p></li>
<li><p>This makes it more clear that the regression model is a reduced version of the ANOVA model.</p></li>
</ul>
</div>
<div id="lack-of-fit-test-summary" class="section level2">
<h2>Lack-of-fit test Summary</h2>
<ul>
<li>Lack-of-fit <span class="math inline">\(F\)</span>-test tests the assumption of linearity.</li>
<li>Needs multiple observations at different predictor values (but some values can have only one observation).</li>
<li>Small <span class="math inline">\(p\)</span>-values indicate lack-of-fit (so linearity is not a valid assumption).</li>
</ul>
</div>
</div>
<div id="other-transformations" class="section level1">
<h1>Other transformations</h1>
<ul>
<li><p>The book suggests other transformations (square root, Box-Cox).</p></li>
<li><p>Square root on the response will compress values more mildly than the log.</p></li>
<li><p>The Box-Cox transformation on the response is really a set of transformations that includes the log and the square root as special cases.</p></li>
<li><p><span class="math inline">\(1/X\)</span> and <span class="math inline">\(exp(X)\)</span> and <span class="math inline">\(exp(-X)\)</span> and <span class="math inline">\(\sqrt{X}\)</span> could all help improve linearity.</p></li>
<li><p>If your goal is <em>prediction</em>, then have at it.</p></li>
<li><p>But if your goal is inference, then you lose all interpretability by using transformations other than <span class="math inline">\(\log()\)</span>. So think carefully before trying other transformations.</p></li>
</ul>
</div>
<div id="back-transforming" class="section level1">
<h1>Back Transforming</h1>
<ul>
<li>After you make a transformation, you often want to back transform predictions, estimates, etc.</li>
</ul>
</div>
<div id="smoothers" class="section level1">
<h1>Smoothers</h1>
<ul>
<li><p>Use non-parametric approaches to curve fitting for exploration purposes. When you have a ton of values, or if the trend is weak relative to the variance, it is sometimes easier to see a trend if you plot an estimate of the curve.</p></li>
<li><p>Lowess works by estimating the mean <span class="math inline">\(Y\)</span> value at a given point <span class="math inline">\(X\)</span> by fitting a regression to points near <span class="math inline">\(X\)</span>, weighting the objective function proportionally to how far away values are from <span class="math inline">\(X\)</span>. E.g. <span class="math display">\[
  \sum_{i: X_i \text{ is near } X} w_i[Y_i - (\beta_0 + \beta_1 X_i)]^2,
  \]</span> where <span class="math inline">\(w_i\)</span> is larger for values closer to <span class="math inline">\(X\)</span>. For each <span class="math inline">\(X\)</span> where you want to make a prediction, it obtains an estimate <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> and estimates the mean value by <span class="math inline">\(\hat{\beta}_0 + \hat{\beta}_1 X\)</span>.</p>
<ul>
<li>This is not exactly what happens, but gives you an idea of how it works.</li>
</ul></li>
<li><p>In practice, you don’t need to know any of this. Just add <code>geom_smooth(se = FALSE)</code> after you make a scatterplot.</p>
<pre class="r"><code>qplot(x = growth, y = vote, data = hibbs) +
  geom_smooth(se = FALSE)</code></pre>
<p><img src="03_diagnostics_files/figure-html/unnamed-chunk-69-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p><strong>Be careful not to over-interpret these.</strong> E.g., don’t say that the above is evidence for a leveling off between 1 and 2, then increasing again. Lowess does not provide inference. It can only generate questions.</p></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
