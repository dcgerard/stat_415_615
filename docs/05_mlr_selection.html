<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2021-11-09" />

<title>MLR: Model Selection and Validation</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">MLR: Model Selection and Validation</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2021-11-09</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Choosing what variables to include in a model.</li>
<li>Chapter 12 of Statistical Sleuth.</li>
<li>Chapter 9 of KNNL</li>
</ul>
</div>
<div id="motivation" class="section level1">
<h1>Motivation</h1>
<ul>
<li><p>You often have many predictor variables.</p></li>
<li><p>Including too few and you are potentially not controlling for key variables.</p></li>
<li><p>Including too many and your regression coefficient estimates might be unstable.</p></li>
<li><p><strong>Model selection</strong>: Choosing which variables (and transformations of these variables) to include.</p></li>
</ul>
</div>
<div id="steps-of-selecting-a-model" class="section level1">
<h1>Steps of Selecting a Model</h1>
<p>From the Statistical Sleuth:</p>
<ol style="list-style-type: decimal">
<li>Identify the key objectives.</li>
<li>Screen the available variables, deciding on a list that is sensitive to the objectives and excludes obvious redundancies.</li>
</ol>
<ul>
<li>Steps 3, 4, and 5: Repeat the following until satisfied:
<ol start="3" style="list-style-type: decimal">
<li>Perform exploratory analysis, examining graphical displays and correlation coeﬃcients.</li>
<li>Perform transformations as necessary.</li>
<li>Examine a residual plot after fitting a rich model, performing further transformations and considering outliers.</li>
</ol></li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>Use a computer-assisted technique for finding a suitable subset of explanatory variables, exerting enough control over the process to be sensitive to the questions of interest.</li>
<li>Proceed with the analysis, using the selected explanatory variables.</li>
</ol>
</div>
<div id="step-1-identify-objectives-and-questions-of-interest" class="section level1">
<h1>Step 1: Identify Objectives and Questions of Interest</h1>
<div id="objective-association-between-x-and-y-controlling-for-z" class="section level2">
<h2>Objective: Association between X and Y controlling for Z</h2>
<ul>
<li><p>Goal is to determine the association between a response and some interesting predictors <em>after adjusting for other nuisance predictors</em>.</p></li>
<li><p><em>Example</em>: Is there evidence of an association between salary and sex after adjusting for other, legitimate, determinants of salary?</p>
<ul>
<li>Association of interest: sex versus salary</li>
<li>Nuisance variables to adjust for: Seniority, age, education, experience</li>
</ul></li>
<li><p>Generally, a good strategy is to (during steps 6 and 7 from above)</p>
<ol style="list-style-type: decimal">
<li>Perform automatic variable selection techniques with everything <em>except</em> the explanatory variables of interest, then</li>
<li>Include the explanatory variables of interest to test for associations.</li>
</ol></li>
<li><p>Automatic variable selection techinques destroy the interpretation of <span class="math inline">\(p\)</span>-values. So</p>
<ul>
<li>Do <strong>not</strong> interpret what set of nuisance variables were chosen,</li>
<li>Do <strong>not</strong> interpret the p-values of the nuisance variables</li>
<li>Do <strong>not</strong> interpret the the coefficients of the nuisance variables.</li>
<li><strong>Only</strong> interpret the coefficients and <em>p</em>-values corresponding to the variables of interest.</li>
</ul></li>
<li><p><strong>Example</strong>: Using automatic selection procedures, it was determined to include experience, seniority, and education, but not age in the final model. After adding in sex, the resulting fit to the final model was</p>
<pre><code>## # A tibble: 5 × 5
##   term         estimate std.error statistic  p.value
##   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)  8.67      0.0955       90.8  9.51e-89
## 2 Senior      -0.00435   0.000944     -4.61 1.38e- 5
## 3 Educ         0.0164    0.00448       3.66 4.26e- 4
## 4 Exper        0.000261  0.000107      2.45 1.62e- 2
## 5 SexMale      0.130     0.0214        6.06 3.30e- 8</code></pre>
<ul>
<li><p>It is tempting to say that Age is not associated with salary, adjusting for other variables, while the other variables are associated with salary, adjusting for other variables. However, <strong>this is wrong</strong>.</p></li>
<li><p>Since we used an automatic variable selection procedure for Age, Seniority, Education, and Experience, we cannot interpret those coefficients or <span class="math inline">\(p\)</span>-values.</p></li>
<li><p>We can only interpret the coefficient and <span class="math inline">\(p\)</span>-value for sex, which we did not use a automatic variable selection technique on.</p></li>
<li><p>We estimate that females make about 89% the salary of males, adjusting for other, legitimate, predictors of base salary. The evidence is strong that this association is not due to chance alone (<span class="math inline">\(p = 3.3\times 10^{-8}\)</span>).</p></li>
<li><p>Technically, we are also adjusting for age in this statement, even though it was not in the model, because it had the chance to be in the model.</p></li>
</ul></li>
</ul>
</div>
<div id="objective-fishing" class="section level2">
<h2>Objective: Fishing</h2>
<ul>
<li><p>You have a response variable and many explanatory variables, and you want to know what variables are possibly associated with your response.</p></li>
<li><p>Then iterate through adding/removing variables, making transformations, checking residuals, until you develop a model with significant terms and no major issues.</p></li>
<li><p><span class="math inline">\(p\)</span>-values/confidence intervals don’t have proper interpretation.</p>
<ul>
<li>Same problems with multiple comparisons — ran many tests and looked at data a lot to come to final model.</li>
</ul></li>
<li><p>You generally build a model and tell stories with it.</p></li>
<li><p>Issues:</p>
<ol style="list-style-type: decimal">
<li>Explanatory variables are not necessarily special. Inclusion/exclusion strongly affected by multicollinearity.
<ul>
<li>I.e., You are just generating hypotheses, <strong>not</strong> conclusions.</li>
</ul></li>
<li>Causal interpretations are not allowed, as usual, with observational studies.</li>
<li><span class="math inline">\(p\)</span>-values are meaningless.</li>
</ol></li>
<li><p>If your dataset is large, then you can split your data into a training set and a test set. You can do whatever variable selection techniques you want on your training set, then fit your final model on your test set to obtain <span class="math inline">\(p\)</span>-values that have the proper interpretation.</p>
<ul>
<li>Test set could be about 25% and the training set about 75%.</li>
<li>Splitting data is feasible if you have more than 6 observations per predictor variable in the training set.</li>
<li>As soon as you look at how well your method works on your test set, you can no longer do anything.</li>
<li>I.e. if you cheat and redo your analysis to better fit the test set, then you are back to the problems of fishing and there was no point in splitting the data in the first place.</li>
</ul></li>
</ul>
</div>
<div id="objective-prediction" class="section level2">
<h2>Objective: Prediction</h2>
<ul>
<li><p>Include variables to maximize predictive power, don’t worry about interpretation.</p></li>
<li><p>This lecture is not when prediction is the goal.</p></li>
<li><p>The automatic variable selection procedures described in this lecture are not often used when the goal is prediction.</p></li>
<li><p>Take the Machine Learning course for details when the goal is prediction.</p>
<ul>
<li>Typical methods include regularization and cross validation.</li>
</ul></li>
</ul>
</div>
</div>
<div id="step-2-screen-available-variables" class="section level1">
<h1>Step 2: Screen Available Variables</h1>
<ul>
<li><p>Choose a list of explanatory variables that are important to the objective.</p></li>
<li><p>Screen out redundant variables.</p></li>
<li><p>Use your domain expertise for screening variables.</p></li>
<li><p>Note: What variables are important will depend on the question being asked.</p></li>
<li><p><strong>Example</strong>: Researchers were interested in what variables were associated with state average SAT scores. Possible predictors include</p>
<ul>
<li><code>Takers</code>: Percentage of high school seniors in the state who took the exam.</li>
<li><code>Income</code>: Median income of families of test-takers (hundreds of dollars).</li>
<li><code>Years</code>: Average number of years test-takers had formal studies.</li>
<li><code>Public</code>: Percentage of test-takers who attended public schools.</li>
<li><code>Expend</code>: Total state expenditure on secondary schools, in hundreds of dollars per student.</li>
<li><code>Rank</code>: Median percentile ranking of test-takers within their secondary school classes.</li>
</ul>
<pre class="r"><code>library(Sleuth3)
data(&quot;case1201&quot;)
sat &lt;- case1201
glimpse(sat)</code></pre>
<pre><code>## Rows: 50
## Columns: 8
## $ State  &lt;fct&gt; Iowa, SouthDakota, NorthDakota, Kansas, Nebraska, Montana, Minn…
## $ SAT    &lt;int&gt; 1088, 1075, 1068, 1045, 1045, 1033, 1028, 1022, 1017, 1011, 100…
## $ Takers &lt;int&gt; 3, 2, 3, 5, 5, 8, 7, 4, 5, 10, 5, 4, 9, 8, 7, 3, 6, 16, 19, 11,…
## $ Income &lt;int&gt; 326, 264, 317, 338, 293, 263, 343, 333, 328, 304, 358, 295, 330…
## $ Years  &lt;dbl&gt; 16.79, 16.07, 16.57, 16.30, 17.25, 15.91, 17.41, 16.57, 16.01, …
## $ Public &lt;dbl&gt; 87.8, 86.2, 88.3, 83.9, 83.6, 93.7, 78.3, 75.2, 97.0, 77.3, 74.…
## $ Expend &lt;dbl&gt; 25.60, 19.95, 20.62, 27.14, 21.05, 29.48, 24.84, 17.42, 25.96, …
## $ Rank   &lt;dbl&gt; 89.7, 90.6, 89.8, 86.3, 88.5, 86.4, 83.4, 85.9, 87.5, 84.2, 85.…</code></pre>
<ul>
<li>Goal 1: Business firm looking for a place to build a new facility. They want to know if SAT scores accurately reflect educational training of the labor market in that state.
<ul>
<li>Only care if SAT score is associated with <code>Rank</code> (the educational training variable) after accounting for selection bias (<code>Takers</code>).</li>
</ul></li>
<li>Goal 2: Government wants to determine impact of state expenditures on SAT scores. Then include all variables as possible predictors to so we can see what effect expenditures has that cannot be accounted by other variables.</li>
</ul></li>
<li><p>Problems with Including Too Few Variables</p>
<ul>
<li><p>You are only picking up <strong>marginal</strong> associations.</p></li>
<li><p>E.g., we already know that men make more money than women. We want to see if men <strong>still</strong> make more money than women when we control for other variables.</p></li>
<li><p>Predictions are less accurate.</p>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></p>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-4-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>If you fit a model with <span class="math inline">\(X\)</span>, then this is what the model is seeing:</p>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-5-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul></li>
<li><p>Problems with too many variables</p>
<ul>
<li><p>Harder to estimate more parameters.</p></li>
<li><p>Formally, the variances of the sampling distributions of the coefficients in the model will get much larger.</p></li>
<li><p>Including highly correlated explanatory variables will <strong>really</strong> increase the variance of the sampling distributions of the coefficient estimates.</p></li>
<li><p>Intuitively, we are less sure if the association of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> is due to that actual associate or is it mediated through <span class="math inline">\(X_2\)</span>?</p></li>
<li><p>Predictions are less accurate.</p></li>
</ul></li>
<li><p>Demonstration when have too many variables</p>
<ul>
<li><p>True model: <span class="math inline">\(E(Y|X_1) = X_1\)</span></p></li>
<li><p>Fit Model: <span class="math inline">\(E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2X_2\)</span></p></li>
<li><p>Correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is 0.9994.</p></li>
<li><p>We will simulate <span class="math inline">\(Y\)</span> and plot the resulting OLS estimates.</p></li>
<li><p>Black is truth</p>
<p><img src="figs/sim_pred.gif" style="display: block; margin: auto;" /></p></li>
</ul></li>
</ul>
</div>
<div id="steps-3-through-5" class="section level1">
<h1>Steps 3 through 5</h1>
<ol start="3" style="list-style-type: decimal">
<li><p>Exploratory data analysis.</p>
<ul>
<li><p>Tons of scatterplots.</p></li>
<li><p>Look at correlation coefficients.</p></li>
</ul></li>
<li><p>Transformations based on EDA.</p></li>
<li><p>Fit a rich model and look at residuals.</p>
<ul>
<li>Look for curvature, non-constant variance, and outliers.</li>
</ul></li>
</ol>
<ul>
<li>Iterate the above steps until you don’t see any issues.</li>
</ul>
<div id="sat-example" class="section level2">
<h2>SAT Example</h2>
<ul>
<li><p>Matrix scatterplot of SAT data</p>
<pre class="r"><code>library(GGally)
ggpairs(sat, columns = 2:8)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Curvature between SAT scores and percentage takers. But it looks like constant variance at first, so maybe taking a log of percentage takers would help.</p>
<pre class="r"><code>sat &lt;- mutate(sat, l_takers = log(Takers))
qplot(x = l_takers, y = SAT, data = sat)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-10-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>There is a huge outlier in expenditures caused by Alaska</p>
<pre class="r"><code>arrange(sat, desc(Expend)) %&gt;%
  select(State, Expend) %&gt;%
  head()</code></pre>
<pre><code>##           State Expend
## 1        Alaska  50.10
## 2       NewYork  33.58
## 3 Massachusetts  31.74
## 4        Oregon  30.49
## 5       Montana  29.48
## 6  Pennsylvania  27.98</code></pre></li>
<li><p>Let’s fit a rich model to identify any other transformations and other possible issues.</p>
<pre class="r"><code>lm_rich &lt;- lm(SAT ~ l_takers + Income + Years + Public + Expend + Rank, data = sat)
a_rich &lt;- augment(lm_rich)
qplot(x = .fitted, y = .resid, data = a_rich) + geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-12-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = l_takers, y = .resid, data = a_rich) + geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-12-2.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = Income, y = .resid, data = a_rich) + geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-12-3.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = Years, y = .resid, data = a_rich) + geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-12-4.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = Public, y = .resid, data = a_rich) + geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-12-5.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = Expend, y = .resid, data = a_rich) + geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-12-6.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = Rank, y = .resid, data = a_rich) + geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_selection_files/figure-html/unnamed-chunk-12-7.png" width="384" style="display: block; margin: auto;" /></p>
<p>The only worry is the expenditures residual plot where, again, we see Alaska.</p></li>
<li><p>In Chapter 10, we will learn about leverage values, and we see that Alaska has a very high Cook’s distance (<code>.cooksd</code>) and a high leverage (<code>.hat</code>).</p>
<pre class="r"><code>a_rich %&gt;%
  bind_cols(select(sat, State)) %&gt;%
  select(State, .cooksd, .hat) %&gt;%
  arrange(desc(.cooksd))</code></pre>
<pre><code>## # A tibble: 50 × 3
##    State         .cooksd   .hat
##    &lt;fct&gt;           &lt;dbl&gt;  &lt;dbl&gt;
##  1 Alaska         1.60   0.572 
##  2 Louisiana      0.207  0.345 
##  3 Mississippi    0.152  0.147 
##  4 SouthCarolina  0.146  0.129 
##  5 Tennessee      0.143  0.207 
##  6 WestVirginia   0.0880 0.169 
##  7 NewHampshire   0.0728 0.0913
##  8 NorthCarolina  0.0437 0.102 
##  9 Indiana        0.0345 0.198 
## 10 Illinois       0.0339 0.106 
## # … with 40 more rows</code></pre>
<ul>
<li><p>A high leverage is typically above <span class="math inline">\(2p/n\)</span>, or in this case <span class="math inline">\(2 * 7 / 50 = 0.28\)</span></p></li>
<li><p>A high Cook’s distance is around or above 1.</p></li>
</ul></li>
<li><p>Let’s remove Alaska. This is probably OK since Alaska uses expenditures on things like long distance travel for teachers, and higher heating bills, which do not have a direct effect on educational attainment. Thus, this expenditure value is likely not representative of our objective.</p>
<pre class="r"><code>sat &lt;- filter(sat, State != &quot;Alaska&quot;)</code></pre></li>
<li><p>We would go through the above steps again to see if everything looks good in our rich model.</p></li>
<li><p>This is also the stage where we would include quadratic terms, do other variable transformations, etc.</p></li>
</ul>
</div>
</div>
<div id="step-6-automated-variable-selection" class="section level1">
<h1>Step 6: Automated Variable Selection</h1>
<ul>
<li><p>If appropriate, use an automatic variable selection technique to choose a suitable subset of explanatory variables.</p></li>
<li><p>An automatic variable selection technique has a criterion where a higher value indicates a better fit.</p></li>
<li><p>The computer adds/deletes variables from the model until it reaches a point where it cannot increase the criterion any more.</p></li>
<li><p>The selected model is considered the “best” one.</p></li>
<li><p>You need to make sure your “rich” model has no major issues before implementing these approaches.</p></li>
<li><p>Most statisticians dislike automatic variable selection approaches, but they can be useful to</p>
<ol style="list-style-type: decimal">
<li>Control for many predictors when your objective is the relationship between a couple key predictors and a response variables, and</li>
<li>Fishing for hypotheses, as long as you don’t take your results too seriously and always hedge when you present results.</li>
</ol></li>
</ul>
<div id="criteria" class="section level2">
<h2>Criteria</h2>
<ul>
<li>AIC: Akaike’s Information Criterion <span class="math display">\[
  AIC = n\log\left(\frac{SSE}{n}\right) + 2p
  \]</span>
<ul>
<li>Lower is better.</li>
<li>Fit (in terms of SSE) + penalty on the number of parameters.</li>
<li>As <span class="math inline">\(p\)</span> increases, SSE decreases, but this is possibly offset by the penalty.</li>
<li>This is the criterion base R includes by default for stepwise procedures.</li>
</ul></li>
<li>BIC (SBC): The Bayesian information Criterion (aka Schwarz’s Bayesian Criterion) penalizes the parameters a little more than AIC. <span class="math display">\[
  AIC = n\log\left(\frac{SSE}{n}\right) + \log(n)p
  \]</span>
<ul>
<li>Lower is better.</li>
<li>Again, fit + penalty.</li>
<li>As <span class="math inline">\(p\)</span> increases, SSE decreases, but this is possibly offset by the penalty.</li>
<li>Better than AIC when you have not been as careful about deleting redundant predictors.</li>
</ul></li>
<li>Mallows <span class="math inline">\(C_p\)</span> <span class="math display">\[
  C_p = p + (n-p)\frac{\hat{\sigma}^2 - \hat{\sigma}^2_{full}}{\hat{\sigma}^2_{full}},
  \]</span>
<ul>
<li><span class="math inline">\(\hat{\sigma}^2\)</span> is the MSE under the model under consideration.</li>
<li><span class="math inline">\(\hat{\sigma}^2_{full}\)</span> is the MSE under the model that uses all possible explanatory variables.</li>
<li>Lower <span class="math inline">\(C_p\)</span> is better.</li>
<li>Mallow’s <span class="math inline">\(C_p\)</span> is an estimate of the “Total Mean Squared Error” which is the sum of the bias squared and the variance. Small values of Mallow’s <span class="math inline">\(C_p\)</span> indicate that the model both has low bias and low variance.</li>
<li>If there is no bias, then <span class="math inline">\(C_p \approx p\)</span> (since <span class="math inline">\(\hat{\sigma}^2 \approx \hat{\sigma}^2_{full}\)</span>). So all models withere <span class="math inline">\(C_p\)</span> is near <span class="math inline">\(p\)</span> are considered “good” models.</li>
</ul></li>
<li><span class="math inline">\(R^2\)</span>: <span class="math display">\[
  R^2 = 1 - \frac{SSE}{SSTO}
  \]</span>
<ul>
<li>Higher is better (explains more variation..</li>
<li>Do not use this one for variable selection.</li>
<li>Recall that <span class="math inline">\(R^2\)</span> will decrease as we add more predictors, so we cannot use it to compare models with differen numbers of predictors.</li>
</ul></li>
<li>Adjusted <span class="math inline">\(R_a^2\)</span>: Look at whether there is a “plateau” when you add predictors. <span class="math display">\[
  R^2 = 1 - \frac{SSE/(n-p)}{SSTO/(n-1)}
  \]</span>
<ul>
<li>Higher is better (explains more variation).</li>
</ul></li>
</ul>
</div>
<div id="automated-procedures" class="section level2">
<h2>Automated Procedures</h2>
<ul>
<li><p>“Best” subsets: Look at all possible models given the set of predictors, choose the one with the best criterion.</p></li>
<li><p>Forward/Backward Selection: Start with a model with no predictors <span class="math inline">\(Y_i = \beta_0 + \epsilon_i\)</span>.</p>
<ul>
<li>Fit all models where you <strong>add</strong> a predictor. Choose the one that improves the criterion the best.</li>
<li>Fit all models where you <strong>remove</strong> a predictor. Choose the one that improves the criterion the best.<br />
</li>
<li>Iterate until you cannot add/remove any more predictors.</li>
</ul></li>
<li><p>The forward/backward approach is not gauranteed to find the model with the “best” criterion.</p></li>
<li><p>When you add/subtract a categorical variable, you should add/subtract all of the indicators associated with that categorical variable.</p></li>
<li><p>Make sure you also include first-order terms if second order terms are kept in the model.</p></li>
</ul>
</div>
</div>
<div id="step-7-proceed-with-caution" class="section level1">
<h1>Step 7: Proceed with Caution</h1>
<ul>
<li><p>Proceed with analysis with chosen explanatory variables.</p></li>
<li><p>Evaluate residual plots for your final model. Perform other model checks.</p></li>
<li><p>Tell stories with the data using <span class="math inline">\(p\)</span>-values, coefficient estimates, confidence intervals, coefficients of determination, etc…</p></li>
<li><p>Step 7 is what we’ve been discussing this whole semester.</p></li>
</ul>
</div>
<div id="implementation-in-r" class="section level1">
<h1>Implementation in R</h1>
<ul>
<li><p>You can get AIC, BIC, <span class="math inline">\(R^2\)</span>, and <span class="math inline">\(R_a^2\)</span> via <code>glance()</code> from <code>{broom}</code></p>
<pre class="r"><code>lm_rich &lt;- lm(SAT ~ l_takers + Income + Years + Public + Expend + Rank, data = sat)
glance(lm_rich)  %&gt;%
  select(AIC, BIC, r.squared, adj.r.squared)</code></pre>
<pre><code>## # A tibble: 1 × 4
##     AIC   BIC r.squared adj.r.squared
##   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;
## 1  453.  468.     0.913         0.900</code></pre></li>
<li><p>No easy default way in R to get <span class="math inline">\(C_p\)</span>. But other software packages use it, so it is good to be aware of.</p></li>
<li><p>Use the <code>step()</code> function to choose a model by AIC/BIC via automated search.</p>
<ul>
<li>The <code>object</code> argument is the intial model that it will search from.</li>
<li>The <code>scope</code> argument specifies the simplest and the most complicated models possible.
<ul>
<li>If you give it a single formula, this is the most complicated model possible.</li>
<li>If you give it a list of formulas, these are the most complicated and simplest models possible.</li>
</ul></li>
<li>You can use the output of <code>step()</code> like the output of <code>lm()</code> (via <code>tidy()</code>, <code>augment()</code>, etc).</li>
</ul></li>
<li><p>To use both a lower and upper bound, do:</p>
<pre class="r"><code>lm_init &lt;- lm(SAT ~ Income + Years, data = sat)
sout &lt;- step(object = lm_init, scope = list(upper = SAT ~ l_takers + Income + Years + Public + Expend + Rank, lower = SAT ~ Income))</code></pre>
<pre><code>## Start:  AIC=393.9
## SAT ~ Income + Years
## 
##            Df Sum of Sq    RSS AIC
## + Rank      1     99984  34294 329
## + l_takers  1     94604  39674 336
## + Public    1     23088 111190 387
## &lt;none&gt;                  134278 394
## - Years     1      9072 143350 395
## + Expend    1        14 134264 396
## 
## Step:  AIC=329
## SAT ~ Income + Years + Rank
## 
##            Df Sum of Sq    RSS AIC
## + Expend    1     10326  23968 313
## &lt;none&gt;                   34294 329
## + Public    1       759  33535 330
## + l_takers  1       412  33882 330
## - Years     1     13581  47876 343
## - Rank      1     99984 134278 394
## 
## Step:  AIC=313.4
## SAT ~ Income + Years + Rank + Expend
## 
##            Df Sum of Sq    RSS AIC
## + l_takers  1      2552  21417 310
## &lt;none&gt;                   23968 313
## + Public    1       422  23547 315
## - Years     1      7353  31322 325
## - Expend    1     10326  34294 329
## - Rank      1    110295 134264 396
## 
## Step:  AIC=309.9
## SAT ~ Income + Years + Rank + Expend + l_takers
## 
##            Df Sum of Sq   RSS AIC
## &lt;none&gt;                  21417 310
## + Public    1        20 21397 312
## - l_takers  1      2552 23968 313
## - Years     1      3011 24428 314
## - Rank      1      3162 24578 315
## - Expend    1     12465 33882 330</code></pre>
<pre class="r"><code>tidy(sout)</code></pre>
<pre><code>## # A tibble: 6 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)  291.      256.         1.14 0.261    
## 2 Income         0.113     0.113      1.01 0.319    
## 3 Years         13.5       5.49       2.46 0.0180   
## 4 Rank           5.06      2.01       2.52 0.0155   
## 5 Expend         3.87      0.774      5.00 0.0000100
## 6 l_takers     -31.2      13.8       -2.26 0.0287</code></pre></li>
<li><p>To use just an upper bound, do:</p>
<pre class="r"><code>lm_init &lt;- lm(SAT ~ Income + Years, data = sat)
sout &lt;- step(object = lm_init, scope = SAT ~ l_takers + Income + Years + Public + Expend + Rank)</code></pre>
<pre><code>## Start:  AIC=393.9
## SAT ~ Income + Years
## 
##            Df Sum of Sq    RSS AIC
## + Rank      1     99984  34294 329
## + l_takers  1     94604  39674 336
## + Public    1     23088 111190 387
## &lt;none&gt;                  134278 394
## - Years     1      9072 143350 395
## + Expend    1        14 134264 396
## - Income    1     84760 219038 416
## 
## Step:  AIC=329
## SAT ~ Income + Years + Rank
## 
##            Df Sum of Sq    RSS AIC
## + Expend    1     10326  23968 313
## &lt;none&gt;                   34294 329
## + Public    1       759  33535 330
## + l_takers  1       412  33882 330
## - Income    1      3322  37616 332
## - Years     1     13581  47876 343
## - Rank      1     99984 134278 394
## 
## Step:  AIC=313.4
## SAT ~ Income + Years + Rank + Expend
## 
##            Df Sum of Sq    RSS AIC
## + l_takers  1      2552  21417 310
## &lt;none&gt;                   23968 313
## + Public    1       422  23547 315
## - Income    1      3048  27016 317
## - Years     1      7353  31322 325
## - Expend    1     10326  34294 329
## - Rank      1    110295 134264 396
## 
## Step:  AIC=309.9
## SAT ~ Income + Years + Rank + Expend + l_takers
## 
##            Df Sum of Sq   RSS AIC
## - Income    1       505 21922 309
## &lt;none&gt;                  21417 310
## + Public    1        20 21397 312
## - l_takers  1      2552 23968 313
## - Years     1      3011 24428 314
## - Rank      1      3162 24578 315
## - Expend    1     12465 33882 330
## 
## Step:  AIC=309.1
## SAT ~ Years + Rank + Expend + l_takers
## 
##            Df Sum of Sq   RSS AIC
## &lt;none&gt;                  21922 309
## + Income    1       505 21417 310
## + Public    1       185 21737 311
## - Rank      1      2676 24598 313
## - Years     1      2870 24792 313
## - l_takers  1      5094 27016 317
## - Expend    1     13620 35542 331</code></pre>
<pre class="r"><code>tidy(sout)</code></pre>
<pre><code>## # A tibble: 5 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)   399.     232.         1.72 0.0929    
## 2 Years          13.1      5.48       2.40 0.0207    
## 3 Rank            4.40     1.90       2.32 0.0252    
## 4 Expend          4.00     0.764      5.23 0.00000452
## 5 l_takers      -38.1     11.9       -3.20 0.00257</code></pre></li>
<li><p>Modify the <code>k</code> argument to be <span class="math inline">\(log(n)\)</span> to fit by BIC</p>
<pre class="r"><code>sout &lt;- step(object = lm_init, scope = SAT ~ l_takers + Income + Years + Public + Expend + Rank, k = log(nrow(sat)))</code></pre>
<pre><code>## Start:  AIC=399.6
## SAT ~ Income + Years
## 
##            Df Sum of Sq    RSS AIC
## + Rank      1     99984  34294 337
## + l_takers  1     94604  39674 344
## + Public    1     23088 111190 394
## - Years     1      9072 143350 399
## &lt;none&gt;                  134278 400
## + Expend    1        14 134264 403
## - Income    1     84760 219038 420
## 
## Step:  AIC=336.6
## SAT ~ Income + Years + Rank
## 
##            Df Sum of Sq    RSS AIC
## + Expend    1     10326  23968 323
## &lt;none&gt;                   34294 337
## - Income    1      3322  37616 337
## + Public    1       759  33535 339
## + l_takers  1       412  33882 340
## - Years     1     13581  47876 349
## - Rank      1     99984 134278 400
## 
## Step:  AIC=322.9
## SAT ~ Income + Years + Rank + Expend
## 
##            Df Sum of Sq    RSS AIC
## + l_takers  1      2552  21417 321
## &lt;none&gt;                   23968 323
## - Income    1      3048  27016 325
## + Public    1       422  23547 326
## - Years     1      7353  31322 332
## - Expend    1     10326  34294 337
## - Rank      1    110295 134264 403
## 
## Step:  AIC=321.3
## SAT ~ Income + Years + Rank + Expend + l_takers
## 
##            Df Sum of Sq   RSS AIC
## - Income    1       505 21922 319
## &lt;none&gt;                  21417 321
## - l_takers  1      2552 23968 323
## - Years     1      3011 24428 324
## - Rank      1      3162 24578 324
## + Public    1        20 21397 325
## - Expend    1     12465 33882 340
## 
## Step:  AIC=318.5
## SAT ~ Years + Rank + Expend + l_takers
## 
##            Df Sum of Sq   RSS AIC
## &lt;none&gt;                  21922 319
## - Rank      1      2676 24598 320
## - Years     1      2870 24792 321
## + Income    1       505 21417 321
## + Public    1       185 21737 322
## - l_takers  1      5094 27016 325
## - Expend    1     13620 35542 338</code></pre>
<pre class="r"><code>tidy(sout)</code></pre>
<pre><code>## # A tibble: 5 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)   399.     232.         1.72 0.0929    
## 2 Years          13.1      5.48       2.40 0.0207    
## 3 Rank            4.40     1.90       2.32 0.0252    
## 4 Expend          4.00     0.764      5.23 0.00000452
## 5 l_takers      -38.1     11.9       -3.20 0.00257</code></pre></li>
</ul>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
