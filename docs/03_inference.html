<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2021-07-30" />

<title>Simple Linear Regression: Inference</title>

<script src="site_libs/header-attrs-2.9/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Simple Linear Regression: Inference</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2021-07-30</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#learning-objectives">Learning Objectives</a></li>
<li><a href="#testing-h_0beta_10">Testing <span class="math inline">\(H_0:\beta_1=0\)</span></a>
<ul>
<li><a href="#sampling-distribution">Sampling distribution</a></li>
<li><a href="#standard-errors">Standard errors</a></li>
<li><a href="#finding-a-test-statistic">Finding a test statistic</a></li>
<li><a href="#implement-in-r">Implement in R</a></li>
</ul></li>
<li><a href="#confidence-interval-for-beta_1">Confidence interval for <span class="math inline">\(\beta_1\)</span></a>
<ul>
<li><a href="#implementation-in-r.">Implementation in R.</a></li>
</ul></li>
<li><a href="#best-practices-and-other-comments">Best Practices and other comments</a></li>
<li><a href="#other-interval-estimates">Other interval estimates</a>
<ul>
<li><a href="#confidence-interval-for-ey_ix_i">Confidence Interval for <span class="math inline">\(E[Y_i|X_i]\)</span></a>
<ul>
<li><a href="#implementation-in-r">Implementation in R</a></li>
</ul></li>
<li><a href="#prediction-interval-for-haty_inew-given-x_inew">Prediction Interval for <span class="math inline">\(\hat{Y}_{i(new)}\)</span> given <span class="math inline">\(X_{i(new)}\)</span></a></li>
<li><a href="#confidence-bands">Confidence Bands</a></li>
<li><a href="#comparing-intervals">Comparing intervals</a></li>
</ul></li>
<li><a href="#anova-approach-to-hypothesis-testing">ANOVA approach to hypothesis testing</a></li>
<li><a href="#coefficient-of-determination">Coefficient of Determination</a></li>
<li><a href="#correlation-coefficient">Correlation Coefficient</a></li>
</ul>
</div>

<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Chapter 2 of KNNL (skip section 2.11).</li>
<li>Sampling distributions of OLS estimates.</li>
<li>Hypothesis testing for regression coefficients.</li>
<li>Confidence intervals for regression coefficients.</li>
<li>Regression line intervals: pointwise confidence, prediction, confidence bands</li>
<li><span class="math inline">\(F\)</span>-tests for regression models.</li>
<li><span class="math inline">\(R\)</span> and <span class="math inline">\(R^2\)</span>.</li>
</ul>
</div>
<div id="testing-h_0beta_10" class="section level1">
<h1>Testing <span class="math inline">\(H_0:\beta_1=0\)</span></h1>
<ul>
<li><p>Recall our model for simple linear regression: <span class="math display">\[\begin{align}
  Y_i &amp;= \beta_0 + \beta_1 X_i + \epsilon_i\\
  \epsilon_i &amp;\overset{i.i.d.}{\sim} N(0, \sigma^2)
  \end{align}\]</span></p></li>
<li><p>A common task is to test the following hypotheses:</p>
<ul>
<li><span class="math inline">\(H_0: \beta_1 = 0\)</span>.
<ul>
<li><span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> are not <em>linearly</em> associated.</li>
<li><span class="math inline">\(E[Y_i|X_i] = \beta_0\)</span> no matter the value of <span class="math inline">\(X_i\)</span>. So <span class="math inline">\(X_i\)</span> doesn’t tell us anything about <span class="math inline">\(Y_i\)</span> (in the context of linear relationships).</li>
</ul></li>
<li><span class="math inline">\(H_A: \beta_1 \neq 0\)</span>.
<ul>
<li><span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> are <em>linearly</em> associated.</li>
<li>We can get better guesses by of <span class="math inline">\(Y_i\)</span> by knowing <span class="math inline">\(X_i\)</span> than if we did not know <span class="math inline">\(X_i\)</span>.</li>
</ul></li>
</ul></li>
<li><p>Overview of the strategy of hypothesis testing:</p>
<p><img src="hypothesis_testing/hyp.png" /> </p>
<ol style="list-style-type: decimal">
<li>We obtain a test statistic whose large/small values will provide support against the null.</li>
<li>We compare the observed test-statistic to the distribution of hypothetical test statistics we would have observed <em>if the null were true</em>.
<ul>
<li>This distribution is called the <strong>sampling distribution</strong> of the test statistic.</li>
</ul></li>
<li>We obtain the <strong><span class="math inline">\(p\)</span>-value</strong>: The probability of seeing a data as extreme or more extreme than what we saw <em>if the null were true</em>.</li>
<li>If the <span class="math inline">\(p\)</span>-value is small, then our data would be very rare if the null were true, so we claim the null is not true.</li>
<li>If the <span class="math inline">\(p\)</span>-value is large, then our data would be common if the null were true. But it could also be common if the null were false, so we make no claims.</li>
</ol></li>
<li><p><strong>Exercise</strong>: What is wrong with the statement: “We test against the null of <span class="math inline">\(H_0: \hat{\beta}_1 = 0\)</span>.”</p></li>
<li><p><strong>Exercise</strong>: What is wrong with the following interpretation of the <span class="math inline">\(p\)</span>-value: “The <span class="math inline">\(p\)</span>-value is the probability that the null hypothesis is true, so small <span class="math inline">\(p\)</span>-values suggest we reject the null hypothesis.”</p></li>
</ul>
<div id="sampling-distribution" class="section level2">
<h2>Sampling distribution</h2>
<ul>
<li><p>The sampling distribution of <span class="math inline">\(\hat{\beta}_1\)</span> is <span class="math display">\[
  \hat{\beta}_1 \sim N\left(\beta_1, \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\right).
  \]</span></p></li>
<li><p>We will prove the mean result of this. If you get a PhD in Statistics, you go through these type of calculations all of the time.</p></li>
<li><p>Note that <span class="math inline">\(\hat{\beta}_1 = \sum_{i=1}^nk_iY_i\)</span> where <span class="math inline">\(k_i = \frac{X_i - \bar{X}}{\sum_{i=1}^n(X_i - \bar{X})^2}\)</span>.</p></li>
<li><p>Property from theory: Any linear combination of independent normal random variables is also normal.</p></li>
<li><p><strong>So we just need the mean and variance of <span class="math inline">\(\hat{\beta}_1\)</span> to know its distribution.</strong></p>
<p><span class="math display">\[\begin{align}
  E[\hat{\beta}_1] &amp;= E\left[\sum_{i=1}^nk_iY_i\right]\\
  &amp;=\sum_{i=1}^nk_iE[Y_i]\\
  &amp;=\sum_{i=1}^nk_i(\beta_0 + \beta_1 X_i)\\
  &amp;= \beta_0\sum_{i=1}^nk_i + \beta_1\sum_{i=1}^nk_iX_i.
  \end{align}\]</span></p></li>
<li><p>We will now prove that <span class="math inline">\(\sum_{i=1}^nk_i = 0\)</span> and <span class="math inline">\(\sum_{i=1}^nk_iX_i = 1\)</span>.</p>
<p><span class="math display">\[\begin{align}
  \sum_{i=1}^nk_i &amp;= \frac{\sum_{i=1}^n(X_i - \bar{X})}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{\sum_{i=1}^nX_i - \sum_{i=1}^n\bar{X}}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{n\bar{X} - n\bar{X}}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= 0.
  \end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
  \sum_{i=1}^nk_iX_i &amp;= \frac{\sum_{i=1}^nX_i(X_i - \bar{X})}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{\sum_{i=1}^nX_i^2 - \bar{X}\sum_{i=1}^nX_i}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{\sum_{i=1}^nX_i^2 - n\bar{X}^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{\sum_{i=1}^nX_i^2 - 2n\bar{X}^2 + n\bar{X}^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{\sum_{i=1}^nX_i^2 - 2\bar{X}\sum_{i=1}^nX_i + n\bar{X}^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{\sum_{i=1}^n(X_i^2 - 2\bar{X}X_i + \bar{X}^2)}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= \frac{\sum_{i=1}^n(X_i - \bar{X})^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\\
  &amp;= 1
  \end{align}\]</span></p></li>
<li><p>Putting this together, we have <span class="math display">\[
  E[\hat{\beta}_1] =  \beta_0\sum_{i=1}^nk_i + \beta_1\sum_{i=1}^nk_iX_i = \beta_0 \times 0 + \beta_1 \times 1 = \beta_1.
  \]</span></p></li>
<li><p>The proof that <span class="math inline">\(var(\hat{\beta}_1) = \frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\)</span> is similar.</p></li>
<li><p><strong>Exercise</strong>: What happens to the variance of <span class="math inline">\(\hat{\beta}_1\)</span> when the <span class="math inline">\(X_i\)</span>’s are more spread out?</p></li>
</ul>
</div>
<div id="standard-errors" class="section level2">
<h2>Standard errors</h2>
<ul>
<li><p>The standard deviation of the sampling distribution of an estimator is called the <strong>standard error</strong>.</p></li>
<li><p>This quantity is important for describing how certain we are about an estimate.</p></li>
<li><p>The standard error <span class="math inline">\(\hat{\beta}_1\)</span> is <span class="math inline">\(\frac{\sigma^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\)</span>.</p></li>
<li><p>We don’t know <span class="math inline">\(\sigma^2\)</span>, but we have an estimator for it, the mean squared error (MSE).</p>
<p><span class="math display">\[
  MSE = \frac{1}{n-2} \sum_{i=1}^n(Y_i - \hat{Y}_i)^2
  \]</span></p></li>
<li><p>So the estimated standard error of <span class="math inline">\(\hat{\beta}_1\)</span> is <span class="math inline">\(s(\hat{\beta}_1)\)</span> where <span class="math display">\[
  s^2(\hat{\beta}_1) = \frac{MSE}{\sum_{i=1}^n (X_i - \bar{X})^2}
  \]</span></p></li>
<li><p>We usually just say “standard error”, even when we are talking about the estimated standard error.</p></li>
</ul>
</div>
<div id="finding-a-test-statistic" class="section level2">
<h2>Finding a test statistic</h2>
<ul>
<li><p>Our goal when testing against <span class="math inline">\(H_0: \beta_1 = 0\)</span> is to find a statistic that (i) can provide evidence against <span class="math inline">\(H_0\)</span> based on its value and (ii) we know the distribution of under <span class="math inline">\(H_0\)</span>.</p></li>
<li><p>The distribution of <span class="math display">\[
  \frac{\hat{\beta}_1 - \beta_1}{s(\hat{\beta}_1)} \sim t_{n-2}
  \]</span></p>
<ul>
<li>A <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</li>
<li><span class="math inline">\(n\)</span> is the sample size.</li>
<li>We subtract two because we estimated two parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> when we were calculating the MSE.</li>
</ul></li>
<li><p>Thus, if <span class="math inline">\(H_0: \beta_1 = 0\)</span> were true, we would have that <span class="math display">\[
  t^* = \frac{\hat{\beta}_1}{s(\hat{\beta}_1)} \sim t_{n-2}
  \]</span> But if <span class="math inline">\(H_A\)</span> were true, then <span class="math inline">\(t^*\)</span> would not follow a <span class="math inline">\(t_{n-2}\)</span> distribution.</p></li>
<li><p>We can compare <span class="math inline">\(t^*\)</span> to a <span class="math inline">\(t_{n-2}\)</span> distribution to see how extreme it is.</p>
<p><span class="math display">\[
  p-\text{value} = 2*\text{pt}(-|t^*|, n-2).
  \]</span></p>
<p><img src="03_inference_files/figure-html/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
<div id="implement-in-r" class="section level2">
<h2>Implement in R</h2>
<ul>
<li><p>R will automatically calculate <span class="math inline">\(t\)</span>-values and <span class="math inline">\(p\)</span>-values, so we never need to manually make these calculations.</p></li>
<li><p>We first fit the linear model using <code>lm()</code> as before, saving the output to a variable.</p>
<pre class="r"><code>library(tidyverse)
library(broom)
hibbs &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/hibbs.csv&quot;)
lmhibbs &lt;- lm(vote ~ growth, data = hibbs)</code></pre></li>
<li><p>We then again use the <code>tidy()</code> function from the <code>{broom}</code> package.</p>
<pre class="r"><code>t_hibbs &lt;- tidy(lmhibbs)</code></pre></li>
<li><p>The <code>statistic</code> variable contains the <span class="math inline">\(t\)</span>-statistics, while the <code>p.value</code> variable contains the <span class="math inline">\(p\)</span>-value for the test that the parameter equals 0.</p>
<pre class="r"><code>t_hibbs</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic  p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 (Intercept)    46.2      1.62      28.5  8.41e-14
## 2 growth          3.06     0.696      4.40 6.10e- 4</code></pre></li>
<li><p>In the above output the <span class="math inline">\(t\)</span>-statistic against the null of <span class="math inline">\(\beta_1 = 0\)</span> is 4.4.</p></li>
<li><p>So in the above output, the <span class="math inline">\(p\)</span>-value against the null of <span class="math inline">\(\beta_1 = 0\)</span> is <code>6.1e-04</code> = <span class="math inline">\(6.1 \times 10^{-4} = 0.00061\)</span>.</p></li>
<li><p>We can verify that the <span class="math inline">\(p\)</span>-value can be derived directly from the <span class="math inline">\(t\)</span>-statistic.</p>
<pre class="r"><code>n &lt;- nrow(hibbs)
2 * pt(q = -abs(4.396), df = n - 2)</code></pre>
<pre><code>## [1] 0.0006095</code></pre></li>
<li><p><strong>Exercise</strong>: Fill in the missing values (the <code>NA</code>’s) from the following R output:</p>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)    -7.52     NA        -1.37 0.180    
## 2 drat            7.68      1.51     NA    0.0000178</code></pre></li>
<li><p><strong>Exercise</strong>: Verify your answer to the previous exercise by fitting a regression of <code>mpg</code> on <code>drat</code> from the <code>mtcars</code> dataset.</p></li>
</ul>
</div>
</div>
<div id="confidence-interval-for-beta_1" class="section level1">
<h1>Confidence interval for <span class="math inline">\(\beta_1\)</span></h1>
<ul>
<li><p>Remember: interpreting confidence intervals:</p>
<ul>
<li>CORRECT: We used a procedure that would capture the true parameter in 95% of repeated samples.</li>
<li>CORRECT: <em>Prior to sampling</em>, the probability of capturing the true parameter is 0.95.</li>
<li>WRONG: After sampling, the probability of capturing the true parameter is 0.95.
<ul>
<li>Because after sampling the parameter is either in the interval or it’s not. We just don’t know which.</li>
</ul></li>
</ul>
<p><img src="03_figs/ci_interp.gif" /> </p></li>
<li><p>Let’s construct a <span class="math inline">\((1-\alpha)\)</span> confidence interval (typically <span class="math inline">\(\alpha = 0.05\)</span> so we would construct a 95% confidence interval).</p></li>
<li><p>Let <span class="math display">\[
  \ell = \text{qt}(\alpha/2, n-2)\\
  u = \text{qt}(1 - \alpha/2, n-2)\\
  \]</span> For some <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>.</p></li>
<li><p>Note that <span class="math inline">\(\ell = - u\)</span></p>
<pre class="r"><code>alpha &lt;- 0.05
n &lt;- 10
qt(alpha / 2, n - 2)</code></pre>
<pre><code>## [1] -2.306</code></pre>
<pre class="r"><code>qt(1 - alpha / 2, n - 2)</code></pre>
<pre><code>## [1] 2.306</code></pre></li>
<li><p>Since <span class="math inline">\(t^* = \frac{\hat{\beta}_1 - \beta_1}{s(\hat{\beta}_1)} \sim t_{n-2}\)</span>, we have that <span class="math display">\[
  Pr\left(-u \leq \frac{\hat{\beta}_1 - \beta_1}{s(\hat{\beta}_1)} \leq u \right) = 1 - \alpha
  \]</span></p>
<p><img src="03_inference_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Thus, we have <span class="math display">\[\begin{align}
  &amp;Pr\left(-u \leq \frac{\hat{\beta}_1 - \beta_1}{s(\hat{\beta}_1)} \leq u \right) = 1 - \alpha\\
  &amp;\Leftrightarrow Pr\left(-u s(\hat{\beta}_1) \leq \hat{\beta}_1 - \beta_1 \leq u s(\hat{\beta}_1) \right) = 1 - \alpha\\
  &amp;\Leftrightarrow Pr\left(-u s(\hat{\beta}_1) - \hat{\beta}_1\leq  -\beta_1 \leq u s(\hat{\beta}_1) - \hat{\beta}_1 \right) = 1 - \alpha\\
  &amp;\Leftrightarrow Pr\left(\hat{\beta}_1 + u s(\hat{\beta}_1)\geq  \beta_1 \geq \hat{\beta}_1 - u s(\hat{\beta}_1) \right) = 1 - \alpha\\
  \end{align}\]</span></p></li>
<li><p>Thus <span class="math inline">\(\hat{\beta}_1 \pm u s(\hat{\beta}_1)\)</span>, where <span class="math inline">\(u = \text{qt}(1 - \alpha/2, n-2)\)</span> is a <span class="math inline">\((1-\alpha)\)</span> confidence interval for <span class="math inline">\(\beta_1\)</span>.</p></li>
<li><p>Again, it is the interval that is random (and varies across repeated samples), not the parameter.</p></li>
</ul>
<div id="implementation-in-r." class="section level2">
<h2>Implementation in R.</h2>
<ul>
<li><p>R will return confidence intervals if you ask, so you typically do not need to do these calculations.</p></li>
<li><p>First, use <code>lm()</code> to fit the linear model.</p>
<pre class="r"><code>hibbs &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/hibbs.csv&quot;)
lmhibbs &lt;- lm(vote ~ growth, data = hibbs)</code></pre></li>
<li><p>Use the <code>tidy()</code> function from the <code>{broom}</code> package, but use the <code>conf.int = TRUE</code> argument. The confidence interval will be in the <code>conf.low</code> and <code>conf.high</code> variables.</p>
<pre class="r"><code>t_hibbs &lt;- tidy(lmhibbs, conf.int = TRUE)
select(t_hibbs, term, estimate, p.value, conf.low, conf.high)</code></pre>
<pre><code>## # A tibble: 2 × 5
##   term        estimate  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)    46.2  8.41e-14    42.8      49.7 
## 2 growth          3.06 6.10e- 4     1.57      4.55</code></pre></li>
<li><p>A full interpretation might go something like this:</p>
<blockquote>
<p>We estimate that incumbants have 3.1% higher vote-share in years which experience 1% more growth (95% confidence interval of 1.6% higher to 4.6% higher). We have strong evidence that this association is not due to chance alone (<span class="math inline">\(p = 0.00061\)</span>, <span class="math inline">\(n = 16\)</span>).</p>
</blockquote></li>
<li><p><strong>Exercise</strong>: Find the</p></li>
</ul>
</div>
</div>
<div id="best-practices-and-other-comments" class="section level1">
<h1>Best Practices and other comments</h1>
<ul>
<li><p>Always provide <span class="math inline">\(p\)</span>-values. Never state that results are “significant” or “not significant” without reporting a <span class="math inline">\(p\)</span>-value.</p></li>
<li><p>Always report effect size (i.e. slope) estimates with their corresponding 95% confidence intervals.</p></li>
<li><p>Always report the sample size.</p></li>
<li><p>Almost never use 1-sided tests (folks will look at you suspiciously).</p></li>
<li><p>Don’t interpret <span class="math inline">\(p\)</span>-values so discretely. I.e. 0.049 is not more significant than 0.051.</p></li>
<li><p>Rules of thumb:</p>
<ul>
<li><span class="math inline">\(p &lt; 0.001\)</span>: Very strong evidence.</li>
<li><span class="math inline">\(p &lt; 0.01\)</span>: Strong evidence.</li>
<li><span class="math inline">\(p &lt; 0.05\)</span>: Evidence.</li>
<li><span class="math inline">\(p &lt; 0.1\)</span>: Weak evidence.</li>
<li><span class="math inline">\(p &gt; 0.1\)</span>: No evidence.</li>
</ul></li>
<li><p>We derived the sampling distribution of <span class="math inline">\(\hat{\beta}_1\)</span> using normality. But this is not that important as long as (i) you have a large sample size or (ii) the data aren’t too skewed.</p>
<ul>
<li>Central limit theorem guarantees distributional results for large sample size.</li>
</ul></li>
<li><p>The sampling distribution assumes the <span class="math inline">\(X_i\)</span>’s don’t change in repeated samples, but this only shows up as important if you are literally trying to reproduce a study.</p></li>
<li><p>Inference on <span class="math inline">\(\beta_0\)</span></p>
<ul>
<li><span class="math inline">\(\hat{\beta}_0\)</span> is a statistic, so it has a sampling distribution.</li>
<li>We can use this sampling distribution to run hypothesis tests for <span class="math inline">\(\beta_0\)</span>, or obtain confidence intervals for <span class="math inline">\(\beta_0\)</span>.</li>
<li>R reports these automatically (in the row called “<code>(Intercept)</code>”)</li>
<li>These are almost never useful.</li>
</ul></li>
</ul>
</div>
<div id="other-interval-estimates" class="section level1">
<h1>Other interval estimates</h1>
<ul>
<li>There are three other types of intervals that we are interested in:</li>
</ul>
<ol style="list-style-type: decimal">
<li><p>A confidence interval for the mean response at a given predictor level <span class="math inline">\(E[Y_i|X_i]\)</span>.</p></li>
<li><p>A prediction interval for a single observation at a given predictor level.</p></li>
<li><p>A confidence band to capture the entire regression line.</p></li>
</ol>
<p>We will go through these three intervals in turn.</p>
<div id="confidence-interval-for-ey_ix_i" class="section level2">
<h2>Confidence Interval for <span class="math inline">\(E[Y_i|X_i]\)</span></h2>
<ul>
<li><p>Recall <span class="math inline">\(E[Y_i|X_i] = \beta_0 + \beta_1 X_i\)</span></p></li>
<li><p>Let <span class="math inline">\(\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i\)</span>.</p></li>
<li><p>Since <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are random (and having sampling distributions), that means that <span class="math inline">\(\hat{Y}_i\)</span> is random (and has a sampling distribution).</p>
<p><span class="math display">\[
  \hat{Y}_i \sim N\left(\beta_0 + \beta_1 X_i, \sigma^2\left[\frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\right]\right)
  \]</span></p></li>
<li><p><span class="math inline">\(\hat{Y}_i\)</span> is an unbiased estimator of <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>.</p></li>
<li><p>The estimated standard error is <span class="math inline">\(s(\hat{Y}_i)\)</span> where <span class="math display">\[
s^2(\hat{Y}_i) = \text{MSE}\left[\frac{1}{n} + \frac{(X_i - \bar{X})^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\right]
\]</span></p></li>
<li><p><strong>Exercise</strong>: What value of <span class="math inline">\(X_i\)</span> results in the smallest standard error?</p></li>
<li><p>From stat theory, we have that <span class="math display">\[
  \frac{\hat{Y}_i - E[Y_i|X_i]}{s(\hat{Y}_i)} \sim t_{n-2}
  \]</span></p></li>
<li><p>We can use this to obtain a confidence interval for <span class="math inline">\(E[Y_i|X_i]\)</span>. <span class="math display">\[
  \hat{Y}_i \pm \text{qt}(1 - \alpha / 2, n-2)s(\hat{Y}_i)
  \]</span></p></li>
<li><p>Graphic of interpretation:</p>
<p><img src="03_figs/ci_mean.gif" /> </p></li>
<li><p>This confidence interval only applies when a <em>single</em> mean response is to be estimated.</p></li>
<li><p>These are called <strong>pointwise confidence intervals</strong> because they provide confidence intervals for the mean at a single <span class="math inline">\(X_i\)</span>.</p></li>
<li><p>If you use <code>geom_smooth()</code> from <code>{ggplot2}</code>, the resulting confidence intervals are all pointwise.</p></li>
<li><p><strong>Exercise</strong>: Write a paragraph on the strategy you would use to test <span class="math inline">\(H_0: E[Y_i|X_i] = \mu\)</span> for some prespecified <span class="math inline">\(\mu\)</span>.</p></li>
</ul>
<div id="implementation-in-r" class="section level3">
<h3>Implementation in R</h3>
<ul>
<li><p>Take the output of <code>lm()</code></p>
<pre class="r"><code>lmhibbs &lt;- lm(vote ~ growth, data = hibbs)</code></pre></li>
<li><p>Create a new data frame that contains the desired level(s) of <span class="math inline">\(X_i\)</span>.</p>
<pre class="r"><code>newdf &lt;- data.frame(growth = c(1.2, 2.4))</code></pre></li>
<li><p>Use <code>predict()</code> with the <code>interval = "confidence"</code> argument to obtain confidence intervals.</p>
<pre class="r"><code>predict(object = lmhibbs, newdata = newdf, interval = &quot;confidence&quot;) %&gt;%
  cbind(newdf)</code></pre>
<pre><code>##     fit   lwr   upr growth
## 1 49.92 47.65 52.19    1.2
## 2 53.59 51.44 55.75    2.4</code></pre></li>
</ul>
</div>
</div>
<div id="prediction-interval-for-haty_inew-given-x_inew" class="section level2">
<h2>Prediction Interval for <span class="math inline">\(\hat{Y}_{i(new)}\)</span> given <span class="math inline">\(X_{i(new)}\)</span></h2>
<p><img src="03_figs/pred_int.gif" /> </p>
</div>
<div id="confidence-bands" class="section level2">
<h2>Confidence Bands</h2>
<p><img src="03_figs/cband.gif" /> </p>
</div>
<div id="comparing-intervals" class="section level2">
<h2>Comparing intervals</h2>
<p><img src="03_inference_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="anova-approach-to-hypothesis-testing" class="section level1">
<h1>ANOVA approach to hypothesis testing</h1>
<ul>
<li><p>Our hypothesis test is a comparison between the two models: <span class="math display">\[\begin{align}
  Y_i = \beta_0 + \epsilon_i\\
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
  \end{align}\]</span></p></li>
<li><p>We will call the first model the <strong>reduced</strong> model and the second model the <strong>full</strong> model. This is because the reduced model is a subset of the full model (you get the reduced from the full by setting <span class="math inline">\(\beta_1 = 0\)</span>).</p></li>
<li><p>Sum of squares of reduced model</p>
<p><span class="math display">\[
  SSE(R) = \sum_{i=1}^n(Y_i - \bar{Y})^2
  \]</span></p>
<p><img src="03_inference_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Sum of squares of full model</p>
<p><span class="math display">\[
  SSE(F) = \sum_{i=1}^n(Y_i - \hat{Y}_i)^2
  \]</span></p>
<p><img src="03_inference_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Under the null that the reduced model is true, we have that the following statistic has a sampling distribution that is <span class="math inline">\(F(1, n-2)\)</span> <span class="math display">\[
  F^* = \frac{[SSE(R) - SSE(F)]/(df_R - df_F)}{SSE(F)/df_F}
  \]</span></p></li>
</ul>
</div>
<div id="coefficient-of-determination" class="section level1">
<h1>Coefficient of Determination</h1>
</div>
<div id="correlation-coefficient" class="section level1">
<h1>Correlation Coefficient</h1>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
