<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2024-04-16" />

<title>MLR: More Complicated Model Diagnostics</title>

<script src="site_libs/header-attrs-2.26/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">MLR: More Complicated Model
Diagnostics</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2024-04-16</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Studentized residuals, Leverage values, DFFits, Cook’s Distance,
Variance Inflation Factor</li>
<li>Chapter 10 of KNNL</li>
</ul>
</div>
<div id="added-variable-plots" class="section level1">
<h1>Added Variable Plots</h1>
<ul>
<li><p><em>Goal</em>: Visualization to determine model adequacy for a
single predictor.</p></li>
<li><p><em>Motivation</em>: <a href="./05_mlr_i.html">Recall</a> that we
said to plot residuals versus predictors to see if there were
curvature.</p>
<ul>
<li>This shows the relationship between the predictor and the residuals,
but <strong>not</strong> after adjusting for other variables.</li>
</ul></li>
<li><p><strong>Added-variable plots</strong> (aka “partial regression
plots” and “adjusted variables plots”)</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(Y\)</span> (as the response) on
every predictor <em>except</em> <span
class="math inline">\(X_k\)</span>. Obtain the residuals, and call these
<span class="math inline">\(e_i(Y)\)</span></li>
<li>Regress <span class="math inline">\(X_k\)</span> (as the response)
on every other predictor. Obtain the residuals, and call these <span
class="math inline">\(e_i(X_k)\)</span></li>
<li>Make a scatterplot of <span class="math inline">\(e_i(X_k)\)</span>
versus <span class="math inline">\(e_i(Y)\)</span>.</li>
</ol></li>
<li><p><em>Intuition</em>:</p>
<ul>
<li><span class="math inline">\(e_i(Y)\)</span> and <span
class="math inline">\(e_i(X_k)\)</span> reflect the part of each
variable that is not linearly associated with the other predictors.</li>
</ul></li>
<li><p><em>Conclusions</em>:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(e_i(Y)\)</span> and <span
class="math inline">\(e_i(X_k)\)</span> do not appear to be linearly
related, then <span class="math inline">\(X_k\)</span> is not associated
with <span class="math inline">\(Y\)</span> given all other predictors.
<img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-1-1.png" width="384" style="display: block; margin: auto;" /></li>
<li>If <span class="math inline">\(e_i(Y)\)</span> and <span
class="math inline">\(e_i(X_k)\)</span> appear to be linearly related,
then <span class="math inline">\(X_k\)</span> is linearly associated
with <span class="math inline">\(Y\)</span> given all other predictors.
<ul>
<li>The slope of the least squares line through the origin in this plot
turns out to be <span class="math inline">\(\hat{\beta}_k\)</span> in
the MLR fit.
<img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-2-1.png" width="384" style="display: block; margin: auto;" /></li>
</ul></li>
<li>If <span class="math inline">\(e_i(Y)\)</span> and <span
class="math inline">\(e_i(X_k)\)</span> appear to have a curved
relationship, then <span class="math inline">\(X_k\)</span> has a curved
relationship with <span class="math inline">\(Y\)</span> given all other
predictors.
<img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></li>
<li>If you see an outlying point, this might be an influential point for
predictor <span class="math inline">\(X_k\)</span> given all other
predictors.
<img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-4-1.png" width="384" style="display: block; margin: auto;" /></li>
</ol></li>
<li><p>Added variable plots can also help us visualize how much the
variability will be reduced if we add predictor <span
class="math inline">\(X_k\)</span> to the model.</p>
<ul>
<li>Variability of <span class="math inline">\(e_i(Y)\)</span> about the
horizontal <span class="math inline">\(y=0\)</span> line gives us the
SSE of model <em>without</em> <span
class="math inline">\(X_k\)</span>.</li>
<li>Variability of <span class="math inline">\(e_i(y)\)</span> about the
OLS line in the scatterplot of <span
class="math inline">\(e(X_k)\)</span> on <span
class="math inline">\(e(Y)\)</span> gives us the SSE of the model
<em>with</em> <span class="math inline">\(X_k\)</span>.</li>
<li>Figure 10.2 from KNNL:
<img src="figs/avplot.png" width="644" style="display: block; margin: auto;" /></li>
</ul></li>
</ul>
</div>
<div id="life-insurance-example" class="section level1">
<h1>Life Insurance Example</h1>
<ul>
<li><p>Researchers were interested in the association between income in
thousands of dollars (<span class="math inline">\(X_1\)</span>), a risk
aversion score (<span class="math inline">\(X_2\)</span>) found via a
questionnaire, and the amount of life insurance carried in thousands of
dollars (<span class="math inline">\(Y\)</span>). You can load these
data into R via:</p>
<pre class="r"><code>library(tidyverse)
library(broom)
managers &lt;- tibble::tribble(
  ~income, ~risk, ~insurance,
    45.01,     6,         91,
   57.204,     4,        162,
   26.852,     5,         11,
    66.29,     7,        240,
   40.964,     5,         73,
   72.996,    10,        311,
    79.38,     1,        316,
   52.766,     8,        154,
   55.916,     6,        164,
   38.122,     4,         54,
    35.84,     6,         53,
   75.796,     9,        326,
   37.408,     5,         55,
   54.376,     2,        130,
   46.186,     7,        112,
    46.13,     4,         91,
   30.366,     3,         14,
    39.06,     5,         63
  )</code></pre></li>
<li><p>We typically make added variable plots after we notice something
fishy via the usual residual plots.</p></li>
<li><p>Lets fit a full of line insurance on income and risk aversion and
plot the residuals</p>
<pre class="r"><code>lm_man &lt;- lm(insurance ~ income + risk, data = managers)
a_man &lt;- augment(lm_man)
ggplot(data = a_man, mapping = aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-7-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = a_man, mapping = aes(x = income, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-7-2.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>ggplot(data = a_man, mapping = aes(x = risk, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-7-3.png" width="384" style="display: block; margin: auto;" />
The big issue appears to be from income.</p></li>
<li><p>Let’s see the form of the relationship between income and
insurance, adjusting for risk aversion.</p></li>
<li><p>To make an added variable plot, first regression <span
class="math inline">\(Y\)</span> on the other predictors, then <span
class="math inline">\(X_k\)</span> on the other predictors.</p>
<pre class="r"><code>lm_y &lt;- lm(insurance ~ risk, data = managers)
lm_xk &lt;- lm(income ~ risk, data = managers)</code></pre>
<p>Then plot the residuals from each fit.</p>
<pre class="r"><code>resdf &lt;- data.frame(x = resid(lm_xk), y = resid(lm_y))
ggplot(data = resdf, mapping = aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  geom_hline(yintercept = 0, lty = 2) +
  xlab(&quot;e(income|risk)&quot;) +
  ylab(&quot;e(insurance|risk)&quot;)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>A faster way (but not for written reports or homeworks since it
looks less nice) is to use the <code>avPlot()</code> function from the
<code>{car}</code> package</p>
<pre class="r"><code>library(car)
avPlot(lm_man, variable = &quot;income&quot;)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-10-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>From this plot, we see that</p>
<ol style="list-style-type: decimal">
<li>The relationship between income and insurance is very strong,
adjusting for risk. The variability about the OLS line is much smaller
than the variability about the <span class="math inline">\(y=0\)</span>
line.</li>
<li>Income appears to have a monotone curved relationship with
insurance, adjusting for risk.</li>
<li>Even though the relationship is clearly curved, most of the
variability can be explained by the linear effect, indicating that using
a linear assumption would be a fine first order approximation (depending
on the goals of the study).</li>
<li>There is one observation in the upper right that might be
influential.</li>
</ol></li>
<li><p><strong>Exercise</strong>: Recall the body fat data, that you can
read about <a
href="https://dcgerard.github.io/stat_415_615/data.html#Body_Fat">here</a>
and download from here: <a
href="https://dcgerard.github.io/stat_415_615/data/body.csv"
class="uri">https://dcgerard.github.io/stat_415_615/data/body.csv</a></p>
<p>Make an added variable plot for triceps given thigh, and thigh given
triceps (omit midarm for this exercise). What conclusions do you draw
from the added variable plots?</p></li>
<li><p>Notes:</p>
<ol style="list-style-type: decimal">
<li>Added variable plots will change based on what other predictors you
are adjusting for. The nature of the relationship between <span
class="math inline">\(X_k\)</span> and <span
class="math inline">\(Y\)</span> might change based on what other
predictors are there.</li>
<li>Added variable plots are only appropriate when the assumptions of
the linear model are fulfilled for the other predictors that you are
adjusting for.</li>
</ol></li>
</ul>
</div>
<div id="plots-to-detect-outliers" class="section level1">
<h1>Plots to Detect Outliers</h1>
<ul>
<li><p><strong>Goal</strong>: Visualization to flag outlying
observations.</p></li>
<li><p>Consider this plot of outliers
<img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-13-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>Outliers have minor effects if</p>
<ol style="list-style-type: decimal">
<li>They are well in the range of typical <span
class="math inline">\(X\)</span> values. This is since many other <span
class="math inline">\(X\)</span> values are there to keep the OLS line
from getting too displaced. This is Case 3 above.</li>
<li>They are consistent with the general trend between the predictors
and <span class="math inline">\(Y\)</span>. This is Case 2 above.</li>
</ol></li>
<li><p>Outliers can have huge effects if they both (i) have atypical
<span class="math inline">\(X\)</span> values and (ii) are not
consistent with the general trend. This is Case 1 above.</p></li>
<li><p>Recall that outliers are hard to detect when there is
multicollinearity</p>
<pre><code>## 
## Attaching package: &#39;cowplot&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:lubridate&#39;:
## 
##     stamp</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-14-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="detect-extreme-y-values-studentized-residuals"
class="section level2">
<h2>Detect Extreme <span class="math inline">\(Y\)</span>-values:
Studentized Residuals</h2>
<ul>
<li><p>So far, I have just been having you “feel” for outliers via
looking at residual plot.</p></li>
<li><p>Let’s try and detect outliers more systematically.</p></li>
<li><p>To know how extreme a residual is, we need to know its standard
deviation (how far it should be from 0 on average).</p></li>
<li><p>Let <span class="math inline">\(h_{ii}\)</span> be the <span
class="math inline">\(i\)</span>th diagonal element of the hat matrix
<span class="math inline">\(\mathbf{H} =
\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>.</p>
<ul>
<li>There are <span class="math inline">\(n\)</span> diagonal elements,
one for each observation.</li>
</ul></li>
<li><p>You can show that the variance of the <span
class="math inline">\(i\)</span>th residual, <span
class="math inline">\(e_i\)</span>, is <span class="math display">\[
  \sigma^2(e_i) = \sigma^2(1 - h_{ii}).
  \]</span></p></li>
<li><p>So we estimate the standard deviation of the <span
class="math inline">\(i\)</span>th residual via <span
class="math display">\[
  s(e_i) = \sqrt{MSE(1 - h_{ii})}.
  \]</span></p></li>
<li><p>Note that each residual has a different standard deviation
depending on its value of <span class="math inline">\(h_{ii}\)</span>.
So</p>
<ul>
<li>Larger <span class="math inline">\(h_{ii}\)</span> means smaller
standard deviation.</li>
<li>Smaller <span class="math inline">\(h_{ii}\)</span> means larger
standard deviation.</li>
</ul></li>
<li><p>The <strong>studentized</strong> residual is the residual divided
by its standard deviation. <span class="math display">\[
  r_i = \frac{e_i}{s(e_i)}
  \]</span></p></li>
<li><p>The idea is that we making all of the residuals to have about the
same standard deviation.</p></li>
<li><p>In a normal distribution 95% of observations will be within 2
standard deviations of the mean, and 99.7% of observations will be
within 3 standard deviations of the mean.</p></li>
<li><p>So residuals have potentially large <span
class="math inline">\(Y\)</span> values if they outside of <span
class="math inline">\([-2,2]\)</span>.</p></li>
<li><p>But remember, we would still expect about 5% of observations to
lie outside of <span class="math inline">\([-2, 2]\)</span>. So be chill
here.</p></li>
<li><p>R calls these “standardized residuals”, and you can gent them
from the <code>.std.resid</code> variable from the output of
<code>augment()</code> from the <code>{broom}</code> package.</p>
<pre class="r"><code>body &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/body.csv&quot;)
lm_bod &lt;- lm(fat ~ thigh + triceps, data = body)
a_bod &lt;- augment(lm_bod)
ggplot(data = a_bod, mapping = aes(x = .fitted, y = .std.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = -2, lty = 2)  +
  geom_hline(yintercept = 2, lty = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-15-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>It is common to also obtain the <strong>studentized deleted
residuals</strong> where you calculate the error variance by
leave-one-out regression.</p>
<ol style="list-style-type: decimal">
<li>For each observation <span class="math inline">\(i\)</span>, fit the
MLR model without <span class="math inline">\(i\)</span> to get the MSE.
Call this <span class="math inline">\(MSE_{(i)}\)</span>.</li>
<li>Calculate the studentized deleted residuals by <span
class="math display">\[
t_i = \frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}}
\]</span></li>
</ol></li>
<li><p>The intuition is that a real outlier might have undue influence
on the OLS line, so we should standardize when it is not used to
calculate the OLS line. See Chapter 10 of KNNL for a deeper
discussion.</p></li>
<li><p>You can get the studentized deleted residuals via the base R
function <code>rstudent()</code>.</p>
<pre class="r"><code>a_bod$.dstd.resid &lt;- rstudent(lm_bod)
ggplot(data = a_bod, mapping = aes(x = .fitted, y = .dstd.resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = -2, lty = 2)  +
  geom_hline(yintercept = 2, lty = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-16-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>In this case, the studentized and studentized deleted residuals
produced the same conclusions (that everything is fine), but they can be
very different if there is a single highly influential point. The
deleted studentized residual would be preferred.</p></li>
<li><p>R terminology NOTE: R calls the studentized residuals
“standardized residuals” and uses the base R function
<code>rstandard()</code>. R calls the studentized deleted residuals just
“studentized residuals” and uses the base R function
<code>rstudent()</code>. I’m sorry.</p></li>
</ul>
</div>
<div id="detect-extreme-x-values-leverage-values"
class="section level2">
<h2>Detect Extreme <span class="math inline">\(X\)</span> values:
Leverage Values</h2>
<ul>
<li><p><strong>Goal</strong>: Numerically quantify how outlying an
observation’s predictor values are.</p></li>
<li><p>The diagonal elements of the hat matrix <span
class="math inline">\(h_{ii}\)</span> are called <strong>leverage
values</strong>.</p></li>
<li><p><span class="math inline">\(h_{ii}\)</span> is a measure of the
distance between the <span class="math inline">\(X\)</span> values for
the <span class="math inline">\(i\)</span>th case and the means of the
<span class="math inline">\(X\)</span> values for all <span
class="math inline">\(n\)</span> cases.</p>
<ul>
<li>The larger <span class="math inline">\(h_{ii}\)</span> is, the
further from the center of the cloud of <span
class="math inline">\(X\)</span> values it is.</li>
</ul></li>
<li><p>Properties: <span class="math display">\[
  0 \leq h_{ii} \leq 1\\
  \sum_{i=1}^nh_{ii} = p
  \]</span></p></li>
<li><p><span class="math inline">\(h_{ii}\)</span> is a function only
the <span class="math inline">\(X\)</span> values, not <span
class="math inline">\(Y\)</span>, and so is a measure of how extreme the
observational units <span class="math inline">\(X\)</span> value is, not
considering <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Example red dot is <span class="math inline">\((\bar{X}_1,
\bar{X}_2)\)</span>. Numbers are leverage values:
<img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-17-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>Rule-of-thumb: A hat value is large if it is greater than <span
class="math inline">\(2p/n\)</span>.</p></li>
<li><p>The <code>.hat</code> variable from the output of
<code>augment()</code> from the <code>{broom}</code> package contains
the leverage values.</p></li>
<li><p>Body fat example: Observation 3 appears to have a high
leverage:</p>
<pre class="r"><code>lm_bod &lt;- lm(fat ~ triceps + thigh + midarm, data = body)
a_bod &lt;- augment(lm_bod)

cutoff &lt;- 2 * ncol(model.matrix(lm_bod)) / nrow(model.matrix(lm_bod))
a_bod &lt;- mutate(a_bod, i = row_number())
ggplot(data = a_bod, mapping = aes(x = .hat, y = .std.resid, label = i)) +
  geom_label() +
  ggtitle(&quot;Residuals vs Leverage&quot;) +
  geom_vline(xintercept = cutoff, lty = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-18-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="hat-matrix-for-hidden-extrapolation" class="section level3">
<h3>Hat-matrix for hidden extrapolation</h3>
<ul>
<li><p>Since the leverage values are a measure for extremeness, it can
be used to flag possible hidden extrapolations.</p>
<ul>
<li>Not a perfect measure, since it could still be a hidden
extrapolation, but it’s better than nothing.</li>
</ul></li>
<li><p>Let <span class="math inline">\(\mathbf{X}_{new}\)</span> be the
vector of length <span class="math inline">\(p-1\)</span> that contains
the <span class="math inline">\(X\)</span> values for the new
observation. Then consider <span class="math display">\[
  h_{new, new} =
\mathbf{X}_{new}^{\intercal}(\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}_{new}
  \]</span></p></li>
<li><p>If <span class="math inline">\(h_{new, new}\)</span> is much
larger than the leverage values <span
class="math inline">\(h_{ii}\)</span>, then this is an indication that
you have a hidden extrapolation.</p>
<pre class="r"><code>X &lt;- model.matrix(lm_bod)
Xnew &lt;- c(1, 22.1, 49.9, 23)
hnew &lt;- t(Xnew) %*% solve(t(X) %*% X) %*% Xnew
hnew</code></pre>
<pre><code>##        [,1]
## [1,] 0.2813</code></pre></li>
<li><p>Show <span class="math inline">\(h_{new, new}\)</span> is
reasonable:</p>
<pre class="r"><code>ggplot(data = a_bod, mapping = aes(x = .hat)) +
  geom_histogram(bins = 6, fill = &quot;white&quot;, color = &quot;black&quot;) +
  geom_vline(xintercept = hnew, lty = 2, col = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-20-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
</div>
<div id="detect-influential-values-cooks-distance"
class="section level2">
<h2>Detect Influential values: Cook’s Distance</h2>
<ul>
<li><p>A case is <strong>influential</strong> if its exclusion causes
major changes in the fitted regression function.</p></li>
<li><p>Cook’s Distance: How much on average all <span
class="math inline">\(n\)</span> fits change when the <span
class="math inline">\(i\)</span>th observation is removed. <span
class="math display">\[
  D_i = \frac{\sum_{j=1}^n\left(\hat{Y}_j -
\hat{Y}_{j(i)}\right)^2}{pMSE}
  \]</span></p>
<ul>
<li><p><span class="math inline">\(\hat{Y}_{j(i)}\)</span>: The fit of
observation <span class="math inline">\(j\)</span> when observation
<span class="math inline">\(i\)</span> is not in the data.</p></li>
<li><p>Cook’s distance is large if it is close to or greater than
1.</p></li>
<li><p>More sophisticated: To see if it is large, calculate its quantile
in an <span class="math inline">\(F(p, n-p)\)</span> distribution. If it
is at the 50th percentile or higher, then this is considered to have
high influence.</p></li>
</ul></li>
<li><p>Cook’s distance is the most widely use “case-influence”
statistic, but there are a couple others that you should be aware
of:</p></li>
<li><p>DFFITS (difference in fits): Number of standard deviations of
<span class="math inline">\(\hat{Y}_i\)</span> that the fitted values
<span class="math inline">\(\hat{Y}_i\)</span> increases or decreases
with the inclusion of the <span class="math inline">\(i\)</span>th case
in fitting the regression model. <span class="math display">\[
  (DFFITS)_i = \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)}h_{ii}}}
  \]</span></p>
<ul>
<li><span class="math inline">\(\hat{Y}_i\)</span>: Fit of observation
<span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\hat{Y}_{i(i)}\)</span>: Estimate of
observation <span class="math inline">\(i\)</span> when observation
<span class="math inline">\(i\)</span> is not in the data.</li>
<li><span class="math inline">\(MSE_{(i)}\)</span>: Mean squared error
when observation <span class="math inline">\(i\)</span> is not in the
data.</li>
<li><span class="math inline">\(h_{ii}\)</span>: <span
class="math inline">\(i\)</span>th leverage value.</li>
<li>Higher means more influence (bad).</li>
<li>If DFFITS exceeds 1 for a small dataset, or if it exceeds <span
class="math inline">\(2\sqrt{p/n}\)</span> for a large dataset, then the
observation has high influence.</li>
</ul></li>
<li><p>DFBETAS (difference in betas): How much the coefficient estimates
change when you omit an observation. <span class="math display">\[
  (DFBETAS)_k(i) = \frac{\hat{\beta}_k -
\hat{\beta}_{k(i)}}{\sqrt{MSE_{(i)}c_{kk}}}
  \]</span></p>
<ul>
<li><span class="math inline">\(\hat{\beta}_k\)</span>: OLS estimate of
<span class="math inline">\(\beta_k\)</span>.</li>
<li><span class="math inline">\(\hat{\beta}_{k(i)}\)</span>: OLS
estimate of <span class="math inline">\(\beta_k\)</span> when
observation <span class="math inline">\(i\)</span> is not in the
data.</li>
<li><span class="math inline">\(MSE_{(i)}\)</span>: Mean squared error
when observation <span class="math inline">\(i\)</span> is not in the
data.</li>
<li><span class="math inline">\(c_{kk}\)</span>: <span
class="math inline">\(k\)</span>th diagonal entry of <span
class="math inline">\((\mathbf{X}^{\intercal}\mathbf{X})^{-1}\)</span>.</li>
<li>Large if it exceeds 1 for small to medium datasets, or <span
class="math inline">\(2/\sqrt{n}\)</span> for large datasets.</li>
</ul></li>
<li><p>Note: These “case influence” statistics all assume that there is
only one outlier in the dataset. If there are two outliers in the same
region of the space of <span class="math inline">\(X\)</span> values,
then these case influence statistics might not be able to detect that
they are outliers.</p></li>
<li><p>In R, Cook’s distance is provided by <code>augment()</code> from
the <code>{broom}</code> package. We get DFFITS by the base R function
<code>dffits()</code> and DFBETAS by the base R function
<code>dfbetas()</code>.</p>
<pre class="r"><code>lm_bod &lt;- lm(fat ~ triceps + thigh + midarm, data = body)
a_bod &lt;- augment(lm_bod)
a_bod$.dffits &lt;- dffits(lm_bod)
a_bod[, paste0(&quot;.dfbetas_&quot;, colnames(model.matrix(lm_bod)))] &lt;- dfbetas(lm_bod)
glimpse(a_bod)</code></pre>
<pre><code>## Rows: 20
## Columns: 15
## $ fat                    &lt;dbl&gt; 11.9, 22.8, 18.7, 20.1, 12.9, 21.7, 27.1, 25.4,…
## $ triceps                &lt;dbl&gt; 19.5, 24.7, 30.7, 29.8, 19.1, 25.6, 31.4, 27.9,…
## $ thigh                  &lt;dbl&gt; 43.1, 49.8, 51.9, 54.3, 42.2, 53.9, 58.5, 52.1,…
## $ midarm                 &lt;dbl&gt; 29.1, 28.2, 37.0, 31.1, 30.9, 23.7, 27.6, 30.6,…
## $ .fitted                &lt;dbl&gt; 14.85, 20.22, 20.99, 23.13, 11.76, 22.24, 25.71…
## $ .resid                 &lt;dbl&gt; -2.9550, 2.5812, -2.2867, -3.0273, 1.1424, -0.5…
## $ .hat                   &lt;dbl&gt; 0.34121, 0.15654, 0.44043, 0.11243, 0.36110, 0.…
## $ .sigma                 &lt;dbl&gt; 2.383, 2.456, 2.437, 2.423, 2.535, 2.557, 2.530…
## $ .cooksd                &lt;dbl&gt; 0.2790493, 0.0595875, 0.2989628, 0.0531662, 0.0…
## $ .std.resid             &lt;dbl&gt; -1.46803, 1.13327, -1.23262, -1.29571, 0.57630,…
## $ .dffits                &lt;dbl&gt; -1.09969, 0.49290, -1.11299, -0.47196, 0.42392,…
## $ `.dfbetas_(Intercept)` &lt;dbl&gt; -0.760029, 0.399095, 0.381842, 0.045973, -0.211…
## $ .dfbetas_triceps       &lt;dbl&gt; -0.731244, 0.396302, 0.343574, 0.025716, -0.221…
## $ .dfbetas_thigh         &lt;dbl&gt; 0.759452, -0.399283, -0.341990, -0.032774, 0.20…
## $ .dfbetas_midarm        &lt;dbl&gt; 0.704904, -0.389289, -0.438916, -0.054325, 0.23…</code></pre></li>
</ul>
</div>
<div id="use-of-outlier-statistics" class="section level2">
<h2>Use of outlier statistics</h2>
<ul>
<li><p>Use outlier statistics to flag unusual values.</p></li>
<li><p>It is common to make an “Index Influence Plot” to see what cases
are most influential.</p>
<pre class="r"><code>lm_bod &lt;- lm(fat ~ triceps + thigh + midarm, data = body)
a_bod &lt;- augment(lm_bod)
a_bod %&gt;%
  mutate(i = row_number()) %&gt;%
  select(i, 
         &quot;Leverage&quot; = .hat, 
         &quot;Cook&#39;s\nDistance&quot; = .cooksd, 
         &quot;Studentized\nResiduals&quot; = .std.resid) %&gt;%
  pivot_longer(cols = -i, names_to = &quot;Metric&quot;, values_to = &quot;value&quot;) %&gt;%
  ggplot(aes(x = i, y = value)) +
  facet_grid(Metric ~ ., scales = &quot;free_y&quot;) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:nrow(a_bod)) +
  xlab(&quot;Case Index Number&quot;) +
  ylab(&quot;Value&quot;)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>If you have some unusual values, go through this pipeline:</p>
<ol style="list-style-type: decimal">
<li>Do the conclusions change when the case is deleted? If No, then
proceed with the case included.</li>
<li>If Yes, is there reason to believe the case belongs to a population
other than the one under consideration? E.g., was it a numerical error,
is there some expectation that this point would behave differently
(e.g. DC versus states). If Yes, then omit the case.</li>
<li>If No, then does the case have an unusually distant explanatory
variable (measured by leverage value)? If yes, then omit the case and
report conclusions based on the subset of data, limiting the scope of
the study to the range of predictor variables.</li>
<li>If Yes, then you can try a more robust approach (e.g. quantile
regression).</li>
</ol></li>
</ul>
</div>
</div>
<div id="multicollinearity-variance-inflation-factor"
class="section level1">
<h1>Multicollinearity: Variance Inflation Factor</h1>
<ul>
<li><p><strong>Goal</strong>: Quantify degree of
multicollinearity.</p></li>
<li><p>Informal diagnostics:</p>
<ol style="list-style-type: decimal">
<li>Matrix scatterplots (but not sufficient to detect all kinds of
multicollinearity).</li>
<li>Large changes in estimated regression coefficients and their
estimated standard errors when a variable is added.</li>
<li>Non-significant results in individual tests on anticipated important
variables.</li>
<li>Different sign than expected on coefficient of a predictor.</li>
</ol></li>
<li><p><strong>Variance Inflation Factor</strong>: How much the
variances of the estimated regression coefficients are inflated as
compared to when the predictor variables are not correlated.</p>
<p><span class="math display">\[
  (VIF)_k = \frac{1}{1 - R_k^2}
  \]</span> where <span class="math inline">\(R_k^2\)</span> is the
coefficient of multiple determination using <span
class="math inline">\(X_k\)</span> as the response and every other <span
class="math inline">\(X\)</span> as the predictor.</p></li>
<li><p>The VIF comes from the equation for the standard error of <span
class="math inline">\(\hat{\beta}_k\)</span>, <span
class="math display">\[
  s^2(\hat{\beta}) = \frac{\sigma^2}{(n-1)\mathrm{var}(X_{k})}\frac{1}{1
- R_k^2}
  \]</span></p></li>
<li><p><span class="math inline">\((VIF)_k\)</span> is equal to 1 when
there is no multicollinearity.</p></li>
<li><p><span class="math inline">\((VIF)_k\)</span> is greater than 1
when there is multicollinearity.</p></li>
<li><p>If the maximum VIF value is greater than 10, then this is an
indication of extreme multicollinearity (I have no idea where this
rule-of-thumb comes from).</p></li>
<li><p>You can get the variance inflation factors by the
<code>vif()</code> function from the <code>{car}</code> package.</p>
<pre class="r"><code>library(car)
vif(lm_bod)</code></pre>
<pre><code>## triceps   thigh  midarm 
##   708.8   564.3   104.6</code></pre>
<p>This indicates tons of multicollinearity.</p>
<p>If we get rid of midarm, how much multicollinearity is there?</p>
<pre class="r"><code>lm_tt &lt;- lm(fat ~ triceps + thigh, data = body)
vif(lm_tt)</code></pre>
<pre><code>## triceps   thigh 
##   6.825   6.825</code></pre></li>
<li><p>Condition Numbers: Ratio of the maximum singular value of <span
class="math inline">\(X\)</span> divided by minimum singular value of
<span class="math inline">\(X\)</span>.</p>
<ul>
<li>Intuition: If <span class="math inline">\(X\)</span> is almost
singular than <span
class="math inline">\((\mathbf{X}^{\intercal}\mathbf{X})^{-1}\)</span>
will blow up.</li>
</ul>
<pre class="r"><code>svals &lt;- svd(model.matrix(lm_bod))$d
svals[[1]] / svals[[length(svals)]]</code></pre>
<pre><code>## [1] 11482</code></pre>
<ul>
<li>Value of 5–10 indicates weak multicollinearity.</li>
<li>Value of 30–100 indicates moderate to strong multicollinearity.</li>
<li>Value &gt; 100 indicates very strong multicollinearity.</li>
<li>NOTE: Condition numbers are not directly comparable to one
another.</li>
<li>For more information on condition numbers, see <span
class="citation">Anderson and Wells (2008)</span>.</li>
</ul></li>
</ul>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-anderson2008numerical" class="csl-entry">
Anderson, William, and Martin T. Wells. 2008. <span>“Numerical Analysis
in Least Squares Regression with an Application to the Abortion-Crime
Debate.”</span> <em>Journal of Empirical Legal Studies</em> 5 (4):
647–81. <a
href="https://doi.org/10.1111/j.1740-1461.2008.00137.x">https://doi.org/10.1111/j.1740-1461.2008.00137.x</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
