<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="David Gerard" />

<meta name="date" content="2021-11-15" />

<title>MLR: More Complicated Model Diagnostics</title>

<script src="site_libs/header-attrs-2.11/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>




<style type="text/css">
/* for pandoc --citeproc since 2.11 */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Teaching Website for STAT 415/615</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="data.html">Data</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/dcgerard/stat_415_615">GitHub</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">MLR: More Complicated Model Diagnostics</h1>
<h4 class="author">David Gerard</h4>
<h4 class="date">2021-11-15</h4>

</div>


<script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script>
<div id="learning-objectives" class="section level1">
<h1>Learning Objectives</h1>
<ul>
<li>Studentized residuals, Leverage values, DFFits, Cook’s Distance, Variance Inflation Factor</li>
<li>Chapter 10 of KNNL</li>
</ul>
</div>
<div id="added-variable-plots" class="section level1">
<h1>Added Variable Plots</h1>
<ul>
<li><p><em>Goal</em>: Visualization to determine model adequacy for a single predictor.</p></li>
<li><p><em>Motivation</em>: <a href="./05_mlr_i.html">Recall</a> that we said to plot residuals versus predictors to see if there were curvature.</p>
<ul>
<li>This shows the relationship between the predictor and the residuals, but <strong>not</strong> after adjusting for other variables.</li>
</ul></li>
<li><p><strong>Added-variable plots</strong> (aka “partial regression plots” and “adjusted variables plots”)</p>
<ol style="list-style-type: decimal">
<li>Regress <span class="math inline">\(Y\)</span> (as the response) on every predictor <em>except</em> <span class="math inline">\(X_k\)</span>. Obtain the residuals, and call these <span class="math inline">\(e_i(Y)\)</span></li>
<li>Regress <span class="math inline">\(X_k\)</span> (as the response) on every other predictor. Obtain the residuals, and call these <span class="math inline">\(e_i(X_k)\)</span></li>
<li>Make a scatterplot of <span class="math inline">\(e_i(X_k)\)</span> versus <span class="math inline">\(e_i(Y)\)</span>.</li>
</ol></li>
<li><p><em>Intuition</em>:</p>
<ul>
<li><span class="math inline">\(e_i(Y)\)</span> and <span class="math inline">\(e_i(X_k)\)</span> reflect the part of each variable that is not linearly associated with the other predictors.</li>
</ul></li>
<li><p><em>Conclusions</em>:</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(e_i(Y)\)</span> and <span class="math inline">\(e_i(X_k)\)</span> do not appear to be linearly related, then <span class="math inline">\(X_k\)</span> is not associated with <span class="math inline">\(Y\)</span> given all other predictors. <img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-1-1.png" width="384" style="display: block; margin: auto;" /></li>
<li>If <span class="math inline">\(e_i(Y)\)</span> and <span class="math inline">\(e_i(X_k)\)</span> appear to be linearly related, then <span class="math inline">\(X_k\)</span> is linearly associated with <span class="math inline">\(Y\)</span> given all other predictors.
<ul>
<li>The slope of the least squares line through the origin in this plot turns out to be <span class="math inline">\(\hat{\beta}_k\)</span> in the MLR fit. <img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-2-1.png" width="384" style="display: block; margin: auto;" /></li>
</ul></li>
<li>If <span class="math inline">\(e_i(Y)\)</span> and <span class="math inline">\(e_i(X_k)\)</span> appear to have a curved relationship, then <span class="math inline">\(X_k\)</span> has a curved relationship with <span class="math inline">\(Y\)</span> given all other predictors. <img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></li>
<li>If you see an outlying point, this might be an influential point for predictor <span class="math inline">\(X_k\)</span> given all other predictors. <img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-4-1.png" width="384" style="display: block; margin: auto;" /></li>
</ol></li>
<li><p>Added variable plots can also help us visualize how much the variability will be reduced if we add predictor <span class="math inline">\(X_k\)</span> to the model.</p>
<ul>
<li>Variability of <span class="math inline">\(e_i(Y)\)</span> about the horizontal <span class="math inline">\(y=0\)</span> line gives us the SSE of model <em>without</em> <span class="math inline">\(X_k\)</span>.</li>
<li>Variability of <span class="math inline">\(e_i(y)\)</span> about the OLS line in the scatterplot of <span class="math inline">\(e(X_k)\)</span> on <span class="math inline">\(e(Y)\)</span> gives us the SSE of the model <em>with</em> <span class="math inline">\(X_k\)</span>.</li>
<li>Figure 10.2 from KNNL: <img src="figs/avplot.png" width="644" style="display: block; margin: auto;" /></li>
</ul></li>
</ul>
</div>
<div id="life-insurance-example" class="section level1">
<h1>Life Insurance Example</h1>
<ul>
<li><p>Researchers were interested in the association between income in thousands of dollars (<span class="math inline">\(X_1\)</span>), a risk aversion score (<span class="math inline">\(X_2\)</span>) found via a questionnaire, and the amount of life insurance carried in thousands of dollars (<span class="math inline">\(Y\)</span>). You can load these data into R via:</p>
<pre class="r"><code>library(tidyverse)
library(broom)
managers &lt;- tibble::tribble(
  ~income, ~risk, ~insurance,
    45.01,     6,         91,
   57.204,     4,        162,
   26.852,     5,         11,
    66.29,     7,        240,
   40.964,     5,         73,
   72.996,    10,        311,
    79.38,     1,        316,
   52.766,     8,        154,
   55.916,     6,        164,
   38.122,     4,         54,
    35.84,     6,         53,
   75.796,     9,        326,
   37.408,     5,         55,
   54.376,     2,        130,
   46.186,     7,        112,
    46.13,     4,         91,
   30.366,     3,         14,
    39.06,     5,         63
  )</code></pre></li>
<li><p>We typically make added variable plots after we notice something fishy via the usual residual plots.</p></li>
<li><p>Lets fit a full of line insurance on income and risk aversion and plot the residuals</p>
<pre class="r"><code>lm_man &lt;- lm(insurance ~ income + risk, data = managers)
a_man &lt;- augment(lm_man)
qplot(x = .fitted, y = .resid, data = a_man) +
  geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-7-1.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = income, y = .resid, data = a_man) +
  geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-7-2.png" width="384" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qplot(x = risk, y = .resid, data = a_man) +
  geom_hline(yintercept = 0)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-7-3.png" width="384" style="display: block; margin: auto;" /> The big issue appears to be from income.</p></li>
<li><p>Let’s see the form of the relationship between income and insurance, adjusting for risk aversion.</p></li>
<li><p>To make an added variable plot, first regression <span class="math inline">\(Y\)</span> on the other predictors, then <span class="math inline">\(X_k\)</span> on the other predictors.</p>
<pre class="r"><code>lm_y &lt;- lm(insurance ~ risk, data = managers)
lm_xk &lt;- lm(income ~ risk, data = managers)</code></pre>
<p>Then plot the residuals from each fit.</p>
<pre class="r"><code>qplot(x = resid(lm_xk), y = resid(lm_y), xlab = &quot;e(income|risk)&quot;, ylab = &quot;e(insurance|risk)&quot;) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  geom_hline(yintercept = 0, lty = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>A faster way (but not for written reports or homeworks since it looks less nice) is to use the <code>avPlot()</code> function from the <code>{car}</code> package</p>
<pre class="r"><code>library(car)
avPlot(lm_man, variable = &quot;income&quot;)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-10-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>From this plot, we see that</p>
<ol style="list-style-type: decimal">
<li>The relationship between income and insurance is very strong, adjusting for risk. The variability about the OLS line is much smaller than the variability about the <span class="math inline">\(y=0\)</span> line.</li>
<li>Income appears to have a monotone curved relationship with insurance, adjusting for risk.</li>
<li>Even though the relationship is clearly curved, most of the variability can be explained by the linear effect, indicating that using a linear assumption would be a fine first order approximation (depending on the goals of the study).</li>
<li>There is one observation in the upper right that might be influential.</li>
</ol></li>
<li><p><strong>Exercise</strong>: Recall the body fat data, that you can read about <a href="https://dcgerard.github.io/stat_415_615/data.html#Body_Fat">here</a> and download from here: <a href="https://dcgerard.github.io/stat_415_615/data/body.csv" class="uri">https://dcgerard.github.io/stat_415_615/data/body.csv</a></p>
<p>Make an added variable plot for triceps given thigh, and thigh given triceps (omit midarm for this exercise). What conclusions do you draw from the added variable plots?</p></li>
<li><p>Notes:</p>
<ol style="list-style-type: decimal">
<li>Added variable plots will change based on what other predictors you are adjusting for. The nature of the relationship between <span class="math inline">\(X_k\)</span> and <span class="math inline">\(Y\)</span> might change based on what other predictors are there.</li>
<li>Added variable plots are only appropriate when the assumptions of the linear model are fulfilled for the other predictors that you are adjusting for.</li>
</ol></li>
</ul>
</div>
<div id="plots-to-detect-outliers" class="section level1">
<h1>Plots to Detect Outliers</h1>
<ul>
<li><p><strong>Goal</strong>: Visualization to flag outlying observations.</p></li>
<li><p>Consider this plot of outliers <img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-13-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>Outliers have minor effects if</p>
<ol style="list-style-type: decimal">
<li>They are well in the range of typical <span class="math inline">\(X\)</span> values. This is since many other <span class="math inline">\(X\)</span> values are there to keep the OLS line from getting too displaced. This is Case 3 above.</li>
<li>They are consistent with the general trend between the predictors and <span class="math inline">\(Y\)</span>. This is Case 2 above.</li>
</ol></li>
<li><p>Outliers can have huge effects if they both (i) have atypical <span class="math inline">\(X\)</span> values and (ii) are not consistent with the general trend. This is Case 3 above.</p></li>
<li><p>Recall that outliers are hard to detect when there is multicollinearity</p>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-14-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="detect-extreme-y-values-studentized-residuals" class="section level2">
<h2>Detect Extreme <span class="math inline">\(Y\)</span>-values: Studentized Residuals</h2>
<ul>
<li><p>So far, I have just been having you “feel” for outliers via looking at residual plot.</p></li>
<li><p>Let’s try and detect outliers more systematically.</p></li>
<li><p>To know how extreme a residual is, we need to know its standard deviation (how far it should be from 0 on average).</p></li>
<li><p>Let <span class="math inline">\(h_{ii}\)</span> be the <span class="math inline">\(i\)</span>th diagonal element of the hat matrix <span class="math inline">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>.</p>
<ul>
<li>There are <span class="math inline">\(n\)</span> diagonal elements, one for each observation.</li>
</ul></li>
<li><p>You can show that the variance of the <span class="math inline">\(i\)</span>th residual, <span class="math inline">\(e_i\)</span>, is <span class="math display">\[
  \sigma^2(e_i) = \sigma^2(1 - h_{ii}).
  \]</span></p></li>
<li><p>So we estimate the standard deviation of the <span class="math inline">\(i\)</span>th residual via <span class="math display">\[
  s(e_i) = \sqrt{MSE(1 - h_{ii})}.
  \]</span></p></li>
<li><p>Note that each residual has a different standard deviation depending on its value of <span class="math inline">\(h_{ii}\)</span>. So</p>
<ul>
<li>Larger <span class="math inline">\(h_{ii}\)</span> means smaller standard deviation.</li>
<li>Smaller <span class="math inline">\(h_{ii}\)</span> means larger standard deviation.</li>
</ul></li>
<li><p>The <strong>studentized</strong> residual is the residual divided by its standard deviation. <span class="math display">\[
  r_i = \frac{e_i}{s(e_i)}
  \]</span></p></li>
<li><p>The idea is that we making all of the residuals to have about the same standard deviation.</p></li>
<li><p>In a normal distribution 95% of observations will be within 2 standard deviations of the mean, and 99.7% of observations will be within 3 standard deviations of the mean.</p></li>
<li><p>So residuals have potentially large <span class="math inline">\(Y\)</span> values if they outside of <span class="math inline">\([-2,2]\)</span>.</p></li>
<li><p>But remember, we would still expect about 5% of observations to lie outside of <span class="math inline">\([-2, 2]\)</span>. So be chill here.</p></li>
<li><p>R calls these “standardized residuals,” and you can gent them from the <code>.std.resid</code> variable from the output of <code>augment()</code> from the <code>{broom}</code> package.</p>
<pre class="r"><code>body &lt;- read_csv(&quot;https://dcgerard.github.io/stat_415_615/data/body.csv&quot;)
lm_bod &lt;- lm(fat ~ thigh + triceps, data = body)
a_bod &lt;- augment(lm_bod)
qplot(x = .fitted, y = .std.resid, data = a_bod) +
  geom_hline(yintercept = 0) +
  geom_hline(yintercept = -2, lty = 2)  +
  geom_hline(yintercept = 2, lty = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-15-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>It is common to also obtain the <strong>studentized deleted residuals</strong> where you calculate the error variance by leave-one-out regression.</p>
<ol style="list-style-type: decimal">
<li>For each observation <span class="math inline">\(i\)</span>, fit the MLR model without <span class="math inline">\(i\)</span> to get the MSE. Call this <span class="math inline">\(MSE_{(i)}\)</span>.</li>
<li>Calculate the studentized deleted residuals by <span class="math display">\[
 t_i = \frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}}
 \]</span></li>
</ol></li>
<li><p>The intuition is that a real outlier might have undue influence on the OLS line, so we should standardize when it is not used to calculate the OLS line. See Chapter 10 of KNNL for a deeper discussion.</p></li>
<li><p>R calls the studentized deleted residuals just “studentized residuals.” You can get them via the base function <code>rstudent()</code>.</p>
<pre class="r"><code>a_bod$.dstd.resid &lt;- rstudent(lm_bod)
qplot(x = .fitted, y = .dstd.resid, data = a_bod) +
      geom_hline(yintercept = 0) +
      geom_hline(yintercept = -2, lty = 2)  +
      geom_hline(yintercept = 2, lty = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-16-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>In this case, the studentized and deleted studentized residuals produced the same conclusions (that everything is fine), but they can be very different if there is a single highly influential point. The deleted studentized residual would be preferred.</p></li>
</ul>
</div>
<div id="detect-extreme-x-values-leverage-values" class="section level2">
<h2>Detect Extreme <span class="math inline">\(X\)</span> values: Leverage Values</h2>
<ul>
<li><p><strong>Goal</strong>: Numerically quantify how outlying an observation is.</p></li>
<li><p>The diagonal elements of the hat matrix <span class="math inline">\(h_{ii}\)</span> are called <strong>leverage values</strong>.</p></li>
<li><p><span class="math inline">\(h_{ii}\)</span> is a measure of the distance between the <span class="math inline">\(X\)</span> values for the <span class="math inline">\(i\)</span>th case and the means of the <span class="math inline">\(X\)</span> values for all <span class="math inline">\(n\)</span> cases.</p>
<ul>
<li>The larger <span class="math inline">\(h_{ii}\)</span> is, the further from the center of the cloud of <span class="math inline">\(X\)</span> values it is.</li>
</ul></li>
<li><p>Properties: <span class="math display">\[
  0 \leq h_{ii} \leq 1\\
  \sum_{i=1}^nh_{ii} = p
  \]</span></p></li>
<li><p><span class="math inline">\(h_{ii}\)</span> is a function only the <span class="math inline">\(X\)</span> values, not <span class="math inline">\(Y\)</span>, and so is a measure of how extreme the observational units <span class="math inline">\(X\)</span> value is, not considering <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Example red dot is <span class="math inline">\((\bar{X}_1, \bar{X}_2)\)</span>. Numbers are leverage values: <img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-17-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>Rule-of-thumb: A hat value is large if it is greater than <span class="math inline">\(2p/n\)</span>.</p></li>
<li><p>The <code>.hat</code> variable from the output of <code>augment()</code> from the <code>{broom}</code> package contains the leverage values.</p></li>
<li><p>Body fat example: Observation 3 appears to have a high leverage:</p>
<pre class="r"><code>lm_bod &lt;- lm(fat ~ triceps + thigh + midarm, data = body)
a_bod &lt;- augment(lm_bod)

cutoff &lt;- 2 * ncol(model.matrix(lm_bod)) / nrow(model.matrix(lm_bod))
a_bod &lt;- mutate(a_bod, i = row_number())
qplot(x = .hat, 
      y = .std.resid, 
      label = i,
      data = a_bod,
      main = &quot;Residuals vs Leverage&quot;,
      geom = &quot;label&quot;) +
  geom_vline(xintercept = cutoff, lty = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-18-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
<div id="hat-matrix-for-hidden-extrapolation" class="section level3">
<h3>Hat-matrix for hidden extrapolation</h3>
<ul>
<li><p>Since the leverage values are a measure for extremeness, it can be used to flag possible hidden extrapolations.</p>
<ul>
<li>Not a perfect measure, since it could still be a hidden extrapolation, but it’s better than nothing.</li>
</ul></li>
<li><p>Let <span class="math inline">\(\mathbf{X}_{new}\)</span> be the vector of length <span class="math inline">\(p-1\)</span> that contains the <span class="math inline">\(X\)</span> values for the new observation. Then consider <span class="math display">\[
  h_{new, new} = \mathbf{X}_{new}^{\intercal}(\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}_{new}
  \]</span></p></li>
<li><p>If <span class="math inline">\(h_{new, new}\)</span> is much larger than the leverage values <span class="math inline">\(h_{ii}\)</span>, then this is an indication that you have a hidden extrapolation.</p>
<pre class="r"><code>X &lt;- model.matrix(lm_bod)
Xnew &lt;- c(1, 22.1, 49.9, 23)
hnew &lt;- t(Xnew) %*% solve(t(X) %*% X) %*% Xnew
hnew</code></pre>
<pre><code>##        [,1]
## [1,] 0.2813</code></pre></li>
<li><p>Show <span class="math inline">\(h_{new, new}\)</span> is reasonable:</p>
<pre class="r"><code>qplot(x = .hat, data = a_bod, bins = 6, fill = I(&quot;white&quot;), color = I(&quot;black&quot;)) +
  geom_vline(xintercept = hnew, lty = 2, col = 2)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-20-1.png" width="384" style="display: block; margin: auto;" /></p></li>
</ul>
</div>
</div>
<div id="detect-influential-values-cooks-distance" class="section level2">
<h2>Detect Influential values: Cook’s Distance</h2>
<ul>
<li><p>A case is <strong>influential</strong> if its exclusion causes major changes in the fitted regression function.</p></li>
<li><p>Cook’s Distance: How much on average all <span class="math inline">\(n\)</span> fits change when the <span class="math inline">\(i\)</span>th observation is removed. <span class="math display">\[
  D_i = \frac{\sum_{j=1}^n\left(\hat{Y}_j - \hat{Y}_{j(i)}\right)^2}{pMSE}
  \]</span></p>
<ul>
<li><p><span class="math inline">\(\hat{Y}_{j(i)}\)</span>: The fit of observation <span class="math inline">\(j\)</span> when observation <span class="math inline">\(i\)</span> is not in the data.</p></li>
<li><p>Cook’s distance is large if it is close to or greater than 1.</p></li>
<li><p>More sophisticated: To see if it is large, calculate its quantile in an <span class="math inline">\(F(p, n-p)\)</span> distribution. If it is at the 50th percentile or higher, then this is considered to have high influence.</p></li>
</ul></li>
<li><p>Cook’s distance is the most widely use “case-influence” statistic, but there are a couple others that you should be aware of:</p></li>
<li><p>DFFITS (difference in fits): Number of standard deviations of <span class="math inline">\(\hat{Y}_i\)</span> that the fitted values <span class="math inline">\(\hat{Y}_i\)</span> increases or decreases with the inclusion of the <span class="math inline">\(i\)</span>th case in fitting the regression model. <span class="math display">\[
  (DFFITS)_i = \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)}h_{ii}}}
  \]</span></p>
<ul>
<li><span class="math inline">\(\hat{Y}_i\)</span>: Fit of observation <span class="math inline">\(i\)</span>.</li>
<li><span class="math inline">\(\hat{Y}_{i(i)}\)</span>: Estimate of observation <span class="math inline">\(i\)</span> when observation <span class="math inline">\(i\)</span> is not in the data.</li>
<li><span class="math inline">\(MSE_{(i)}\)</span>: Mean squared error when observation <span class="math inline">\(i\)</span> is not in the data.</li>
<li><span class="math inline">\(h_{ii}\)</span>: <span class="math inline">\(i\)</span>th leverage value.</li>
<li>Higher means more influence (bad).</li>
<li>If DFFITS exceeds 1 for a small dataset, or if it exceeds <span class="math inline">\(2\sqrt{p/n}\)</span> for a large dataset, then the observation has high influence.</li>
</ul></li>
<li><p>DFBETAS (difference in betas): How much the coefficient estimates change when you omit an observation. <span class="math display">\[
  (DFBETAS)_k(i) = \frac{\hat{\beta}_k - \hat{\beta}_{k(i)}}{\sqrt{MSE_{(i)}c_{kk}}}
  \]</span></p>
<ul>
<li><span class="math inline">\(\hat{\beta}_k\)</span>: OLS estimate of <span class="math inline">\(\beta_k\)</span>.</li>
<li><span class="math inline">\(\hat{\beta}_{k(i)}\)</span>: OLS estimate of <span class="math inline">\(\beta_k\)</span> when observation <span class="math inline">\(i\)</span> is not in the data.</li>
<li><span class="math inline">\(MSE_{(i)}\)</span>: Mean squared error when observation <span class="math inline">\(i\)</span> is not in the data.</li>
<li><span class="math inline">\(c_{kk}\)</span>: <span class="math inline">\(k\)</span>th diagonal entry of <span class="math inline">\((\mathbf{X}^{\intercal}\mathbf{X})^{-1}\)</span>.</li>
<li>Large if it exceeds 1 for small to medium datasets, or <span class="math inline">\(2/\sqrt{n}\)</span> for large datasets.</li>
</ul></li>
<li><p>Note: These “case influence” statistics all assume that there is only one outlier in the dataset. If there are two outliers in the same region of the space of <span class="math inline">\(X\)</span> values, then these case influence statistics might not be able to detect that they are outliers.</p></li>
<li><p>In R, Cook’s distance is provided by <code>augment()</code> from the <code>{broom}</code> package. We get DFFITS by the base R function <code>dffits()</code> and DFBETAS by the base R function <code>dfbetas()</code>.</p>
<pre class="r"><code>lm_bod &lt;- lm(fat ~ triceps + thigh + midarm, data = body)
a_bod &lt;- augment(lm_bod)
a_bod$.dffits &lt;- dffits(lm_bod)
a_bod[, paste0(&quot;.dfbetas_&quot;, colnames(model.matrix(lm_bod)))] &lt;- dfbetas(lm_bod)
glimpse(a_bod)</code></pre>
<pre><code>## Rows: 20
## Columns: 15
## $ fat                    &lt;dbl&gt; 11.9, 22.8, 18.7, 20.1, 12.9, 21.7, 27.1, 25.4,…
## $ triceps                &lt;dbl&gt; 19.5, 24.7, 30.7, 29.8, 19.1, 25.6, 31.4, 27.9,…
## $ thigh                  &lt;dbl&gt; 43.1, 49.8, 51.9, 54.3, 42.2, 53.9, 58.5, 52.1,…
## $ midarm                 &lt;dbl&gt; 29.1, 28.2, 37.0, 31.1, 30.9, 23.7, 27.6, 30.6,…
## $ .fitted                &lt;dbl&gt; 14.85, 20.22, 20.99, 23.13, 11.76, 22.24, 25.71…
## $ .resid                 &lt;dbl&gt; -2.9550, 2.5812, -2.2867, -3.0273, 1.1424, -0.5…
## $ .hat                   &lt;dbl&gt; 0.34121, 0.15654, 0.44043, 0.11243, 0.36110, 0.…
## $ .sigma                 &lt;dbl&gt; 2.383, 2.456, 2.437, 2.423, 2.535, 2.557, 2.530…
## $ .cooksd                &lt;dbl&gt; 0.2790493, 0.0595875, 0.2989628, 0.0531662, 0.0…
## $ .std.resid             &lt;dbl&gt; -1.46803, 1.13327, -1.23262, -1.29571, 0.57630,…
## $ .dffits                &lt;dbl&gt; -1.09969, 0.49290, -1.11299, -0.47196, 0.42392,…
## $ `.dfbetas_(Intercept)` &lt;dbl&gt; -0.760029, 0.399095, 0.381842, 0.045973, -0.211…
## $ .dfbetas_triceps       &lt;dbl&gt; -0.731244, 0.396302, 0.343574, 0.025716, -0.221…
## $ .dfbetas_thigh         &lt;dbl&gt; 0.759452, -0.399283, -0.341990, -0.032774, 0.20…
## $ .dfbetas_midarm        &lt;dbl&gt; 0.704904, -0.389289, -0.438916, -0.054325, 0.23…</code></pre></li>
</ul>
</div>
<div id="use-of-outlier-statistics" class="section level2">
<h2>Use of outlier statistics</h2>
<ul>
<li><p>Use outlier statistics to flag unusual values.</p></li>
<li><p>It is common to make an “Index Influence Plot” to see what cases are most influential.</p>
<pre class="r"><code>lm_bod &lt;- lm(fat ~ triceps + thigh + midarm, data = body)
a_bod &lt;- augment(lm_bod)
a_bod %&gt;%
  mutate(i = row_number()) %&gt;%
  select(i, 
         &quot;Leverage&quot; = .hat, 
         &quot;Cook&#39;s\nDistance&quot; = .cooksd, 
         &quot;Studentized\nResiduals&quot; = .std.resid) %&gt;%
  pivot_longer(cols = -i, names_to = &quot;Metric&quot;, values_to = &quot;value&quot;) %&gt;%
  ggplot(aes(x = i, y = value)) +
  facet_grid(Metric ~ ., scales = &quot;free_y&quot;) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 1:nrow(a_bod)) +
  xlab(&quot;Case Index Number&quot;) +
  ylab(&quot;Value&quot;)</code></pre>
<p><img src="05_mlr_diagnostics_files/figure-html/unnamed-chunk-22-1.png" width="384" style="display: block; margin: auto;" /></p></li>
<li><p>If you have some unusual values, go through this pipeline:</p>
<ol style="list-style-type: decimal">
<li>Do the conclusions change when the case is deleted? If No, then proceed with the case included.</li>
<li>If Yes, is there reason to believe the case belongs to a population other than the one under consideration? E.g., was it a numerical error, is there some expectation that this point would behave differently (e.g. DC versus states). If Yes, then omit the case.</li>
<li>If No, then does the case have an unusually distant explanatory variable (measured by leverage value)? If yes, then omit the case and report conclusions based, limiting the scope of the study to the range of predictor variables.</li>
<li>If Yes, then you can try a more robust approach.</li>
</ol></li>
</ul>
</div>
</div>
<div id="multicollinearity-variance-inflation-factor" class="section level1">
<h1>Multicollinearity: Variance Inflation Factor</h1>
<ul>
<li><p><strong>Goal</strong>: Quantify degree of multicollinearity.</p></li>
<li><p>Informal diagnostics:</p>
<ol style="list-style-type: decimal">
<li>Matrix scatterplots (but not sufficient to detect all kinds of multicollinearity).</li>
<li>Large changes in estimated regression coefficients and their estimated standard errors when a variable is added.</li>
<li>Non-significant results in individual tests on anticipated important variables.</li>
<li>Different sign than expected on coefficient of a predictor.</li>
</ol></li>
<li><p><strong>Variance Inflation Factor</strong>: How much the variances of the estimated regression coefficients are inflated as compared to when the predictor variables are not correlated.</p>
<p><span class="math display">\[
  (VIF)_k = \frac{1}{1 - R_k^2}
  \]</span> where <span class="math inline">\(R_k^2\)</span> is the coefficient of multiple determination using <span class="math inline">\(X_k\)</span> as the response and every other <span class="math inline">\(X\)</span> as the predictor.</p></li>
<li><p><span class="math inline">\((VIF)_k\)</span> is equal to 1 when there is no multicollinearity.</p></li>
<li><p><span class="math inline">\((VIF)_k\)</span> is greater than 1 when there is multicollinearity.</p></li>
<li><p>If the maximum VIF value is greater than 10, then this is an indication of extreme multicollinearity.</p></li>
<li><p>You can get the variance inflation factors by the <code>vif()</code> function from the <code>{car}</code> package.</p>
<pre class="r"><code>library(car)
vif(lm_bod)</code></pre>
<pre><code>## triceps   thigh  midarm 
##   708.8   564.3   104.6</code></pre>
<p>This indicates tons of multicollinearity.</p></li>
<li><p>Condition Numbers: Ratio of the maximum singular value of <span class="math inline">\(X\)</span> divided by minimum singular value of <span class="math inline">\(X\)</span>.</p>
<ul>
<li>Intuition: If <span class="math inline">\(X\)</span> is almost singular than <span class="math inline">\((\mathbf{X}^{\intercal}\mathbf{X})^{-1}\)</span> will blow up.</li>
</ul>
<pre class="r"><code>svals &lt;- svd(model.matrix(lm_bod))$d
svals[[1]] / svals[[length(svals)]]</code></pre>
<pre><code>## [1] 11482</code></pre>
<ul>
<li>Value of 5–10 indicates weak multicollinearity.</li>
<li>Value of 30–100 indicates moderate to strong multicollinearity.</li>
<li>Value &gt; 100 indicates very strong multicollinearity.</li>
<li>NOTE: Condition numbers are not directly comparable to one another.</li>
<li>For more information on condition numbers, see <span class="citation">Anderson and Wells (2008)</span>.</li>
</ul></li>
</ul>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-anderson2008numerical" class="csl-entry">
Anderson, William, and Martin T. Wells. 2008. <span>“Numerical Analysis in Least Squares Regression with an Application to the Abortion-Crime Debate.”</span> <em>Journal of Empirical Legal Studies</em> 5 (4): 647–81. <a href="https://doi.org/10.1111/j.1740-1461.2008.00137.x">https://doi.org/10.1111/j.1740-1461.2008.00137.x</a>.
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
